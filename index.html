<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="generator" content="pandoc">

    <title>Sign Language Processing</title>

        <meta name="author" content="Amit Moryossef (amitmoryossef@gmail.com)">
        <meta name="author" content="Yoav Goldberg (yoav.goldberg@biu.ac.il)">
    
    
    
    <!-- Bootstrap core CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">

    <!-- Code syntax highlighting -->
    <style type="text/css">code{white-space: pre;}</style>
        <style type="text/css">pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */</style>
    
        <link rel="stylesheet" href="style.css" />
    
    

    <!-- All other additional includes -->
    <meta name="google-site-verification" content="IEQ1c5BwY2ZQl5N3uQS9xbNv7LX-txlqJCbbPuaz3Qc"/>
<meta name="description" content="Sign Language Processing (SLP) is a field of artificial intelligence
    concerned with automatic processing and analysis of sign language content.
    This project aims to organize the sign language processing literature, datasets, and tasks."/>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XHQQ8R0NSQ"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){
        dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-XHQQ8R0NSQ');
</script>
</head>

<body>
    
    <header class="navbar sticky-top navbar-dark bg-dark bd-navbar">
        <div class="container-fluid">
            <a class="navbar-brand mb-0 h1" href="#">Sign Language Processing</a>

            <span class="navbar-text">
                <a class="github-button" href="https://github.com/sign-language-processing/sign-language-processing.github.io" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star sign-language-processing/sign-language-processing.github.io on GitHub">Star</a>
            </span>
        </div>
    </header>

    <div class="row" id="container">
                <aside class="bd-sidebar col-12 col-lg-2" id="navbar">
            <nav class="bd-links nav nav-pills flex-column sticky-top"><ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#brief-history-of-signed-languages-and-deaf-culture">(Brief) History of Signed Languages and Deaf Culture</a></li>
</ul></li>
<li><a href="#sign-language-linguistics-overview">Sign Language Linguistics Overview</a></li>
<li><a href="#sign-language-representations">Sign Language Representations</a></li>
<li><a href="#tasks">Tasks</a>
<ul>
<li><a href="#sign-language-detection">Sign Language Detection</a></li>
<li><a href="#sign-language-identification">Sign Language Identification</a></li>
<li><a href="#sign-language-segmentation">Sign Language Segmentation</a></li>
<li><a href="#sign-language-recognition-translation-and-production">Sign Language Recognition, Translation, and Production</a></li>
<li><a href="#sign-language-retrieval">Sign Language Retrieval</a></li>
<li><a href="#fingerspelling-1">Fingerspelling</a></li>
<li><a href="#pretraining-and-representation-learning">Pretraining and Representation-learning</a></li>
</ul></li>
<li><a href="#annotation-tools">Annotation Tools</a></li>
<li><a href="#resources">Resources</a>
<ul>
<li><a href="#collect-real-world-data">Collect Real-World Data</a></li>
<li><a href="#practice-deaf-collaboration">Practice Deaf Collaboration</a></li>
<li><a href="#downloading">Downloading</a></li>
</ul></li>
<li><a href="#list-of-datasets">List of Datasets</a></li>
<li><a href="#other-resources">Other Resources</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#references">References</a></li>
</ul></nav>
        </aside>
        
        <main class="bd-main order-1 col-12 col-lg-10"
            data-bs-spy="scroll" data-bs-target="#navbar" data-bs-smooth-scroll="true" tabindex="0"><!--
            --><p style="text-align: center;overflow:visible">
<iframe src="https://sign.mt/?embed=&spl=en&sil=ase&text=Hello%20world!" allow="camera;microphone" title="sign.mt translation demo"></iframe>
Try <a href="https://sign.mt">sign translate</a> to experience state-of-the art-sign language translation technology.
</p>
<h2 id="introduction">Introduction</h2>
<p>Signed languages (also known as sign languages) are languages that use the visual-gestural modality to convey meaning through manual articulations in combination with non-manual elements like the face and body. They serve as the primary means of communication for numerous deaf and hard-of-hearing individuals. Similar to spoken languages, signed languages are natural languages governed by a set of linguistic rules <span class="citation" data-cites="sandler2006sign">(Sandler and Lillo-Martin <a href="#ref-sandler2006sign" role="doc-biblioref">2006</a>)</span>, both emerging through an abstract, protracted aging process and evolving without deliberate meticulous planning. Signed languages are not universal or mutually intelligible, despite often having striking similarities among them. They are also distinct from spoken languages—i.e., American Sign Language (ASL) is not a visual form of English but its own unique language.</p>
<p>Sign Language Processing <span class="citation" data-cites="bragg2019sign yin-etal-2021-including">(Bragg et al. <a href="#ref-bragg2019sign" role="doc-biblioref">2019</a>; Yin et al. <a href="#ref-yin-etal-2021-including" role="doc-biblioref">2021</a>)</span> is an emerging field of artificial intelligence concerned with the automatic processing and analysis of sign language content. While research has focused more on the visual aspects of signed languages, it is a subfield of both Natural Language Processing (NLP) and Computer Vision (CV). Challenges in sign language processing often include machine translation of sign language videos into spoken language text (sign language translation), from spoken language text (sign language production), or sign language recognition for sign language understanding.</p>
<p>Unfortunately, the latest advances in language-based artificial intelligence, like machine translation and personal assistants, expect a spoken language input (text or transcribed speech), excluding around 200 to 300 different signed languages <span class="citation" data-cites="un2022">(United Nations <a href="#ref-un2022" role="doc-biblioref">2022</a>)</span> and up to 70 million deaf people <span class="citation" data-cites="who2021 wfd2022">(World Health Organization <a href="#ref-who2021" role="doc-biblioref">2021</a>; World Federation of the Deaf <a href="#ref-wfd2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>Throughout history, Deaf communities fought for the right to learn and use signed languages and for the public recognition of signed languages as legitimate ones. Indeed, signed languages are sophisticated communication modalities, at least as capable as spoken languages in all aspects, both linguistic and social. However, in a predominantly oral society, deaf people are constantly encouraged to use spoken languages through lip-reading or text-based communication. The exclusion of signed languages from modern language technologies further suppresses signing in favor of spoken languages. This exclusion disregards the preferences of the Deaf communities who strongly prefer to communicate in signed languages both online and for in-person day-to-day interactions, among themselves and when interacting with spoken language communities <span class="citation" data-cites="padden1988deaf glickman2018language">(C. A. Padden and Humphries <a href="#ref-padden1988deaf" role="doc-biblioref">1988</a>; Glickman and Hall <a href="#ref-glickman2018language" role="doc-biblioref">2018</a>)</span>. Thus, it is essential to make signed languages accessible.</p>
<p>To date, a large amount of research on Sign Language Processing (SLP) has been focused on the visual aspect of signed languages, led by the Computer Vision (CV) community, with little NLP involvement. This focus is not unreasonable, given that a decade ago, we lacked adequate CV tools to process videos for further linguistic analyses. However, similar to spoken languages, signed languages are fully-fledged systems exhibiting all the fundamental characteristics of natural languages, and existing SLP techniques do not adequately address or leverage the linguistic structure of signed languages. Signed languages introduce novel challenges for NLP due to their visual-gestural modality, simultaneity, spatial coherence, and lack of written form. The lack of a written form makes the spoken language processing pipelines - which often start with audio transcription before processing - incompatible with signed languages, forcing researchers to work directly on the raw video signal.</p>
<p>Furthermore, SLP is not only intellectually appealing but also an important research area with significant potential to benefit signing communities. Beneficial applications enabled by signed language technologies include improved documentation of endangered sign languages; educational tools for sign language learners; tools for query and retrieval of information from signed language videos; personal assistants that react to signed languages; real-time automatic sign language interpretations; and more. Needless to say, in addressing this research area, researchers should work <em>alongside</em> and <em>under the direction of</em> deaf communities, and to benefit the signing communities’ interest above all <span class="citation" data-cites="harris2009research">(Harris, Holmes, and Mertens <a href="#ref-harris2009research" role="doc-biblioref">2009</a>)</span>.</p>
<p>In this work, we describe the different representations used for sign language processing, as well as survey the various tasks and recent advances on them. We also make a comprehensive list of existing datasets and make the ones available easy to load using a simple and standardized interface.</p>
<h3 id="brief-history-of-signed-languages-and-deaf-culture">(Brief) History of Signed Languages and Deaf Culture</h3>
<p>Throughout modern history, spoken languages were dominant, so much so that signed languages struggled to be recognized as languages in their own right, and educators developed misconceptions that signed language acquisition might hinder the development of speech skills. For example, in 1880, a large international conference of deaf educators called the “Second International Congress on Education of the Deaf” banned teaching signed languages, favoring speech therapy instead. It was not until the seminal work on American Sign Language (ASL) by <span class="citation" data-cites="writing:stokoe1960sign">Stokoe Jr (<a href="#ref-writing:stokoe1960sign" role="doc-biblioref">1960</a>)</span> that signed languages started gaining recognition as natural, independent, and well-defined languages, which inspired other researchers to further explore signed languages as a research area. Nevertheless, antiquated attitudes that placed less importance on signed languages continue to inflict harm and subject many to linguistic neglect <span class="citation" data-cites="humphries2016avoiding">(Humphries et al. <a href="#ref-humphries2016avoiding" role="doc-biblioref">2016</a>)</span>. Several studies have shown that deaf children raised solely with spoken languages do not gain enough access to a first language during their critical period of language acquisition <span class="citation" data-cites="murray2020importance">(Murray, Hall, and Snoddon <a href="#ref-murray2020importance" role="doc-biblioref">2020</a>)</span>. This language deprivation can lead to life-long consequences on the cognitive, linguistic, socio-emotional, and academic development of the deaf <span class="citation" data-cites="hall2017language">(Hall, Levin, and Anderson <a href="#ref-hall2017language" role="doc-biblioref">2017</a>)</span>.</p>
<p>Signed languages are the primary languages of communication for the Deaf<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and are at the heart of Deaf communities. In the past, the failure to recognize signed languages as fully-fledged natural language systems in their own right has had detrimental effects, and in an increasingly digitized world, NLP research should strive to enable a world in which all people, including the Deaf, have access to languages that fit their lived experience.</p>
<h2 id="sign-language-linguistics-overview">Sign Language Linguistics Overview</h2>
<p>Signed languages consist of phonological, morphological, syntactic, and semantic levels of structure that fulfill the same social, cognitive, and communicative purposes as other natural languages. While spoken languages primarily channel the oral-auditory modality, signed languages use the visual-gestural modality, relying on the signer’s face, hands, body, and space around them to create distinctions in meaning. We present the linguistic features of signed languages<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that researchers must consider during their modeling.</p>
<section id="phonology" class="unnumbered">
<h6 class="unnumbered">Phonology</h6>
<p>Signs are composed of minimal units that combine manual features such as hand configuration, palm orientation, placement, contact, path movement, local movement, as well as non-manual features including eye aperture, head movement, and torso positioning <span class="citation" data-cites="liddell1989american johnson2011toward brentari2011sign sandler2012phonological">(Liddell and Johnson <a href="#ref-liddell1989american" role="doc-biblioref">1989</a>; Johnson and Liddell <a href="#ref-johnson2011toward" role="doc-biblioref">2011</a>; Brentari <a href="#ref-brentari2011sign" role="doc-biblioref">2011</a>; Sandler <a href="#ref-sandler2012phonological" role="doc-biblioref">2012</a>)</span>. Not all possible phonemes are realized in both signed and spoken languages, and inventories of two languages’ phonemes/features may not overlap completely. Different languages are also subject to rules for the allowed combinations of features.</p>
</section>
<section id="simultaneity" class="unnumbered">
<h6 class="unnumbered">Simultaneity</h6>
<p>Though an ASL sign takes about twice as long to produce than an English word, the rates of transmission of information between the two languages are similar <span class="citation" data-cites="bellugi1972comparison">(Bellugi and Fischer <a href="#ref-bellugi1972comparison" role="doc-biblioref">1972</a>)</span>. One way signed languages compensate for the slower production rate of signs is through simultaneity: Signed languages use multiple visual cues to convey different information simultaneously <span class="citation" data-cites="sandler2012phonological">(Sandler <a href="#ref-sandler2012phonological" role="doc-biblioref">2012</a>)</span>. For example, the signer may produce the sign for “cup” on one hand while simultaneously pointing to the actual cup with the other to express “that cup.” Similarly to tone in spoken languages, the face and torso can convey additional affective information <span class="citation" data-cites="liddell2003grammar johnston2007australian">(Liddell and others <a href="#ref-liddell2003grammar" role="doc-biblioref">2003</a>; Johnston and Schembri <a href="#ref-johnston2007australian" role="doc-biblioref">2007</a>)</span>. Facial expressions can modify adjectives, adverbs, and verbs; a head shake can negate a phrase or sentence; eye direction can help indicate referents.</p>
</section>
<section id="referencing" class="unnumbered">
<h6 class="unnumbered">Referencing</h6>
<p>The signer can introduce referents in discourse either by pointing to their actual locations in space or by assigning a region in the signing space to a non-present referent and by pointing to this region to refer to it <span class="citation" data-cites="rathmann2011featural schembri2018indicating">(Rathmann and Mathur <a href="#ref-rathmann2011featural" role="doc-biblioref">2011</a>; Schembri, Cormier, and Fenlon <a href="#ref-schembri2018indicating" role="doc-biblioref">2018</a>)</span>. Signers can also establish relations between referents grounded in signing space by using directional signs or embodying the referents using body shift or eye gaze <span class="citation" data-cites="dudis2004body liddell1998gesture">(Dudis <a href="#ref-dudis2004body" role="doc-biblioref">2004</a>; Liddell and Metzger <a href="#ref-liddell1998gesture" role="doc-biblioref">1998</a>)</span>. Spatial referencing also impact morphology when the directionality of a verb depends on the location of the reference to its subject and/or object <span class="citation" data-cites="de2008pointing fenlon2018modification">(Beuzeville <a href="#ref-de2008pointing" role="doc-biblioref">2008</a>; Fenlon, Schembri, and Cormier <a href="#ref-fenlon2018modification" role="doc-biblioref">2018</a>)</span>: For example, a directional verb can move from its subject’s location and end at its object’s location. While the relation between referents and verbs in spoken language is more arbitrary, referent relations are usually grounded in signed languages. The visual space is heavily exploited to make referencing clear.</p>
<p>Another way anaphoric entities are referenced in sign language is by using classifiers or depicting signs <span class="citation" data-cites="supalla1986classifier wilcox2004rethinking roy2011discourse">(Supalla <a href="#ref-supalla1986classifier" role="doc-biblioref">1986</a>; Wilcox and Hafer <a href="#ref-wilcox2004rethinking" role="doc-biblioref">2004</a>; Roy <a href="#ref-roy2011discourse" role="doc-biblioref">2011</a>)</span> that help describe the characteristics of the referent. Classifiers are typically one-handed signs that do not have a particular location or movement assigned to them, or derive features from meaningful discourse <span class="citation" data-cites="liddell2003grammar">(Liddell and others <a href="#ref-liddell2003grammar" role="doc-biblioref">2003</a>)</span>, so they can be used to convey how the referent relates to other entities, describe its movement, and give more details. For example, to tell about a car swerving and crashing, one might use the hand classifier for a vehicle, move it to indicate swerving, and crash it with another entity in space.</p>
<p>To quote someone other than oneself, signers perform <em>role shift</em> <span class="citation" data-cites="cormier2015rethinking">(Cormier, Smith, and Sevcikova-Sehyr <a href="#ref-cormier2015rethinking" role="doc-biblioref">2015</a>)</span>, where they may physically shift in space to mark the distinction and take on some characteristics of the people they represent. For example, to recount a dialogue between a taller and a shorter person, the signer may shift to one side and look up when taking the shorter person’s role, shift to the other side and look down when taking the taller person’s role.</p>
</section>
<section id="fingerspelling" class="unnumbered">
<h6 class="unnumbered">Fingerspelling</h6>
<p>Fingerspelling results from language contact between a signed language and a surrounding spoken language written form <span class="citation" data-cites="battison1978lexical wilcox1992phonetics brentari2001language patrie2011fingerspelled">(Battison <a href="#ref-battison1978lexical" role="doc-biblioref">1978</a>; Wilcox <a href="#ref-wilcox1992phonetics" role="doc-biblioref">1992</a>; Brentari and Padden <a href="#ref-brentari2001language" role="doc-biblioref">2001</a>; Patrie and Johnson <a href="#ref-patrie2011fingerspelled" role="doc-biblioref">2011</a>)</span>. A set of manual gestures correspond with a written orthography or phonetic system. This phenomenon, found in most signed languages, is often used to indicate names or places or new concepts from the spoken language but has often become integrated into the signed languages as another linguistic strategy <span class="citation" data-cites="padden1998asl montemurro2018emphatic">(Padden <a href="#ref-padden1998asl" role="doc-biblioref">1998</a>; Montemurro and Brentari <a href="#ref-montemurro2018emphatic" role="doc-biblioref">2018</a>)</span>.</p>
</section>
<h2 id="sign-language-representations">Sign Language Representations</h2>
<p>Representation is a significant challenge for SLP. Unlike spoken languages, signed languages have no widely adopted written form. As signed languages are conveyed through the visual-gestural modality, video recording is the most straightforward way to capture them. However, as videos include more information than needed for modeling and are expensive to record, store, and transmit, a lower-dimensional representation has been sought after.</p>
<p>The following figure illustrates each signed language representation we will describe below. In this demonstration, we deconstruct the video into its individual frames to exemplify the alignment of the annotations between the video and representations.</p>
<object type="image/svg+xml" data="assets/representation/continuous.pdf#toolbar=0&navpanes=0&scrollbar=0" id="continuous-rep"></object>
<section id="videos" class="unnumbered">
<h6 class="unnumbered">Videos</h6>
<p>are the most straightforward representation of a signed language and can amply incorporate the information conveyed through signing. One major drawback of using videos is their high dimensionality: They usually include more information than needed for modeling and are expensive to store, transmit, and encode. As facial features are essential in sign, anonymizing raw videos remains an open problem, limiting the possibility of making these videos publicly available <span class="citation" data-cites="isard2020approaches">(Isard <a href="#ref-isard2020approaches" role="doc-biblioref">2020</a>)</span>.</p>
</section>
<section id="skeletal-poses" class="unnumbered">
<h6 class="unnumbered">Skeletal Poses</h6>
<p>reduce the visual cues in videos to skeleton-like wireframes or mesh representing the location of joints. This technique has been extensively used in the field of computer vision to estimate human pose from video data, where the goal is to determine the spatial configuration of the body at each point in time. Although high-quality pose estimation can be achieved using motion capture equipment, such methods are often expensive and intrusive. As a result, estimating pose from videos has become the preferred method in recent years <span class="citation" data-cites="pose:pishchulin2012articulated pose:chen2017adversarial pose:cao2018openpose pose:alp2018densepose">(Pishchulin et al. <a href="#ref-pose:pishchulin2012articulated" role="doc-biblioref">2012</a>; Chen et al. <a href="#ref-pose:chen2017adversarial" role="doc-biblioref">2017</a>; Cao et al. <a href="#ref-pose:cao2018openpose" role="doc-biblioref">2019</a>; Güler, Neverova, and Kokkinos <a href="#ref-pose:alp2018densepose" role="doc-biblioref">2018</a>)</span>. Compared to video representations, accurate skeletal poses have a lower complexity and provide a semi-anonymized representation of the human body, while observing relatively low information loss. However, they remain a continuous, multidimensional representation that is not adapted to most NLP models.</p>
</section>
<section id="written-notation-systems" class="unnumbered">
<h6 class="unnumbered">Written notation systems</h6>
<p>represent signs as discrete visual features. Some systems are written linearly, and others use graphemes in two dimensions. While various universal <span class="citation" data-cites="writing:sutton1990lessons writing:prillwitz1990hamburg">(Sutton <a href="#ref-writing:sutton1990lessons" role="doc-biblioref">1990</a>; Prillwitz and Zienert <a href="#ref-writing:prillwitz1990hamburg" role="doc-biblioref">1990</a>)</span> and language-specific notation systems <span class="citation" data-cites="writing:stokoe1960sign writing:kakumasu1968urubu writing:bergman1977tecknad">(Stokoe Jr <a href="#ref-writing:stokoe1960sign" role="doc-biblioref">1960</a>; Kakumasu <a href="#ref-writing:kakumasu1968urubu" role="doc-biblioref">1968</a>; Bergman <a href="#ref-writing:bergman1977tecknad" role="doc-biblioref">1977</a>)</span> have been proposed, no writing system has been adopted widely by any sign language community, and the lack of standards hinders the exchange and unification of resources and applications between projects. The figure above depicts two universal notation systems: SignWriting <span class="citation" data-cites="writing:sutton1990lessons">(Sutton <a href="#ref-writing:sutton1990lessons" role="doc-biblioref">1990</a>)</span>, a two-dimensional pictographic system, and HamNoSys <span class="citation" data-cites="writing:prillwitz1990hamburg">(Prillwitz and Zienert <a href="#ref-writing:prillwitz1990hamburg" role="doc-biblioref">1990</a>)</span>, a linear stream of graphemes designed to be machine-readable.</p>
</section>
<section id="glosses" class="unnumbered">
<h6 class="unnumbered">Glosses</h6>
<p>are the transcription of signed languages sign-by-sign, with each sign having a unique semantic identifier. While various sign language corpus projects have provided guidelines for gloss annotation <span class="citation" data-cites="mesch2015gloss johnston2016auslan konrad2018public">(Mesch and Wallin <a href="#ref-mesch2015gloss" role="doc-biblioref">2015</a>; Johnston and De Beuzeville <a href="#ref-johnston2016auslan" role="doc-biblioref">2016</a>; Konrad et al. <a href="#ref-konrad2018public" role="doc-biblioref">2018</a>)</span>, a standardized gloss annotation protocol has yet to be established. Linear gloss annotations have been criticized for their imprecise representation of signed language. These annotations fail to capture all the information expressed simultaneously through different cues, such as body posture, eye gaze, or spatial relations, leading to a loss of information that can significantly affect downstream performance on SLP tasks <span class="citation" data-cites="yin-read-2020-better muller-etal-2023-considerations">(Yin and Read <a href="#ref-yin-read-2020-better" role="doc-biblioref">2020</a>; Müller et al. <a href="#ref-muller-etal-2023-considerations" role="doc-biblioref">2023</a>)</span>.</p>
<p><span class="citation" data-cites="muller-etal-2023-considerations">Müller et al. (<a href="#ref-muller-etal-2023-considerations" role="doc-biblioref">2023</a>)</span> conduct an extensive review of the use of glosses in sign language translation research and make the following recommendations for research using glosses:</p>
<ul>
<li>Demonstrate awareness of limitations of gloss approaches and explicitly discuss them.</li>
<li>Focus on datasets beyond RWTH-PHOENIX-Weather-2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span>. Openly discuss the limited size and linguistic domain of this dataset.</li>
<li>Use metrics that are well-established in MT. If BLEU <span class="citation" data-cites="papineni-etal-2002-bleu">(Papineni et al. <a href="#ref-papineni-etal-2002-bleu" role="doc-biblioref">2002</a>)</span> is used, compute it with SacreBLEU <span class="citation" data-cites="post-2018-call-sacrebleu">(Post <a href="#ref-post-2018-call-sacrebleu" role="doc-biblioref">2018</a>)</span>, report metric signatures and disable internal tokenization for gloss outputs. Do not compare to scores produced with a different or unknown evaluation procedure.</li>
<li>Given that glossing is corpus-specific, process glosses in a corpus-specific way, informed by transcription conventions.</li>
<li>Optimize gloss translation baselines with methods shown to be effective for low-resource MT.</li>
</ul>
<p>The following table additionally exemplifies the various representations for more isolated signs. For this example, we use SignWriting as the notation system. Note that the same sign might have two unrelated glosses, and the same gloss might have multiple valid spoken language translations.</p>
<div id="formats-table" class="table">
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Video</th>
<th style="text-align: center;">Pose Estimation</th>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">Gloss</th>
<th style="text-align: center;">English Translation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="assets/videos/original/asl_house.gif" style="width:2.5cm" alt="ASL HOUSE" /></td>
<td style="text-align: center;"><img src="assets/videos/pose/asl_house.gif" style="width:2.5cm" alt="ASL HOUSE" /></td>
<td style="text-align: center;"><img src="assets/writing/house.png" style="width:1cm" alt="HOUSE" /></td>
<td style="text-align: center;">HOUSE</td>
<td style="text-align: center;">House</td>
</tr>
<tr class="even">
<td style="text-align: center;"><img src="assets/videos/original/asl_wrong_what.gif" style="width:2.5cm" alt="ASL WRONG-WHAT" /></td>
<td style="text-align: center;"><img src="assets/videos/pose/asl_wrong_what.gif" style="width:2.5cm" alt="ASL WRONG-WHAT" /></td>
<td style="text-align: center;"><img src="assets/writing/wrong_what.png" style="width:0.7cm" alt="WRONG-WHAT" /></td>
<td style="text-align: center;">WRONG-WHAT</td>
<td style="text-align: center;">What’s the matter?<br> What’s wrong?</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><img src="assets/videos/original/asl_different.gif" style="width:2.5cm" alt="ASL DIFFERENT" /></td>
<td style="text-align: center;"><img src="assets/videos/pose/asl_different.gif" style="width:2.5cm" alt="ASL DIFFERENT" /></td>
<td style="text-align: center;"><img src="assets/writing/different.png" style="width:1cm" alt="DIFFERENT" /></td>
<td style="text-align: center;">DIFFERENT<br> BUT</td>
<td style="text-align: center;">Different<br> But</td>
</tr>
</tbody>
</table>
</div>
</section>
<h2 id="tasks">Tasks</h2>
<p>So far, the computer vision community has primarily led the SLP research to focus on processing the visual features in signed language videos. As a result, current SLP methods do not fully address the linguistic complexity of signed languages. We survey common SLP tasks and current methods’ limitations, drawing on signed languages’ linguistic theories.</p>
<h3 id="sign-language-detection">Sign Language Detection</h3>
<p>Sign language detection <span class="citation" data-cites="detection:borg2019sign detection:moryossef2020real detection:pal2023importance">(Borg and Camilleri <a href="#ref-detection:borg2019sign" role="doc-biblioref">2019</a>; Moryossef et al. <a href="#ref-detection:moryossef2020real" role="doc-biblioref">2020</a>; Pal et al. <a href="#ref-detection:pal2023importance" role="doc-biblioref">2023</a>)</span> is the binary classification task of determining whether signing activity is present in a given video frame. A similar task in spoken languages is voice activity detection (VAD) <span class="citation" data-cites="sohn1999statistical ramirez2004efficient">(Sohn, Kim, and Sung <a href="#ref-sohn1999statistical" role="doc-biblioref">1999</a>; Ramırez et al. <a href="#ref-ramirez2004efficient" role="doc-biblioref">2004</a>)</span>, the detection of when a human voice is used in an audio signal. As VAD methods often rely on speech-specific representations such as spectrograms, they are not necessarily applicable to videos.</p>
<p><span class="citation" data-cites="detection:borg2019sign">Borg and Camilleri (<a href="#ref-detection:borg2019sign" role="doc-biblioref">2019</a>)</span> introduced the classification of frames taken from YouTube videos as either signing or not signing. They took a spatial and temporal approach based on VGG-16 <span class="citation" data-cites="simonyan2015very">(Simonyan and Zisserman <a href="#ref-simonyan2015very" role="doc-biblioref">2015</a>)</span> CNN to encode each frame and used a Gated Recurrent Unit (GRU) <span class="citation" data-cites="cho2014learning">(Cho et al. <a href="#ref-cho2014learning" role="doc-biblioref">2014</a>)</span> to encode the sequence of frames in a window of 20 frames at 5fps. In addition to the raw frame, they either encoded optical-flow history, aggregated motion history, or frame difference.</p>
<p><span class="citation" data-cites="detection:moryossef2020real">Moryossef et al. (<a href="#ref-detection:moryossef2020real" role="doc-biblioref">2020</a>)</span> improved upon their method by performing sign language detection in real time. They identified that sign language use involves movement of the body and, as such, designed a model that works on top of estimated human poses rather than directly on the video signal. They calculated the optical flow norm of every joint detected on the body and applied a shallow yet effective contextualized model to predict for every frame whether the person is signing or not.</p>
<p>While these recent detection models achieve high performance, we need well-annotated data that include interference and distractions with non-signing instances for proper real-world evaluation. <span class="citation" data-cites="detection:pal2023importance">Pal et al. (<a href="#ref-detection:pal2023importance" role="doc-biblioref">2023</a>)</span> conducted a detailed analysis of the impact of signer overlap between the training and test sets on two sign detection benchmark datasets (Signing in the Wild <span class="citation" data-cites="detection:borg2019sign">(Borg and Camilleri <a href="#ref-detection:borg2019sign" role="doc-biblioref">2019</a>)</span> and the DGS Corpus <span class="citation" data-cites="dataset:hanke-etal-2020-extending">(Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span>) used by <span class="citation" data-cites="detection:borg2019sign">Borg and Camilleri (<a href="#ref-detection:borg2019sign" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="detection:moryossef2020real">Moryossef et al. (<a href="#ref-detection:moryossef2020real" role="doc-biblioref">2020</a>)</span>. By comparing the accuracy with and without overlap, they noticed a relative decrease in performance for signers not present during training. As a result, they suggested new dataset partitions that eliminate overlap between train and test sets and facilitate a more accurate evaluation of performance.</p>
<h3 id="sign-language-identification">Sign Language Identification</h3>
<p>Sign language identification <span class="citation" data-cites="identification:gebre2013automatic identification:monteiro2016detecting">(Gebre, Wittenburg, and Heskes <a href="#ref-identification:gebre2013automatic" role="doc-biblioref">2013</a>; Monteiro et al. <a href="#ref-identification:monteiro2016detecting" role="doc-biblioref">2016</a>)</span> classifies which signed language is used in a given video.</p>
<p><span class="citation" data-cites="identification:gebre2013automatic">Gebre, Wittenburg, and Heskes (<a href="#ref-identification:gebre2013automatic" role="doc-biblioref">2013</a>)</span> found that a simple random-forest classifier utilizing the distribution of phonemes can distinguish between British Sign Language (BSL) and Greek Sign Language (ENN) with a 95% F1 score. This finding is further supported by <span class="citation" data-cites="identification:monteiro2016detecting">Monteiro et al. (<a href="#ref-identification:monteiro2016detecting" role="doc-biblioref">2016</a>)</span>, which, based on activity maps in signing space, manages to differentiate between British Sign Language and French Sign Language (Langue des Signes Française, LSF) with a 98% F1 score in videos with static backgrounds, and between American Sign Language and British Sign Language, with a 70% F1 score for videos mined from popular video-sharing sites. The authors attribute their success mainly to the different fingerspelling systems, which are two-handed in the case of BSL and one-handed in the case of ASL and LSF.</p>
<p>Although these pairwise classification results seem promising, better models would be needed for classifying from a large set of signed languages. These methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences <span class="citation" data-cites="mckee2000lexical kimmelman2014information ferreira1984similarities shroyer1984signs">(McKee and Kennedy <a href="#ref-mckee2000lexical" role="doc-biblioref">2000</a>; Kimmelman <a href="#ref-kimmelman2014information" role="doc-biblioref">2014</a>; Ferreira-Brito <a href="#ref-ferreira1984similarities" role="doc-biblioref">1984</a>; Shroyer and Shroyer <a href="#ref-shroyer1984signs" role="doc-biblioref">1984</a>)</span> which have not been explored for this task.</p>
<h3 id="sign-language-segmentation">Sign Language Segmentation</h3>
<p>Segmentation consists of detecting the frame boundaries for signs or phrases in videos to divide them into meaningful units. While the most canonical way of dividing a spoken language text is into a linear sequence of words, due to the simultaneity of sign language, the notion of a sign language “word” is ill-defined, and sign language cannot be fully linearly modeled.</p>
<p>Current methods resort to segmenting units loosely mapped to signed language units <span class="citation" data-cites="segmentation:santemiz2009automatic segmentation:farag2019learning segmentation:bull2020automatic segmentation:renz2021signa segmentation:renz2021signb segmentation:bull2021aligning">(Santemiz et al. <a href="#ref-segmentation:santemiz2009automatic" role="doc-biblioref">2009</a>; Farag and Brock <a href="#ref-segmentation:farag2019learning" role="doc-biblioref">2019</a>; Bull, Gouiffès, and Braffort <a href="#ref-segmentation:bull2020automatic" role="doc-biblioref">2020</a>; Renz, Stache, et al. <a href="#ref-segmentation:renz2021signa" role="doc-biblioref">2021</a><a href="#ref-segmentation:renz2021signa" role="doc-biblioref">a</a>, <a href="#ref-segmentation:renz2021signb" role="doc-biblioref">2021</a><a href="#ref-segmentation:renz2021signb" role="doc-biblioref">b</a>; Bull et al. <a href="#ref-segmentation:bull2021aligning" role="doc-biblioref">2021</a>)</span> and do not explicitly leverage reliable linguistic predictors of sentence boundaries such as prosody in signed languages (i.e., pauses, extended sign duration, facial expressions) <span class="citation" data-cites="sandler2010prosody ormel2012prosodic">(Sandler <a href="#ref-sandler2010prosody" role="doc-biblioref">2010</a>; Ormel and Crasborn <a href="#ref-ormel2012prosodic" role="doc-biblioref">2012</a>)</span>. <span class="citation" data-cites="segmentation:de-sisto-etal-2021-defining">De Sisto et al. (<a href="#ref-segmentation:de-sisto-etal-2021-defining" role="doc-biblioref">2021</a>)</span> call for a better understanding of sign language structure, which they believe is the necessary ground for the design and development of sign language recognition and segmentation methodologies.</p>
<p><span class="citation" data-cites="segmentation:santemiz2009automatic">Santemiz et al. (<a href="#ref-segmentation:santemiz2009automatic" role="doc-biblioref">2009</a>)</span> automatically extracted isolated signs from continuous signing by aligning the sequences obtained via speech recognition, modeled by Dynamic Time Warping (DTW) and Hidden Markov Models (HMMs) approaches.</p>
<p><span class="citation" data-cites="segmentation:farag2019learning">Farag and Brock (<a href="#ref-segmentation:farag2019learning" role="doc-biblioref">2019</a>)</span> used a random forest classifier to distinguish frames containing signs in Japanese Sign Language based on the composition of spatio-temporal angular and distance features between domain-specific pairs of joint segments.</p>
<p><span class="citation" data-cites="segmentation:bull2020automatic">Bull, Gouiffès, and Braffort (<a href="#ref-segmentation:bull2020automatic" role="doc-biblioref">2020</a>)</span> segmented French Sign Language into segments corresponding to subtitle units by relying on the alignment between subtitles and sign language videos, leveraging a spatio-temporal graph convolutional network (ST-GCN; <span class="citation" data-cites="Yu2017SpatioTemporalGC">Yu, Yin, and Zhu (<a href="#ref-Yu2017SpatioTemporalGC" role="doc-biblioref">2018</a>)</span>) with a BiLSTM on 2D skeleton data.</p>
<p><span class="citation" data-cites="segmentation:renz2021signa">Renz, Stache, et al. (<a href="#ref-segmentation:renz2021signa" role="doc-biblioref">2021</a><a href="#ref-segmentation:renz2021signa" role="doc-biblioref">a</a>)</span> located temporal boundaries between signs in continuous sign language videos by employing 3D convolutional neural network representations with iterative temporal segment refinement to resolve ambiguities between sign boundary cues. <span class="citation" data-cites="segmentation:renz2021signb">Renz, Stache, et al. (<a href="#ref-segmentation:renz2021signb" role="doc-biblioref">2021</a><a href="#ref-segmentation:renz2021signb" role="doc-biblioref">b</a>)</span> further proposed the Changepoint-Modulated Pseudo-Labelling (CMPL) algorithm to solve the problem of source-free domain adaptation.</p>
<p><span class="citation" data-cites="segmentation:bull2021aligning">Bull et al. (<a href="#ref-segmentation:bull2021aligning" role="doc-biblioref">2021</a>)</span> presented a Transformer-based approach to segment sign language videos and align them with subtitles simultaneously, encoding subtitles by BERT <span class="citation" data-cites="devlin-etal-2019-bert">(Devlin et al. <a href="#ref-devlin-etal-2019-bert" role="doc-biblioref">2019</a>)</span> and videos by CNN video representations.</p>
<p><span class="citation" data-cites="segmentation:moryossef-etal-2023-linguistically">Moryossef, Jiang, et al. (<a href="#ref-segmentation:moryossef-etal-2023-linguistically" role="doc-biblioref">2023</a>)</span> presented a method motivated by linguistic cues observed in sign language corpora, such as prosody (pauses, pace, etc) and handshape changes. They also find that using BIO, an annotation scheme that notes the beginning, inside and outside, makes a significant difference over previous ones that only note IO (inside or outside). They find that including optical flow and 3D hand normalization helps with out-of-domain generalization and other signed languages as well.</p>
<!-- @segmentation:de-sisto-etal-2021-defining introduce a proposal for mapping segments to meaning in the form of an agglomerate of lexical and non-lexical information. -->
<h3 id="sign-language-recognition-translation-and-production">Sign Language Recognition, Translation, and Production</h3>
<p>Sign language translation (SLT) commonly refers to the translation of signed language to spoken language <span class="citation" data-cites="de2022machine muller-etal-2022-findings">(De Coster et al. <a href="#ref-de2022machine" role="doc-biblioref">2022</a>; Müller et al. <a href="#ref-muller-etal-2022-findings" role="doc-biblioref">2022</a>)</span>. Sign language production is the reverse process of producing a sign language video from spoken language text. Sign language recognition (SLR) <span class="citation" data-cites="adaloglou2020comprehensive">(Adaloglou et al. <a href="#ref-adaloglou2020comprehensive" role="doc-biblioref">2020</a>)</span> detects and labels signs from a video, either on isolated <span class="citation" data-cites="dataset:imashev2020dataset dataset:sincan2020autsl">(Imashev et al. <a href="#ref-dataset:imashev2020dataset" role="doc-biblioref">2020</a>; Sincan and Keles <a href="#ref-dataset:sincan2020autsl" role="doc-biblioref">2020</a>)</span> or continuous <span class="citation" data-cites="cui2017recurrent cihan2018neural camgoz2020sign">(Cui, Liu, and Zhang <a href="#ref-cui2017recurrent" role="doc-biblioref">2017</a>; Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>; N. C. Camgöz et al. <a href="#ref-camgoz2020sign" role="doc-biblioref">2020</a><a href="#ref-camgoz2020sign" role="doc-biblioref">b</a>)</span> signs.</p>
<p>In the following graph, we can see a fully connected pentagon where each node is a single data representation, and each directed edge represents the task of converting one data representation to another.</p>
<p>We split the graph into two:</p>
<ul>
<li>Every edge to the left, on the orange background, represents a task in computer vision. These tasks are inherently language-agnostic; thus, they generalize between signed languages.</li>
<li>Every edge to the right, on the blue background, represents a task in natural language processing. These tasks are sign language-specific, requiring a specific sign language lexicon or spoken language tokens.</li>
<li>Every edge on both backgrounds represents a task requiring a combination of computer vision and natural language processing.</li>
</ul>
<div class="tasks">
    <span style="font-weight: bold;">Language Agnostic Tasks</span>
    <span style="font-weight: bold;float:right">Language Specific Tasks</span>
    <img src="assets/tasks/tasks.svg" alt="Sign language tasks graph" />
</div>
<p>There are 20 tasks conceptually defined by this graph, with varying amounts of previous research. Every path between two nodes might or might not be valid, depending on how lossy the tasks in the path are.</p>
<hr />
<h4 id="video-to-pose">Video-to-Pose</h4>
<p>Video-to-Pose—commonly known as pose estimation—is the task of detecting human figures in images and videos, so that one could determine, for example, where someone’s elbow shows up in an image. It was shown that the face pose correlates with facial non-manual features like head direction <span class="citation" data-cites="vogler2005analysis">(Vogler and Goldenstein <a href="#ref-vogler2005analysis" role="doc-biblioref">2005</a>)</span>.</p>
<p>This area has been thoroughly researched <span class="citation" data-cites="pose:pishchulin2012articulated pose:chen2017adversarial pose:cao2018openpose pose:alp2018densepose">(Pishchulin et al. <a href="#ref-pose:pishchulin2012articulated" role="doc-biblioref">2012</a>; Chen et al. <a href="#ref-pose:chen2017adversarial" role="doc-biblioref">2017</a>; Cao et al. <a href="#ref-pose:cao2018openpose" role="doc-biblioref">2019</a>; Güler, Neverova, and Kokkinos <a href="#ref-pose:alp2018densepose" role="doc-biblioref">2018</a>)</span> with objectives varying from predicting 2D / 3D poses to a selection of a small specific set of landmarks or a dense mesh of a person.</p>
<p>OpenPose <span class="citation" data-cites="pose:cao2018openpose pose:simon2017hand pose:cao2017realtime pose:wei2016cpm">(Cao et al. <a href="#ref-pose:cao2018openpose" role="doc-biblioref">2019</a>; Simon et al. <a href="#ref-pose:simon2017hand" role="doc-biblioref">2017</a>; Cao et al. <a href="#ref-pose:cao2017realtime" role="doc-biblioref">2017</a>; Wei et al. <a href="#ref-pose:wei2016cpm" role="doc-biblioref">2016</a>)</span> is the first multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) in 2D on single images. While their model can estimate the full pose directly from an image in a single inference, they also suggest a pipeline approach where they first estimate the body pose and then independently estimate the hands and face pose by acquiring higher resolution crops around those areas. Building on the slow pipeline approach, a single network whole body OpenPose model has been proposed <span class="citation" data-cites="pose:hidalgo2019singlenetwork">(Martinez et al. <a href="#ref-pose:hidalgo2019singlenetwork" role="doc-biblioref">2019</a>)</span>, which is faster and more accurate for the case of obtaining all keypoints. With multiple recording angles, OpenPose also offers keypoint triangulation to reconstruct the pose in 3D.</p>
<p>DensePose <span class="citation" data-cites="pose:alp2018densepose">(Güler, Neverova, and Kokkinos <a href="#ref-pose:alp2018densepose" role="doc-biblioref">2018</a>)</span> takes a different approach. Instead of classifying for every keypoint which pixel is most likely, they suggest a method similar to semantic segmentation, for each pixel to classify which body part it belongs to. Then, for each pixel, knowing the body part, they predict where that pixel is on the body part relative to a 2D projection of a representative body model. This approach results in the reconstruction of the full-body mesh and allows sampling to find specific keypoints similar to OpenPose.</p>
<p>However, 2D human poses might not be sufficient to fully understand the position and orientation of landmarks in space, and applying pose estimation per frame disregards video temporal movement information into account, especially in cases of rapid movement, which contain motion blur.</p>
<p><span class="citation" data-cites="pose:pavllo20193d">Pavllo et al. (<a href="#ref-pose:pavllo20193d" role="doc-biblioref">2019</a>)</span> developed two methods to convert between 2D poses to 3D poses. The first, a supervised method, was trained to use the temporal information between frames to predict the missing Z-axis. The second is an unsupervised method, leveraging the fact that the 2D poses are merely a projection of an unknown 3D pose and training a model to estimate the 3D pose and back-project to the input 2D poses. This back projection is a deterministic process, applying constraints on the 3D pose encoder. <span class="citation" data-cites="pose:zelinka2020neural">Zelinka and Kanis (<a href="#ref-pose:zelinka2020neural" role="doc-biblioref">2020</a>)</span> followed a similar process and added a constraint for bones to stay of a fixed length between frames.</p>
<p><span class="citation" data-cites="pose:panteleris2018using">Panteleris, Oikonomidis, and Argyros (<a href="#ref-pose:panteleris2018using" role="doc-biblioref">2018</a>)</span> suggest converting the 2D poses to 3D using inverse kinematics (IK), a process taken from computer animation and robotics to calculate the variable joint parameters needed to place the end of a kinematic chain, such as a robot manipulator or animation character’s skeleton, in a given position and orientation relative to the start of the chain. Demonstrating their approach to hand pose estimation, they manually explicitly encode the constraints and limits of each joint, resulting in 26 degrees of freedom. Then, non-linear least-squares minimization fits a 3D model of the hand to the estimated 2D joint positions, recovering the 3D hand pose. This process is similar to the back-projection used by <span class="citation" data-cites="pose:pavllo20193d">Pavllo et al. (<a href="#ref-pose:pavllo20193d" role="doc-biblioref">2019</a>)</span>, except here, no temporal information is being used.</p>
<p>MediaPipe Holistic <span class="citation" data-cites="mediapipe2020holistic">(Grishchenko and Bazarevsky <a href="#ref-mediapipe2020holistic" role="doc-biblioref">2020</a>)</span> attempts to solve 3D pose estimation by taking a similar approach to OpenPose, having a pipeline system to estimate the body, then the face and hands. Unlike OpenPose, the estimated poses are in 3D, and the pose estimator runs in real-time on CPU, allowing for pose-based sign language models on low-powered mobile devices. This pose estimation tool is widely available and built for Android, iOS, C++, Python, and the Web using JavaScript.</p>
<h4 id="pose-to-video">Pose-to-Video</h4>
<p>Pose-to-Video, also known as motion transfer or skeletal animation in the field of robotics and animation, is the conversion of a sequence of poses to a video. This task is the final “rendering” of sign language in a visual modality.</p>
<p><span class="citation" data-cites="pose:chan2019everybody">Chan et al. (<a href="#ref-pose:chan2019everybody" role="doc-biblioref">2019</a>)</span> demonstrated a semi-supervised approach where they took a set of videos, ran pose estimation with OpenPose <span class="citation" data-cites="pose:cao2018openpose">(Cao et al. <a href="#ref-pose:cao2018openpose" role="doc-biblioref">2019</a>)</span>, and learned an image-to-image translation <span class="citation" data-cites="isola2017image">(Isola et al. <a href="#ref-isola2017image" role="doc-biblioref">2017</a>)</span> between the rendered skeleton and the original video. They demonstrated their approach on human dancing, extracting poses from a choreography and rendering any person as if <em>they</em> were dancing. They predicted two consecutive frames for temporally coherent video results and introduced a separate pipeline for a more realistic face synthesis, although still flawed.</p>
<p><span class="citation" data-cites="pose:wang2018vid2vid">Wang et al. (<a href="#ref-pose:wang2018vid2vid" role="doc-biblioref">2018</a>)</span> suggested a similar method using DensePose <span class="citation" data-cites="pose:alp2018densepose">(Güler, Neverova, and Kokkinos <a href="#ref-pose:alp2018densepose" role="doc-biblioref">2018</a>)</span> representations in addition to the OpenPose <span class="citation" data-cites="pose:cao2018openpose">(Cao et al. <a href="#ref-pose:cao2018openpose" role="doc-biblioref">2019</a>)</span> ones. They formalized a different model, with various objectives to optimize for, such as background-foreground separation and temporal coherence by using the previous two timestamps in the input.</p>
<p>Using the method of <span class="citation" data-cites="pose:chan2019everybody">Chan et al. (<a href="#ref-pose:chan2019everybody" role="doc-biblioref">2019</a>)</span> on “Everybody Dance Now”, <span class="citation" data-cites="pose:girocan2020slrtp">Giró-i-Nieto (<a href="#ref-pose:girocan2020slrtp" role="doc-biblioref">2020</a>)</span> asked, “Can Everybody Sign Now?” and investigated if people could understand sign language from automatically generated videos. They conducted a study in which participants watched three types of videos: the original signing videos, videos showing only poses (skeletons), and reconstructed videos with realistic signing. The researchers evaluated the participants’ understanding after watching each type of video. Results revealed a preference for reconstructed videos over skeleton videos. However, the standard video synthesis methods used in the study were not effective enough for clear sign language translation. Participants had trouble understanding the reconstructed videos, suggesting that improvements are needed for better sign language translation in the future.</p>
<p>As a direct response, <span class="citation" data-cites="saunders2020everybody">Saunders, Camgöz, and Bowden (<a href="#ref-saunders2020everybody" role="doc-biblioref">2020</a><a href="#ref-saunders2020everybody" role="doc-biblioref">a</a>)</span> showed that like in <span class="citation" data-cites="pose:chan2019everybody">Chan et al. (<a href="#ref-pose:chan2019everybody" role="doc-biblioref">2019</a>)</span>, where an adversarial loss was added to specifically generate the face, adding a similar loss to the hand generation process yielded high-resolution, more photo-realistic continuous sign language videos. To further improve the hand image synthesis quality, they introduced a keypoint-based loss function to avoid issues caused by motion blur.</p>
<p>In a follow-up paper, <span class="citation" data-cites="anonysign">Saunders, Camgöz, and Bowden (<a href="#ref-anonysign" role="doc-biblioref">2021</a>)</span> introduced the task of Sign Language Video Anonymisation (SLVA) as an automatic method to anonymize the visual appearance of a sign language video while retaining the original sign language content. Using a conditional variational autoencoder framework, they first extracted pose information from the source video to remove the original signer appearance, then generated a photo-realistic sign language video of a novel appearance from the pose sequence. The authors proposed a novel style loss that ensures style consistency in the anonymized sign language videos.</p>
<h5 id="sign-language-avatars">Sign Language Avatars</h5>
<section id="jasigning" class="unnumbered">
<h6 class="unnumbered">JASigning</h6>
<p>is a virtual signing system that generates sign language performances using virtual human characters. This system evolved from the earlier SiGMLSigning system, which was developed during the ViSiCAST <span class="citation" data-cites="bangham2000virtual elliott2000development">(Bangham et al. <a href="#ref-bangham2000virtual" role="doc-biblioref">2000</a>; Elliott et al. <a href="#ref-elliott2000development" role="doc-biblioref">2000</a>)</span> and eSIGN <span class="citation" data-cites="zwitserlood2004synthetic">(Zwitserlood et al. <a href="#ref-zwitserlood2004synthetic" role="doc-biblioref">2004</a>)</span> projects, and later underwent further development as part of the Dicta-Sign project <span class="citation" data-cites="dataset:matthes2012dicta dataset:efthimiou2012sign">(Matthes et al. <a href="#ref-dataset:matthes2012dicta" role="doc-biblioref">2012</a>; Efthimiou et al. <a href="#ref-dataset:efthimiou2012sign" role="doc-biblioref">2012</a>)</span>.</p>
<p>Originally, JASigning relied on Java JNLP apps for standalone use and integration into web pages. However, this approach became outdated due to the lack of support for Java in modern browsers. Consequently, the more recent CWA Signing Avatars (CWASA) system was developed, which is based on HTML5, utilizing JavaScript and WebGL technologies.</p>
<p>SiGML (Signing Gesture Markup Language) <span class="citation" data-cites="elliott2004overview">(Elliott et al. <a href="#ref-elliott2004overview" role="doc-biblioref">2004</a>)</span> is an XML application that enables the transcription of sign language gestures. SiGML builds on HamNoSys, and indeed, one variant of SiGML is essentially an encoding of HamNoSys manual features, accompanied by a representation of non-manual aspects. SiGML is the input notation used by the JASigning applications and web applets. A number of editing tools for SiGML are available, mostly produced by the University of Hamburg.</p>
<p>The system parses the English text into SiGML before mapping it onto a 3D signing avatar that can produce signing. CWASA then uses a large database of pre-defined 3D signing avatar animations, which can be combined to form new sentences. The system includes a 3D editor, allowing users to create custom signing avatars and animations.</p>
</section>
<section id="paula-pauladavidson2006paula" class="unnumbered">
<h6 class="unnumbered">PAULA <span class="citation" data-cites="paula:davidson2006paula">(Davidson <a href="#ref-paula:davidson2006paula" role="doc-biblioref">2006</a>)</span></h6>
<p>is a computer-based sign language avatar, initially developed for teaching sign language to hearing adults. The avatar is a 3D model of a person with a sign vocabulary that is manually animated. It takes an ASL utterance as a stream of glosses, performs syntactic and morphological modifications, decides on the appropriate phonemes and timings, and combines the results into a 3D animation of the avatar. Over the years, several techniques were used to make the avatar look more realistic.</p>
<p>Over the years, several advancements have been made to enhance the realism and expressiveness of the PAULA avatar, such as refining the eyebrow motion to appear more natural <span class="citation" data-cites="paula:wolfe2011linguistics">(Wolfe et al. <a href="#ref-paula:wolfe2011linguistics" role="doc-biblioref">2011</a>)</span>, combining emotion and co-occurring facial nonmanual signals <span class="citation" data-cites="paula:schnepp2012combining paula:schnepp2013generating">(Schnepp et al. <a href="#ref-paula:schnepp2012combining" role="doc-biblioref">2012</a>, <a href="#ref-paula:schnepp2013generating" role="doc-biblioref">2013</a>)</span>, improving smoothness while avoiding robotic movements <span class="citation" data-cites="paula:mcdonald2016automated">(McDonald et al. <a href="#ref-paula:mcdonald2016automated" role="doc-biblioref">2016</a>)</span>, and facilitating simultaneity <span class="citation" data-cites="paula:mcdonald2017improved">(McDonald et al. <a href="#ref-paula:mcdonald2017improved" role="doc-biblioref">2017</a>)</span>. Other developments include interfacing with sign language notation systems like AZee <span class="citation" data-cites="paula:filhol2017synthesizing">(Filhol, McDonald, and Wolfe <a href="#ref-paula:filhol2017synthesizing" role="doc-biblioref">2017</a>)</span>, enhancing mouthing animation <span class="citation" data-cites="paula:johnson2018improved paula:wolfe2022supporting">(Johnson, Brumm, and Wolfe <a href="#ref-paula:johnson2018improved" role="doc-biblioref">2018</a>; Wolfe et al. <a href="#ref-paula:wolfe2022supporting" role="doc-biblioref">2022</a>)</span>, multi-layering facial textures and makeup <span class="citation" data-cites="paula:wolfecase">(Wolfe et al. <a href="#ref-paula:wolfecase" role="doc-biblioref">2019</a>)</span>, and applying adverbial modifiers <span class="citation" data-cites="paula:moncrief2020extending paula:moncrief2021generalizing">(Moncrief <a href="#ref-paula:moncrief2020extending" role="doc-biblioref">2020</a>, <a href="#ref-paula:moncrief2021generalizing" role="doc-biblioref">2021</a>)</span>.</p>
<p>Additional improvements to PAULA focus on making the avatar more lifelike by relaxing wrist orientations and other extreme “mathematical” angles <span class="citation" data-cites="paula:filhol2020synthesis">(Filhol and McDonald <a href="#ref-paula:filhol2020synthesis" role="doc-biblioref">2020</a>)</span>, refining hand shape transition, relaxation, and collision <span class="citation" data-cites="paula:baowidan2021improving">(Baowidan <a href="#ref-paula:baowidan2021improving" role="doc-biblioref">2021</a>)</span>, implementing hierarchical phrase transitions <span class="citation" data-cites="paula:mcdonald2021natural">(McDonald and Filhol <a href="#ref-paula:mcdonald2021natural" role="doc-biblioref">2021</a>)</span>, creating more realistic facial muscle control <span class="citation" data-cites="paula:mcdonald2022novel">(McDonald, Johnson, and Wolfe <a href="#ref-paula:mcdonald2022novel" role="doc-biblioref">2022</a>)</span>, and supporting geometric relocations <span class="citation" data-cites="paula:filhol2022representation">(Filhol and McDonald <a href="#ref-paula:filhol2022representation" role="doc-biblioref">2022</a>)</span>.</p>
</section>
<section id="simax-simax_2020" class="unnumbered">
<h6 class="unnumbered">SiMAX <span class="citation" data-cites="SiMAX_2020">(<span class="citeproc-not-found" data-reference-id="SiMAX_2020"><strong>???</strong></span>)</span></h6>
<p>is a software application developed to transform textual input into 3D animated sign language representations. Utilizing a comprehensive database and the expertise of deaf sign language professionals, SiMAX ensures accurate translations of both written and spoken content. The process begins with the generation of a translation suggestion, which is subsequently reviewed and, if necessary, modified by deaf translators to ensure accuracy and cultural appropriateness. These translations are carried out by a customizable digital avatar that can be adapted to reflect the corporate identity or target audience of the user. This approach offers a cost-effective alternative to traditional sign language video production, as it eliminates the need for expensive film studios and complex video technology typically associated with such productions.</p>
</section>
<h5 id="image-and-video-generation-models">Image and Video Generation Models</h5>
<p>Most recently in the field of image and video generation, there have been notable advances in methods such as Style-Based Generator Architecture for Generative Adversarial Networks <span class="citation" data-cites="style-to-image:Karras2018ASG style-to-image:Karras2019stylegan2 style-to-image:Karras2021">(Karras, Laine, and Aila <a href="#ref-style-to-image:Karras2018ASG" role="doc-biblioref">2019</a>; Karras et al. <a href="#ref-style-to-image:Karras2019stylegan2" role="doc-biblioref">2020</a>, <a href="#ref-style-to-image:Karras2021" role="doc-biblioref">2021</a>)</span>, Variational Diffusion Models <span class="citation" data-cites="text-to-image:Kingma2021VariationalDM">(Kingma et al. <a href="#ref-text-to-image:Kingma2021VariationalDM" role="doc-biblioref">2021</a>)</span>, High-Resolution Image Synthesis with Latent Diffusion Models <span class="citation" data-cites="text-to-image:Rombach2021HighResolutionIS">(Rombach et al. <a href="#ref-text-to-image:Rombach2021HighResolutionIS" role="doc-biblioref">2021</a>)</span>, High Definition Video Generation with Diffusion Models <span class="citation" data-cites="text-to-video:Ho2022ImagenVH">(Ho et al. <a href="#ref-text-to-video:Ho2022ImagenVH" role="doc-biblioref">2022</a>)</span>, and High-Resolution Video Synthesis with Latent Diffusion Models <span class="citation" data-cites="text-to-video:blattmann2023videoldm">(Blattmann et al. <a href="#ref-text-to-video:blattmann2023videoldm" role="doc-biblioref">2023</a>)</span>. These methods have significantly improved image and video synthesis quality, providing stunningly realistic and visually appealing results.</p>
<p>However, despite their remarkable progress in generating high-quality images and videos, these models trade-off computational efficiency. The complexity of these algorithms often results in slower inference times, making real-time applications challenging. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. While compute-aware optimizations, specifically targeting hardware capabilities of different devices, could improve the inference latency of these models, <span class="citation" data-cites="Chen2023SpeedIA">Chen et al. (<a href="#ref-Chen2023SpeedIA" role="doc-biblioref">2023</a>)</span> found that optimizing such models on top-of-the-line mobile devices such as the Samsung S23 Ultra or iPhone 14 Pro Max can decrease per-frame inference latency from around 23 seconds to around 12.</p>
<p>ControlNet <span class="citation" data-cites="pose-to-image:zhang2023adding">(L. Zhang and Agrawala <a href="#ref-pose-to-image:zhang2023adding" role="doc-biblioref">2023</a>)</span> recently presented a neural network structure for controlling pretrained large diffusion models with additional input conditions. This approach enables end-to-end learning of task-specific conditions, even with a small training dataset. Training a ControlNet is as fast as fine-tuning a diffusion model and can be executed on personal devices or scaled to large amounts of data using powerful computation clusters. ControlNet has been demonstrated to augment large diffusion models like Stable Diffusion with conditional inputs such as edge maps, segmentation maps, and keypoints. One of the applications of ControlNet is pose-to-image translation control, which allows the generation of images based on pose information. Although this method has shown promising results, it still requires retraining the model and does not inherently support temporal coherency, which is important for tasks like sign language translation.</p>
<p>In the near future, we can expect many works on controlling video diffusion models directly from text for sign language translation. These models will likely generate visually appealing and realistic videos. However, they may still make mistakes and be limited to scenarios with more training data available. Developing models that can accurately generate sign language videos from text or pose information while maintaining visual quality and temporal coherency will be essential for advancing the field of sign language production.</p>
<hr />
<h4 id="pose-to-gloss">Pose-to-Gloss</h4>
<p>Pose-to-Gloss, also known as sign language recognition, is the task of recognizing a sequence of signs from a sequence of poses. Though some previous works have referred to this as “sign language translation,” recognition merely determines the associated label of each sign, without handling the syntax and morphology of the signed language <span class="citation" data-cites="padden1988interaction">(C. Padden <a href="#ref-padden1988interaction" role="doc-biblioref">1988</a>)</span> to create a spoken language output. Instead, SLR has often been used as an intermediate step during translation to produce glosses from signed language videos.</p>
<p><span class="citation" data-cites="jiang2021sign">Jiang et al. (<a href="#ref-jiang2021sign" role="doc-biblioref">2021</a>)</span> proposed a novel Skeleton Aware Multi-modal Framework with a Global Ensemble Model (GEM) for isolated SLR (SAM-SLR-v2) to learn and fuse multimodal feature representations. Specifically, they use a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics of skeleton keypoints and a Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. The proposed late-fusion GEM fuses the skeleton-based predictions with other RGB and depth-based modalities to provide global information and make an accurate SLR prediction.</p>
<p><span class="citation" data-cites="dafnis2022bidirectional">Dafnis et al. (<a href="#ref-dafnis2022bidirectional" role="doc-biblioref">2022</a>)</span> work on the same modified WLASL dataset as <span class="citation" data-cites="jiang2021sign">Jiang et al. (<a href="#ref-jiang2021sign" role="doc-biblioref">2021</a>)</span>, but do not require multimodal data input. Instead, they propose a bidirectional skeleton-based graph convolutional network framework with linguistically motivated parameters and attention to the start and end frames of signs. They cooperatively use forward and backward data streams, including various sub-streams, as input. They also use pre-training to leverage transfer learning.</p>
<p><span class="citation" data-cites="selvaraj-etal-2022-openhands">Selvaraj et al. (<a href="#ref-selvaraj-etal-2022-openhands" role="doc-biblioref">2022</a>)</span> introduced an open-source <a href="https://github.com/AI4Bharat/OpenHands">OpenHands</a> library, which consists of standardized pose datasets for different existing sign language datasets and trained checkpoints of four pose-based isolated sign language recognition models across six languages (American, Argentinian, Chinese, Greek, Indian, and Turkish). To address the lack of labeled data, they propose self-supervised pretraining on unlabeled data and curate the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). They established that pretraining is effective for sign language recognition by demonstrating improved fine-tuning performance especially in low-resource settings and high crosslingual transfer from Indian-SL to a few other sign languages.</p>
<p>The work of <span class="citation" data-cites="kezar2023improving">Kezar, Thomason, and Sehyr (<a href="#ref-kezar2023improving" role="doc-biblioref">2023</a>)</span>, based on the <a href="https://github.com/AI4Bharat/OpenHands">OpenHands</a> library, explicitly recognizes the role of phonology to achieve more accurate isolated sign language recognition (ISLR). To allow additional predictions on phonological characteristics (such as handshape), they combine the phonological annotations in ASL-LEX 2.0 <span class="citation" data-cites="dataset:sehyr2021asl">(Sehyr et al. <a href="#ref-dataset:sehyr2021asl" role="doc-biblioref">2021</a>)</span> with signs in the WLASL 2000 ISLR benchmark <span class="citation" data-cites="dataset:li2020word">(Li et al. <a href="#ref-dataset:li2020word" role="doc-biblioref">2020</a>)</span>. Interestingly, <span class="citation" data-cites="tavella-etal-2022-wlasl">Tavella et al. (<a href="#ref-tavella-etal-2022-wlasl" role="doc-biblioref">2022</a>)</span> construct a similar dataset aiming just for phonological property recognition in American Sign Language (ASL).</p>
<h4 id="gloss-to-pose">Gloss-to-Pose</h4>
<p>Gloss-to-Pose, subsumed under the task of sign language production, is the task of producing a sequence of poses that adequately represent a sequence of signs written as gloss.</p>
<p>To produce a sign language video, <span class="citation" data-cites="stoll2018sign">Stoll et al. (<a href="#ref-stoll2018sign" role="doc-biblioref">2018</a>)</span> constructed a lookup table between glosses and sequences of 2D poses. They aligned all pose sequences at the neck joint of a reference skeleton and grouped all sequences belonging to the same gloss. Then, for each group, they applied dynamic time warping and averaged out all sequences in the group to construct the mean pose sequence. This approach suffers from not having an accurate set of poses aligned to the gloss and from unnatural motion transitions between glosses.</p>
<p>To alleviate the downsides of the previous work, <span class="citation" data-cites="stoll2020text2sign">Stoll et al. (<a href="#ref-stoll2020text2sign" role="doc-biblioref">2020</a>)</span> constructed a lookup table of gloss to a group of sequences of poses rather than creating a mean pose sequence. They built a Motion Graph <span class="citation" data-cites="min2012motion">(Min and Chai <a href="#ref-min2012motion" role="doc-biblioref">2012</a>)</span>, which is a Markov process used to generate new motion sequences that are representative of natural motion, and selected the motion primitives (sequence of poses) per gloss with the highest transition probability. To smooth that sequence and reduce unnatural motion, they used a Savitzky–Golay motion transition smoothing filter <span class="citation" data-cites="savitzky1964smoothing">(Savitzky and Golay <a href="#ref-savitzky1964smoothing" role="doc-biblioref">1964</a>)</span>. <span class="citation" data-cites="moryossef2023baseline">Moryossef, Müller, et al. (<a href="#ref-moryossef2023baseline" role="doc-biblioref">2023</a>)</span> re-implemented their approach and made it open-source.</p>
<p><span class="citation" data-cites="huang2021towards">Huang et al. (<a href="#ref-huang2021towards" role="doc-biblioref">2021</a>)</span> used a new non-autoregressive model to generate a sequence of poses for a sequence of glosses. They argued that existing models like <span class="citation" data-cites="saunders2020adversarial">Saunders, Bowden, and Camgöz (<a href="#ref-saunders2020adversarial" role="doc-biblioref">2020</a>)</span> are prone to error accumulation and high inference latency due to their autoregressive nature. Their model performs gradual upsampling of the poses, by starting with a pose including only two joints in the first layer, and gradually introducing more keypoints. They evaluated their model on the Phoenix-14T dataset <span class="citation" data-cites="dataset:forster2014extensions">(Forster et al. <a href="#ref-dataset:forster2014extensions" role="doc-biblioref">2014</a>)</span> using Dynamic Time Warping (DTW) <span class="citation" data-cites="Berndt1994UsingDT">(Berndt and Clifford <a href="#ref-Berndt1994UsingDT" role="doc-biblioref">1994</a>)</span> to align the poses before computing Mean Joint Error (DTW-MJE). They demonstrated that their model outperforms existing methods in terms of accuracy and speed, making it a promising approach for fast and high-quality sign language production.</p>
<hr />
<h4 id="video-to-gloss">Video-to-Gloss</h4>
<p>Video-to-Gloss, also known as sign language recognition, is the task of recognizing a sequence of signs from a video.</p>
<p>For this recognition, <span class="citation" data-cites="cui2017recurrent">Cui, Liu, and Zhang (<a href="#ref-cui2017recurrent" role="doc-biblioref">2017</a>)</span> constructs a three-step optimization model. First, they train a video-to-gloss end-to-end model, where they encode the video using a spatio-temporal CNN encoder and predict the gloss using a Connectionist Temporal Classification (CTC) <span class="citation" data-cites="graves2006connectionist">(Graves et al. <a href="#ref-graves2006connectionist" role="doc-biblioref">2006</a>)</span>. Then, from the CTC alignment and category proposal, they encode each gloss-level segment independently, trained to predict the gloss category, and use this gloss video segments encoding to optimize the sequence learning model.</p>
<p><span class="citation" data-cites="cihan2018neural">Camgöz et al. (<a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> fundamentally differ from that approach and formulate this problem as if it is a natural-language translation problem. They encode each video frame using AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton <a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span>, initialized using weights trained on ImageNet <span class="citation" data-cites="deng2009imagenet">(Deng et al. <a href="#ref-deng2009imagenet" role="doc-biblioref">2009</a>)</span>. Then they apply a GRU encoder-decoder architecture with Luong Attention <span class="citation" data-cites="luong2015effective">(Luong, Pham, and Manning <a href="#ref-luong2015effective" role="doc-biblioref">2015</a>)</span> to generate the gloss. In follow-up work, <span class="citation" data-cites="camgoz2020sign">N. C. Camgöz et al. (<a href="#ref-camgoz2020sign" role="doc-biblioref">2020</a><a href="#ref-camgoz2020sign" role="doc-biblioref">b</a>)</span> use a transformer encoder <span class="citation" data-cites="vaswani2017attention">(Vaswani et al. <a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span> to replace the GRU and use a CTC to decode the gloss. They show a slight improvement with this approach on the video-to-gloss task.</p>
<p><span class="citation" data-cites="adaloglou2020comprehensive">Adaloglou et al. (<a href="#ref-adaloglou2020comprehensive" role="doc-biblioref">2020</a>)</span> perform a comparative experimental assessment of computer vision-based methods for the video-to-gloss task. They implement various approaches from previous research <span class="citation" data-cites="camgoz2017subunets cui2019deep dataset:joze2018ms">(Camgöz et al. <a href="#ref-camgoz2017subunets" role="doc-biblioref">2017</a>; Cui, Liu, and Zhang <a href="#ref-cui2019deep" role="doc-biblioref">2019</a>; Joze and Koller <a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>)</span> and test them on multiple datasets <span class="citation" data-cites="dataset:huang2018video cihan2018neural dataset:von2007towards dataset:joze2018ms">(Huang et al. <a href="#ref-dataset:huang2018video" role="doc-biblioref">2018</a>; Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>; Von Agris and Kraiss <a href="#ref-dataset:von2007towards" role="doc-biblioref">2007</a>; Joze and Koller <a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>)</span> either for isolated sign recognition or continuous sign recognition. They conclude that 3D convolutional models outperform models using only recurrent networks to capture the temporal information, and that these models are more scalable given the restricted receptive field, which results from the CNN “sliding window” technique.</p>
<p><span class="citation" data-cites="momeni2022automatic">Momeni, Bull, Prajwal, et al. (<a href="#ref-momeni2022automatic" role="doc-biblioref">2022</a><a href="#ref-momeni2022automatic" role="doc-biblioref">a</a>)</span> developed a comprehensive pipeline that combines various models to densely annotate sign language videos. By leveraging the use of synonyms and subtitle-signing alignment, their approach demonstrates the value of pseudo-labeling from a sign recognition model for sign spotting. They propose a novel method to increase annotations for both known and unknown classes, relying on in-domain exemplars. As a result, their framework significantly expands the number of confident automatic annotations on the BOBSL BSL sign language corpus <span class="citation" data-cites="dataset:albanie2021bobsl">(Albanie et al. <a href="#ref-dataset:albanie2021bobsl" role="doc-biblioref">2021</a>)</span> from 670K to 5M, and they generously make these annotations publicly available.</p>
<h4 id="gloss-to-video">Gloss-to-Video</h4>
<p>Gloss-to-Video, also known as sign language production, is the task of producing a video that adequately represents a sequence of signs written as gloss.</p>
<p>As of 2020, no research discusses the direct translation task between gloss and video. This lack of discussion results from the computational impracticality of the desired model, leading researchers to refrain from performing this task directly and instead rely on pipeline approaches using intermediate pose representations.</p>
<hr />
<h4 id="gloss-to-text">Gloss-to-Text</h4>
<p>Gloss-to-Text, also known as sign language translation, is the natural language processing task of translating between gloss text representing sign language signs and spoken language text. These texts commonly differ in terminology, capitalization, and sentence structure.</p>
<p><span class="citation" data-cites="cihan2018neural">Camgöz et al. (<a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> experimented with various machine-translation architectures and compared using an LSTM <span class="citation" data-cites="hochreiter1997long">(Hochreiter and Schmidhuber <a href="#ref-hochreiter1997long" role="doc-biblioref">1997</a>)</span> vs. GRU for the recurrent model, as well as Luong attention <span class="citation" data-cites="luong2015effective">(Luong, Pham, and Manning <a href="#ref-luong2015effective" role="doc-biblioref">2015</a>)</span> vs. Bahdanau attention <span class="citation" data-cites="bahdanau2014neural">(Bahdanau, Cho, and Bengio <a href="#ref-bahdanau2014neural" role="doc-biblioref">2015</a>)</span> and various batch sizes. They concluded that on the RWTH-PHOENIX-Weather-2014T dataset, which was also presented in this work, using GRUs, Luong attention, and a batch size of 1 outperforms all other configurations.</p>
<p>In parallel with the advancements in spoken language machine translation, <span class="citation" data-cites="yin-read-2020-better">Yin and Read (<a href="#ref-yin-read-2020-better" role="doc-biblioref">2020</a>)</span> proposed replacing the RNN with a Transformer <span class="citation" data-cites="vaswani2017attention">(Vaswani et al. <a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span> encoder-decoder model, showing improvements on both RWTH-PHOENIX-Weather-2014T (DGS) and ASLG-PC12 (ASL) datasets both using a single model and ensemble of models. Interestingly, in gloss-to-text, they show that using the sign language recognition (video-to-gloss) system output outperforms using the gold annotated glosses.</p>
<p>Building on the code published by <span class="citation" data-cites="yin-read-2020-better">Yin and Read (<a href="#ref-yin-read-2020-better" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="moryossef-etal-2021-data">Moryossef, Yin, et al. (<a href="#ref-moryossef-etal-2021-data" role="doc-biblioref">2021</a>)</span> show it is beneficial to pre-train these translation models using augmented monolingual spoken language corpora. They try three different approaches for data augmentation: (1) Back-translation; (2) General text-to-gloss rules, including lemmatization, word reordering, and dropping of words; (3) Language-pair-specific rules augmenting the spoken language syntax to its corresponding sign language syntax. When pretraining, all augmentations show improvements over the baseline for RWTH-PHOENIX-Weather-2014T (DGS) and NCSLGR (ASL).</p>
<h4 id="text-to-gloss">Text-to-Gloss</h4>
<p>Text-to-gloss, an instantiation of sign language translation, is the task of translating between a spoken language text and sign language glosses. It is an appealing area of research because of its simplicity for integrating in existing NMT pipelines, despite recent works such as <span class="citation" data-cites="yin-read-2020-better">Yin and Read (<a href="#ref-yin-read-2020-better" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="muller-etal-2023-considerations">Müller et al. (<a href="#ref-muller-etal-2023-considerations" role="doc-biblioref">2023</a>)</span> claim that glosses are an inefficient representation of sign language, and that glosses are not a complete representation of signs <span class="citation" data-cites="pizzuto:06001:sign-lang:lrec">(Pizzuto, Rossini, and Russo <a href="#ref-pizzuto:06001:sign-lang:lrec" role="doc-biblioref">2006</a>)</span>.</p>
<p><span class="citation" data-cites="zhao2000machine">Zhao et al. (<a href="#ref-zhao2000machine" role="doc-biblioref">2000</a>)</span> used a Tree Adjoining Grammar (TAG)-based system to translate English sentences to American Sign Language (ASL) gloss sequences. They parsed the English text and simultaneously assembled an ASL gloss tree, using Synchronous TAGs <span class="citation" data-cites="shieber1990synchronous shieber1994restricting">(Shieber and Schabes <a href="#ref-shieber1990synchronous" role="doc-biblioref">1990</a>; Shieber <a href="#ref-shieber1994restricting" role="doc-biblioref">1994</a>)</span>, by associating the ASL elementary trees with the English elementary trees and associating the nodes at which subsequent substitutions or adjunctions can occur. Synchronous TAGs have been used for machine translation between spoken languages <span class="citation" data-cites="abeille1991using">(Abeille, Schabes, and Joshi <a href="#ref-abeille1991using" role="doc-biblioref">1990</a>)</span>, but this was the first application to a signed language.</p>
<p>For the automatic translation of gloss-to-text, <span class="citation" data-cites="dataset:othman2012english">Othman and Jemni (<a href="#ref-dataset:othman2012english" role="doc-biblioref">2012</a>)</span> identified the need for a large parallel sign language gloss and spoken language text corpus. They developed a part-of-speech-based grammar to transform English sentences from the Gutenberg Project ebooks collection <span class="citation" data-cites="lebert2008project">(Lebert <a href="#ref-lebert2008project" role="doc-biblioref">2008</a>)</span> into American Sign Language gloss. Their final corpus contains over 100 million synthetic sentences and 800 million words and is the most extensive English-ASL gloss corpus we know of. Unfortunately, it is hard to attest to the quality of the corpus, as the authors did not evaluate their method on real English-ASL gloss pairs.</p>
<p><span class="citation" data-cites="egea-gomez-etal-2021-syntax">Egea Gómez, McGill, and Saggion (<a href="#ref-egea-gomez-etal-2021-syntax" role="doc-biblioref">2021</a>)</span> presented a syntax-aware transformer for this task, by injecting word dependency tags to augment the embeddings inputted to the encoder. This involves minor modifications in the neural architecture leading to negligible impact on computational complexity of the model. Testing their model on the RWTH-PHOENIX-Weather-2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span>, they demonstrated that injecting this additional information results in better translation quality.</p>
<hr />
<h4 id="video-to-text">Video-to-Text</h4>
<p>Video-to-text, also known as sign language translation, is the task of translating a raw video to spoken language text.</p>
<p><span class="citation" data-cites="camgoz2020sign">N. C. Camgöz et al. (<a href="#ref-camgoz2020sign" role="doc-biblioref">2020</a><a href="#ref-camgoz2020sign" role="doc-biblioref">b</a>)</span> proposed a single architecture to perform this task that can use both the sign language gloss and the spoken language text in joint supervision. They use the pre-trained spatial embeddings from <span class="citation" data-cites="koller2019weakly">Koller et al. (<a href="#ref-koller2019weakly" role="doc-biblioref">2019</a>)</span> to encode each frame independently and encode the frames with a transformer. On this encoding, they use a Connectionist Temporal Classification (CTC) <span class="citation" data-cites="graves2006connectionist">(Graves et al. <a href="#ref-graves2006connectionist" role="doc-biblioref">2006</a>)</span> to classify the sign language gloss. Using the same encoding, they use a transformer decoder to decode the spoken language text one token at a time. They show that adding gloss supervision improves the model over not using it and that it outperforms previous video-to-gloss-to-text pipeline approaches <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span>.</p>
<p>Following up, <span class="citation" data-cites="camgoz2020multi">N. C. Camgöz et al. (<a href="#ref-camgoz2020multi" role="doc-biblioref">2020</a><a href="#ref-camgoz2020multi" role="doc-biblioref">a</a>)</span> propose a new architecture that does not require the supervision of glosses, named “Multi-channel Transformers for Multi-articulatory Sign Language Translation”. In this approach, they crop the signing hand and the face and perform 3D pose estimation to obtain three separate data channels. They encode each data channel separately using a transformer, then encode all channels together and concatenate the separate channels for each frame. Like their previous work, they use a transformer decoder to decode the spoken language text, but unlike their previous work, do not use the gloss as additional supervision. Instead, they add two “anchoring” losses to predict the hand shape and mouth shape from each frame independently, as silver annotations are available to them using the model proposed in <span class="citation" data-cites="koller2019weakly">Koller et al. (<a href="#ref-koller2019weakly" role="doc-biblioref">2019</a>)</span>. They conclude that this approach is on-par with previous approaches requiring glosses, and so they have broken the dependency upon costly annotated gloss information in the video-to-text task.</p>
<p><span class="citation" data-cites="shi-etal-2022-open">Shi et al. (<a href="#ref-shi-etal-2022-open" role="doc-biblioref">2022</a>)</span> introduce OpenASL, a large-scale American Sign Language (ASL) - English dataset collected from online video sites (e.g., YouTube), and then propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features to improve translation quality in the absence of glosses and in the presence of visually challenging data.</p>
<!-- Really should put MMTLB here, a number of papers cite it including chen2022, which actually builds on it directly, cites it as a source for "mBART is good for SLT", etc. -->
<p><span class="citation" data-cites="chen2022TwoStreamNetworkSign">Yutong Chen, Zuo, et al. (<a href="#ref-chen2022TwoStreamNetworkSign" role="doc-biblioref">2022</a>)</span> present a two-stream network for sign language recognition (SLR) and translation (SLT), utilizing a dual visual encoder architecture to encode RGB video frames and pose keypoints in separate streams. These streams interact via bidirectional lateral connections. For SLT, the visual encoders based on an S3D backbone <span class="citation" data-cites="xie2018SpatiotemporalS3D">(Xie et al. <a href="#ref-xie2018SpatiotemporalS3D" role="doc-biblioref">2018</a>)</span> output to a multilingual translation network using mBART <span class="citation" data-cites="liu-etal-2020-multilingual-denoising">(Liu et al. <a href="#ref-liu-etal-2020-multilingual-denoising" role="doc-biblioref">2020</a>)</span>. The model achieves state-of-the-art performance on the RWTH-PHOENIX-Weather-2014 <span class="citation" data-cites="dataset:forster2014extensions">(Forster et al. <a href="#ref-dataset:forster2014extensions" role="doc-biblioref">2014</a>)</span>, RWTH-PHOENIX-Weather-2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> and CSL-Daily <span class="citation" data-cites="dataset:Zhou2021_SignBackTranslation_CSLDaily">(Zhou et al. <a href="#ref-dataset:Zhou2021_SignBackTranslation_CSLDaily" role="doc-biblioref">2021</a>)</span> datasets.</p>
<p><span class="citation" data-cites="zhang2023sltunet">B. Zhang, Müller, and Sennrich (<a href="#ref-zhang2023sltunet" role="doc-biblioref">2023</a>)</span> propose a multi-modal, multi-task learning approach to end-to-end sign language translation. The model features shared representations for different modalities such as text and video and is trained jointly on several tasks such as video-to-gloss, gloss-to-text, and video-to-text. The approach allows leveraging external data such as parallel data for spoken language machine translation.</p>
<p><span class="citation" data-cites="Zhao_Zhang_Fu_Hu_Su_Chen_2024">Zhao et al. (<a href="#ref-Zhao_Zhang_Fu_Hu_Su_Chen_2024" role="doc-biblioref">2024</a>)</span> introduce CV-SLT, employing conditional variational autoencoders to address the modality gap between video and text. Their approach involves guiding the model to encode visual and textual data similarly through two paths: one with visual data alone and one with both modalities. Using KL divergences, they steer the model towards generating consistent embeddings and accurate outputs regardless of the path. Once the model achieves consistent performance across paths, it can be utilized for translation without gloss supervision. Evaluation on the RWTH-PHOENIX-Weather-2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> and CSL-Daily <span class="citation" data-cites="dataset:Zhou2021_SignBackTranslation_CSLDaily">(Zhou et al. <a href="#ref-dataset:Zhou2021_SignBackTranslation_CSLDaily" role="doc-biblioref">2021</a>)</span> datasets demonstrates its efficacy. They provide a <a href="https://github.com/rzhao-zhsq/CV-SLT">code implementation</a> based largely on <span class="citation" data-cites="chenSimpleMultiModalityTransfer2022a">Yutong Chen, Wei, et al. (<a href="#ref-chenSimpleMultiModalityTransfer2022a" role="doc-biblioref">2022</a>)</span>. <!-- The CV-SLT code looks pretty nice! Conda env file, data prep, not too old, paths in .yaml files, checkpoints provided (including the ones for replication), commands to train and evaluate, very nice --></p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: the "previous gloss-free frameworks" that gongLLMsAreGood2024 cite are: Gloss Attention for Gloss-free Sign Language Translation (2023) and Gloss-free sign language translation: Improving from visual-language pretraining, 2023 aka GFSLT-VLP. Could be good to lead into it with explanations of those? -->
<p><span class="citation" data-cites="gongLLMsAreGood2024">Gong et al. (<a href="#ref-gongLLMsAreGood2024" role="doc-biblioref">2024</a>)</span> introduce SignLLM, a framework for gloss-free sign language translation that leverages the strengths of Large Language Models (LLMs). SignLLM converts sign videos into discrete and hierarchical representations compatible with LLMs through two modules: (1) The Vector-Quantized Visual Sign (VQ-Sign) module, which translates sign videos into discrete “character-level” tokens, and (2) the Codebook Reconstruction and Alignment (CRA) module, which restructures these tokens into “word-level” representations. During inference, the “word-level” tokens are projected into the LLM’s embedding space, which is then prompted for translation. The LLM itself can be taken “off the shelf” and does not need to be trained. In training, the VQ-Sign “character-level” module is trained with a context prediction task, the CRA “word-level” module with an optimal transport technique, and a sign-text alignment loss further enhances the semantic alignment between sign and text tokens. The framework achieves state-of-the-art results on the RWTH-PHOENIX-Weather-2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> and CSL-Daily <span class="citation" data-cites="dataset:Zhou2021_SignBackTranslation_CSLDaily">(Zhou et al. <a href="#ref-dataset:Zhou2021_SignBackTranslation_CSLDaily" role="doc-biblioref">2021</a>)</span> datasets without relying on gloss annotations. <!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: c.f. SignLLM with https://github.com/sign-language-processing/sign-vq? --></p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: YoutubeASL explanation would fit nicely here before Rust et al 2024. They don't just do data IIRC. -->
<p><span class="citation" data-cites="rust2024PrivacyAwareSign">Rust et al. (<a href="#ref-rust2024PrivacyAwareSign" role="doc-biblioref">2024</a>)</span> introduce a two-stage privacy-aware method for sign language translation (SLT) at scale, termed Self-Supervised Video Pretraining for Sign Language Translation (SSVP-SLT). The first stage involves self-supervised pretraining of a Hiera vision transformer <span class="citation" data-cites="ryali2023HieraVisionTransformer">(Ryali et al. <a href="#ref-ryali2023HieraVisionTransformer" role="doc-biblioref">2023</a>)</span> on large unannotated video datasets <span class="citation" data-cites="dataset:duarte2020how2sign dataset:uthus2023YoutubeASL">(Duarte et al. <a href="#ref-dataset:duarte2020how2sign" role="doc-biblioref">2021</a>; Uthus, Tanzer, and Georg <a href="#ref-dataset:uthus2023YoutubeASL" role="doc-biblioref">2023</a>)</span>. In the second stage, the vision model’s outputs are fed into a multilingual language model <span class="citation" data-cites="raffel2020T5Transformer">(Raffel et al. <a href="#ref-raffel2020T5Transformer" role="doc-biblioref">2020</a>)</span> for finetuning on the How2Sign dataset <span class="citation" data-cites="dataset:duarte2020how2sign">(Duarte et al. <a href="#ref-dataset:duarte2020how2sign" role="doc-biblioref">2021</a>)</span>. To mitigate privacy risks, the framework employs facial blurring during pretraining. They find that while pretraining with blurring hurts performance, some can be recovered when finetuning with unblurred data. SSVP-SLT achieves state-of-the-art performance on How2Sign <span class="citation" data-cites="dataset:duarte2020how2sign">(Duarte et al. <a href="#ref-dataset:duarte2020how2sign" role="doc-biblioref">2021</a>)</span>. They conclude that SLT models can be pretrained in a privacy-aware manner without sacrificing too much performance. Additionally, the authors release DailyMoth-70h, a new 70-hour ASL dataset from <a href="https://www.dailymoth.com/">The Daily Moth</a>.</p>
<h4 id="text-to-video">Text-to-Video</h4>
<p>Text-to-Video, also known as sign language production, is the task of producing a video that adequately represents a spoken language text in sign language.</p>
<p>As of 2020, no research discusses the direct translation task between text and video. This lack of discussion results from the computational impracticality of the desired model, leading researchers to refrain from performing this task directly and instead rely on pipeline approaches using intermediate pose representations.</p>
<hr />
<h4 id="pose-to-text">Pose-to-Text</h4>
<p>Pose-to-text, also known as sign language translation, is the task of translating a captured or estimated pose sequence to spoken language text.</p>
<p><span class="citation" data-cites="dataset:ko2019neural">Ko et al. (<a href="#ref-dataset:ko2019neural" role="doc-biblioref">2019</a>)</span> demonstrated impressive performance on the pose-to-text task by inputting the pose sequence into a standard encoder-decoder translation network. They experimented both with GRU and various types of attention <span class="citation" data-cites="luong2015effective bahdanau2014neural">(Luong, Pham, and Manning <a href="#ref-luong2015effective" role="doc-biblioref">2015</a>; Bahdanau, Cho, and Bengio <a href="#ref-bahdanau2014neural" role="doc-biblioref">2015</a>)</span> and with a Transformer <span class="citation" data-cites="vaswani2017attention">(Vaswani et al. <a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>, and showed similar performance, with the transformer underperforming on the validation set and overperforming on the test set, which consists of unseen signers. They experimented with various normalization schemes, mainly subtracting the mean and dividing by the standard deviation of every individual keypoint either concerning the entire frame or the relevant “object” (Body, Face, and Hand).</p>
<h4 id="text-to-pose">Text-to-Pose</h4>
<p>Text-to-Pose, also known as sign language production, is the task of producing a sequence of poses that adequately represent a spoken language text in sign language, as an intermediate representation to overcome challenges in animation. Most efforts use poses as an intermediate representation to overcome the challenges in generating videos directly, with the goal of using computer animation or pose-to-video models to perform video production.</p>
<p><span class="citation" data-cites="saunders2020progressive">Saunders, Camgöz, and Bowden (<a href="#ref-saunders2020progressive" role="doc-biblioref">2020</a><a href="#ref-saunders2020progressive" role="doc-biblioref">b</a>)</span> proposed Progressive Transformers, a model to translate from discrete spoken language sentences to continuous 3D sign pose sequences in an autoregressive manner. Unlike symbolic transformers <span class="citation" data-cites="vaswani2017attention">(Vaswani et al. <a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>, which use a discrete vocabulary and thus can predict an end-of-sequence (<code>EOS</code>) token in every step, the progressive transformer predicts a <span class="math inline"><em>c</em><em>o</em><em>u</em><em>n</em><em>t</em><em>e</em><em>r</em> ∈ [0, 1]</span> in addition to the pose. In inference time, <span class="math inline"><em>c</em><em>o</em><em>u</em><em>n</em><em>t</em><em>e</em><em>r</em> = 1</span> is considered the end of the sequence. They tested their approach on the RWTH-PHOENIX-Weather-2014T dataset using OpenPose 2D pose estimation, uplifted to 3D <span class="citation" data-cites="pose:zelinka2020neural">(Zelinka and Kanis <a href="#ref-pose:zelinka2020neural" role="doc-biblioref">2020</a>)</span>, and showed favorable results when evaluating using back-translation from the generated poses to spoken language. They further showed <span class="citation" data-cites="saunders2020adversarial">(Saunders, Bowden, and Camgöz <a href="#ref-saunders2020adversarial" role="doc-biblioref">2020</a>)</span> that using an adversarial discriminator between the ground truth poses and the generated poses, conditioned on the input spoken language text, improves the production quality as measured using back-translation.</p>
<p>To overcome the issues of under-articulation seen in the above works, <span class="citation" data-cites="saunders2020everybody">Saunders, Camgöz, and Bowden (<a href="#ref-saunders2020everybody" role="doc-biblioref">2020</a><a href="#ref-saunders2020everybody" role="doc-biblioref">a</a>)</span> expanded on the progressive transformer model using a Mixture Density Network (MDN) <span class="citation" data-cites="bishop1994mixture">(Bishop <a href="#ref-bishop1994mixture" role="doc-biblioref">1994</a>)</span> to model the variation found in sign language. While this model underperformed on the validation set, compared to previous work, it outperformed on the test set.</p>
<p><span class="citation" data-cites="pose:zelinka2020neural">Zelinka and Kanis (<a href="#ref-pose:zelinka2020neural" role="doc-biblioref">2020</a>)</span> presented a similar autoregressive decoder approach, with added dynamic-time-warping (DTW) and soft attention. They tested their approach on Czech Sign Language weather data extracted from the news, which is not manually annotated, or aligned to the spoken language captions, and showed their DTW is advantageous for this kind of task.</p>
<p><span class="citation" data-cites="xiao2020skeleton">Xiao, Qin, and Yin (<a href="#ref-xiao2020skeleton" role="doc-biblioref">2020</a>)</span> closed the loop by proposing a text-to-pose-to-text model for the case of isolated sign language recognition. They first trained a classifier to take a sequence of poses encoded by a BiLSTM and classify the relevant sign, then proposed a production system to take a single sign and sample a constant length sequence of 50 poses from a Gaussian Mixture Model. These components are combined such that given a sign class <span class="math inline"><em>y</em></span>, a pose sequence is generated, then classified back into a sign class <span class="math inline"><em>ŷ</em></span>, and the loss is applied between <span class="math inline"><em>y</em></span> and <span class="math inline"><em>ŷ</em></span>, and not directly on the generated pose sequence. They evaluate their approach on the CSL dataset <span class="citation" data-cites="dataset:huang2018video">(Huang et al. <a href="#ref-dataset:huang2018video" role="doc-biblioref">2018</a>)</span> and show that their generated pose sequences almost reach the same classification performance as the reference sequences.</p>
<p>Due to the need for more suitable automatic evaluation methods for generated signs, existing works resort to measuring back-translation quality, which cannot accurately capture the quality of the produced signs nor their usability in real-world settings. Understanding how distinctions in meaning are created in signed language may help develop a better evaluation method.</p>
<hr />
<h4 id="notation-to-text">Notation-to-Text</h4>
<p><span class="citation" data-cites="jiang2022machine">Jiang et al. (<a href="#ref-jiang2022machine" role="doc-biblioref">2023</a>)</span> explore text-to-text sign to spoken language translation, with SignWriting as the chosen sign language notation system. Despite SignWriting usually represented in 2D, they use the 1D Formal SignWriting specification and propose a neural factored machine translation approach to encode sequences of SignWriting graphemes as well as their positions in the 2D space. They verify the proposed approach on the SignBank dataset in both a bilingual setup (American Sign Language to English) and two multilingual setups (4 and 21 language pairs, respectively). They apply several low-resource machine translation techniques used to improve spoken language translation to similarly improve the performance of sign language translation. Their findings validate the use of an intermediate text representation for signed language translation, and pave the way for including sign language translation in natural language processing research.</p>
<h4 id="text-to-notation">Text-to-Notation</h4>
<p><span class="citation" data-cites="jiang2022machine">Jiang et al. (<a href="#ref-jiang2022machine" role="doc-biblioref">2023</a>)</span> also explore the reverse translation direction, i.e., text to SignWriting translation. They conduct experiments under a same condition of their multilingual SignWriting to text (4 language pairs) experiment, and again propose a neural factored machine translation approach to decode the graphemes and their position separately. They borrow BLEU from spoken language translation to evaluate the predicted graphemes and mean absolute error to evaluate the positional numbers.</p>
<p><span class="citation" data-cites="walsh2022changing">Walsh, Saunders, and Bowden (<a href="#ref-walsh2022changing" role="doc-biblioref">2022</a>)</span> explore Text to HamNoSys (T2H) translation, with HamNoSys as the target sign language notation system. They experiment with direct T2H and Text to Gloss to HamNoSys (T2G2H) on a subset of the data from the MEINE DGS dataset <span class="citation" data-cites="dataset:hanke-etal-2020-extending">(Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span>, where all glosses are mapped to HamNoSys by a dictionary lookup. They find that direct T2H translation results in higher BLEU (it still needs to be clarified how well BLEU represents the quality of HamNoSys translations, though). They encode HamNoSys with BPE <span class="citation" data-cites="sennrich-etal-2016-neural">(Sennrich, Haddow, and Birch <a href="#ref-sennrich-etal-2016-neural" role="doc-biblioref">2016</a>)</span>, outperforming character-level and word-level tokenization. They also leverage BERT to create better sentence-level embeddings and use HamNoSys to extract the hand shapes of a sign as additional supervision during training.</p>
<hr />
<h4 id="notation-to-pose">Notation-to-Pose</h4>
<p><span class="citation" data-cites="shalev2022ham2pose">Arkushin, Moryossef, and Fried (<a href="#ref-shalev2022ham2pose" role="doc-biblioref">2023</a>)</span> proposed Ham2Pose, a model to animate HamNoSys into a sequence of poses. They first encode the HamNoSys into a meaningful “context” representation using a transform encoder, and use it to predict the length of the pose sequence to be generated. Then, starting from a still frame they used an iterative non-autoregressive decoder to gradually refine the sign over <span class="math inline"><em>T</em></span> steps, In each time step <span class="math inline"><em>t</em></span> from <span class="math inline"><em>T</em></span> to <span class="math inline">1</span>, the model predicts the required change from step <span class="math inline"><em>t</em></span> to step <span class="math inline"><em>t</em> − 1</span>. After <span class="math inline"><em>T</em></span> steps, the pose generator outputs the final pose sequence. Their model outperformed previous methods like <span class="citation" data-cites="saunders2020progressive">Saunders, Camgöz, and Bowden (<a href="#ref-saunders2020progressive" role="doc-biblioref">2020</a><a href="#ref-saunders2020progressive" role="doc-biblioref">b</a>)</span>, animating HamNoSys into more realistic sign language sequences.</p>
<h3 id="sign-language-retrieval">Sign Language Retrieval</h3>
<p>Sign Language Retrieval is the task of finding a particular data item, given some input. In contrast to translation, generation or production tasks, there can exist a correct corresponding piece of data already, and the task is to find it out of many, if it exists. Metrics used include retrieval at Rank K (R@K, higher is better) and median rank (MedR, lower is better).</p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: text-to-sign-video (T2V) section, sign-video-to-text (V2T) retrieval? -->
<p><span class="citation" data-cites="athitsos2010LargeLexiconIndexingRetrieval">Athitsos et al. (<a href="#ref-athitsos2010LargeLexiconIndexingRetrieval" role="doc-biblioref">2010</a>)</span> present one of the early works on this task, using a method based on hand centroids and dynamic time warping to enable users to submit videos of a sign and thus query within the ASL Lexicon Video Dataset <span class="citation" data-cites="dataset:athitsos2008american">(Athitsos et al. <a href="#ref-dataset:athitsos2008american" role="doc-biblioref">2008</a>)</span>.</p>
<p><span class="citation" data-cites="Zhang2010RevisedEditDistanceSignVideoRetrieval">Zhang and Zhang (<a href="#ref-Zhang2010RevisedEditDistanceSignVideoRetrieval" role="doc-biblioref">2010</a>)</span> provide another early method for video-based querying. They use classical image feature extraction methods to calculate movement trajectories. They then use modified string edit distances between these trajectories as a way to find similar videos.</p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: write here about SPOT-ALIGN aka Duarte2022SignVideoRetrivalWithTextQueries. Cheng2023CiCoSignLanguageRetrieval say retrival is "recently introduced... by SPOT-ALIGN" and cite Amanda Duarte, Samuel Albanie, Xavier Gir ́ o-i Nieto, and G ̈ ul Varol. Sign language video retrieval with free-form textual queries. Also Sign language video retrieval with free-form textual queries was the only other paper that Cheng2023CiCoSignLanguageRetrieval compared with. -->
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: write here also about jui-etal-2022-machine, the other paper cited by Cheng2023CiCoSignLanguageRetrieval for "Previous methods [16, 18] have demonstrated the feasibility of transferring a sign encoder pre-trained on large-scale sign-spotting data into downstream tasks." -->
<p><span class="citation" data-cites="costerQueryingSignLanguage2023">Coster and Dambre (<a href="#ref-costerQueryingSignLanguage2023" role="doc-biblioref">2023</a>)</span> present a method to query sign language dictionaries using dense vector search. They pretrain a <a href="#pose-to-gloss">Sign Language Recognition model</a> on a subset of the VGT corpus <span class="citation" data-cites="dataset:herreweghe2015VGTCorpus">(Van Herreweghe, Mieke and Vermeerbergen, Myriam and Demey, Eline and De Durpel, Hannes and Nyffels, Hilde and Verstraete, Sam, <a href="#ref-dataset:herreweghe2015VGTCorpus" role="doc-biblioref">n.d.</a>)</span> to embed sign inputs. Once the encoder is trained, they use it to generate embeddings for all dictionary signs. When a user submits a query video, the system compares the input embeddings with those of the dictionary entries using Euclidean distance. Tests on a <a href="https://github.com/m-decoster/VGT-SL-Dictionary">proof-of-concept Flemish Sign Language dictionary</a> show that the system can successfully retrieve a limited vocabulary of signs, including some not in the training set.</p>
<p><span class="citation" data-cites="Cheng2023CiCoSignLanguageRetrieval">Cheng et al. (<a href="#ref-Cheng2023CiCoSignLanguageRetrieval" role="doc-biblioref">2023</a>)</span> introduce a video-to-text and text-to-video retrieval method using cross-lingual contrastive learning. Inspired by transfer learning from sign-spotting/segmentation models <span class="citation" data-cites="jui-etal-2022-machine Duarte2022SignVideoRetrivalWithTextQueries">(Jui, Bejarano, and Rivas <a href="#ref-jui-etal-2022-machine" role="doc-biblioref">2022</a>; Duarte et al. <a href="#ref-Duarte2022SignVideoRetrivalWithTextQueries" role="doc-biblioref">2022</a>)</span>, the authors employ a “domain-agnostic” I3D encoder, pretrained on large-scale sign language datasets for the sign-spotting task <span class="citation" data-cites="Varol2021ReadAndAttend">(Varol et al. <a href="#ref-Varol2021ReadAndAttend" role="doc-biblioref">2021</a>)</span>. On target datasets with continuous signing videos, they use this model with a sliding window to identify high confidence predictions, which are then used to finetune a “domain-aware” sign-spotting encoder. The two encoders each pre-extract features from videos, which are then fused via a weighted sum. Cross-lingual contrastive learning <span class="citation" data-cites="Radford2021LearningTV">(Radford et al. <a href="#ref-Radford2021LearningTV" role="doc-biblioref">2021</a>)</span> is then applied to align the extracted features with paired texts within a shared embedding space. This allows the calculation of similarity scores between text and video embeddings, and thus retrieval in either direction. Evaluations on the How2Sign <span class="citation" data-cites="dataset:duarte2020how2sign">(Duarte et al. <a href="#ref-dataset:duarte2020how2sign" role="doc-biblioref">2021</a>)</span> and RWTH-PHOENIX-Weather 2014T <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> datasets demonstrate improvement over the previous state-of-the-art <span class="citation" data-cites="Duarte2022SignVideoRetrivalWithTextQueries">(Duarte et al. <a href="#ref-Duarte2022SignVideoRetrivalWithTextQueries" role="doc-biblioref">2022</a>)</span>. Baseline retrieval results are also provided for the CSL-Daily dataset <span class="citation" data-cites="dataset:Zhou2021_SignBackTranslation_CSLDaily">(Zhou et al. <a href="#ref-dataset:Zhou2021_SignBackTranslation_CSLDaily" role="doc-biblioref">2021</a>)</span>.</p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: tie in with Automatic Dense Annotation of Large-Vocabulary Sign Language Videos, mentioned in video-to-gloss? Also uses Pseudo-labeling -->
<h3 id="fingerspelling-1">Fingerspelling</h3>
<p>Fingerspelling is spelling a word letter-by-letter, borrowing from the spoken language alphabet <span class="citation" data-cites="battison1978lexical wilcox1992phonetics brentari2001language patrie2011fingerspelled">(Battison <a href="#ref-battison1978lexical" role="doc-biblioref">1978</a>; Wilcox <a href="#ref-wilcox1992phonetics" role="doc-biblioref">1992</a>; Brentari and Padden <a href="#ref-brentari2001language" role="doc-biblioref">2001</a>; Patrie and Johnson <a href="#ref-patrie2011fingerspelled" role="doc-biblioref">2011</a>)</span>. This phenomenon, found in most signed languages, often occurs when there is no previously agreed-upon sign for a concept, like in technical language, colloquial conversations involving names, conversations involving current events, emphatic forms, and the context of code-switching between the signed language and the corresponding spoken language <span class="citation" data-cites="padden1998asl montemurro2018emphatic">(Padden <a href="#ref-padden1998asl" role="doc-biblioref">1998</a>; Montemurro and Brentari <a href="#ref-montemurro2018emphatic" role="doc-biblioref">2018</a>)</span>. The relative amount of fingerspelling varies between signed languages, and for American Sign Language (ASL), accounts for 12-35% of the signed content <span class="citation" data-cites="padden2003alphabet">(Padden and Gunsauls <a href="#ref-padden2003alphabet" role="doc-biblioref">2003</a>)</span>.</p>
<p><span class="citation" data-cites="patrie2011fingerspelled">Patrie and Johnson (<a href="#ref-patrie2011fingerspelled" role="doc-biblioref">2011</a>)</span> described the following terminology to describe three different forms of fingerspelling:</p>
<ul>
<li><strong>Careful</strong>—slower spelling where each letter pose is clearly formed.</li>
<li><strong>Rapid</strong>—quick spelling where letters are often not completed and contain remnants of other letters in the word.</li>
<li><strong>Lexicalized</strong>—a sign produced by often using no more than two letter-hand-shapes <span class="citation" data-cites="battison1978lexical">(Battison <a href="#ref-battison1978lexical" role="doc-biblioref">1978</a>)</span>.<br> For example, lexicalized <code>ALL</code> uses <code>A</code> and <code>L</code>, lexicalized <code>BUZZ</code> uses <code>B</code> and <code>Z</code>, etc…</li>
</ul>
<h4 id="recognition">Recognition</h4>
<p>Fingerspelling recognition, a sub-task of sign language recognition, is the task of recognizing fingerspelled words from a sign language video.</p>
<p><span class="citation" data-cites="dataset:fs18slt">Shi et al. (<a href="#ref-dataset:fs18slt" role="doc-biblioref">2018</a>)</span> introduced a large dataset available for American Sign Language fingerspelling recognition. This dataset includes both the “careful” and “rapid” forms of fingerspelling collected from naturally occurring videos “in the wild”, which are more challenging than studio conditions. They trained a baseline model to take a sequence of images cropped around the signing hand and either use an autoregressive decoder or a CTC. They found that the CTC outperformed the autoregressive decoder model, but both achieved poor recognition rates (35-41% character level accuracy) compared to human performance (around 82%).</p>
<p>In follow-up work, <span class="citation" data-cites="dataset:fs18iccv">Shi et al. (<a href="#ref-dataset:fs18iccv" role="doc-biblioref">2019</a>)</span> collected nearly an order-of-magnitude larger dataset and designed a new recognition model. Instead of detecting the signing hand, they detected the face and cropped a large area around it. Then, they performed an iterative process of zooming in to the hand using visual attention to retain sufficient information in high resolution of the hand. Finally, like their previous work, they encoded the image hand crops sequence and used a CTC to obtain the frame labels. They showed that this method outperformed their original “hand crop” method by 4% and that they could achieve up to 62.3% character-level accuracy using the additional data collected. Looking through this dataset, we note that the videos in the dataset were taken from longer videos, and as they were cut, they did not retain the signing before the fingerspelling. This context relates to language modeling, where at first, one fingerspells a word carefully, and when repeating it, might fingerspell it rapidly, but the interlocutors can infer they are fingerspelling the same word.</p>
<h4 id="production">Production</h4>
<p>Fingerspelling production, a sub-task of sign language production, is the task of producing a fingerspelling video for words.</p>
<p>In its basic form, “careful” fingerspelling production can be trivially solved using pre-defined letter handshapes interpolation. <span class="citation" data-cites="adeline2013fingerspell">Adeline (<a href="#ref-adeline2013fingerspell" role="doc-biblioref">2013</a>)</span> demonstrated this approach for American Sign Language and English fingerspelling. They rigged a hand armature for each letter in the English alphabet (<span class="math inline"><em>N</em> = 26</span>) and generated all (<span class="math inline"><em>N</em><sup>2</sup> = 676</span>) transitions between every two letters using interpolation or manual animation. Then, to fingerspell entire words, they chain pairs of letter transitions. For example, for the word “CHLOE”, they would chain the following transitions sequentially: <code>#C</code> <code>CH</code> <code>HL</code> <code>LO</code> <code>OE</code> <code>E#</code>.</p>
<p>However, to produce life-like animations, one must also consider the rhythm and speed of holding letters, and transitioning between letters, as those can affect how intelligible fingerspelling motions are to an interlocutor (<span class="citation" data-cites="wilcox1992phonetics">Wilcox (<a href="#ref-wilcox1992phonetics" role="doc-biblioref">1992</a>)</span>). <span class="citation" data-cites="wheatland2016analysis">Wheatland et al. (<a href="#ref-wheatland2016analysis" role="doc-biblioref">2016</a>)</span> analyzed both “careful” and “rapid” fingerspelling videos for these features. They found that for both forms of fingerspelling, on average, the longer the word, the shorter the transition and hold time. Furthermore, they found that less time is spent on middle letters on average, and the last letter is held on average for longer than the other letters in the word. Finally, they used this information to construct an animation system using letter pose interpolation and controlled the timing using a data-driven statistical model.</p>
<h3 id="pretraining-and-representation-learning">Pretraining and Representation-learning</h3>
<!-- SignBERT, SignBERT+, BEST. Possibly also Sign-VQ or CV-SLT can be discussed here -->
<p>In this paradigm, rather than targeting a specific task (e.g. pose-to-text), the aim is to learn a generally-useful Sign Language Understanding model or representation which can be applied or finetuned to specific downstream tasks.</p>
<p><span class="citation" data-cites="hu2023SignBertPlus">Hu et al. (<a href="#ref-hu2023SignBertPlus" role="doc-biblioref">2023</a>)</span> introduce SignBERT+, a self-supervised pretraining method for sign language understanding (SLU) based on masked modeling of pose sequences. This is an extension of their earlier SignBERT <span class="citation" data-cites="hu2021SignBert">(H. Hu, Zhao, et al. <a href="#ref-hu2021SignBert" role="doc-biblioref">2021</a>)</span>, with several improvements. For pretraining they extract pose sequences from over 230k videos using MMPose <span class="citation" data-cites="mmpose2020">(Contributors <a href="#ref-mmpose2020" role="doc-biblioref">2020</a>)</span>. They then perform multi-level masked modeling (joints, frames, clips) on these sequences, integrating a statistical hand model <span class="citation" data-cites="romero2017MANOHandModel">(Romero, Tzionas, and Black <a href="#ref-romero2017MANOHandModel" role="doc-biblioref">2017</a>)</span> to constrain the decoder’s predictions for anatomical realism and enhanced accuracy. Validation on isolated SLR (MS-ASL <span class="citation" data-cites="dataset:joze2018ms">(Joze and Koller <a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>)</span>, WLASL <span class="citation" data-cites="dataset:li2020word">(Li et al. <a href="#ref-dataset:li2020word" role="doc-biblioref">2020</a>)</span>, SLR500 <span class="citation" data-cites="huang2019attention3DCNNsSLR">(Huang et al. <a href="#ref-huang2019attention3DCNNsSLR" role="doc-biblioref">2019</a>)</span>), continuous SLR (RWTH-PHOENIX-Weather <span class="citation" data-cites="koller2015ContinuousSLR">(Koller, Forster, and Ney <a href="#ref-koller2015ContinuousSLR" role="doc-biblioref">2015</a>)</span>), and SLT (RWTH-PHOENIX-Weather 2014T <span class="citation" data-cites="dataset:forster2014extensions cihan2018neural">(Forster et al. <a href="#ref-dataset:forster2014extensions" role="doc-biblioref">2014</a>; Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span>) demonstrates state-of-the-art performance.</p>
<!-- BEST seems to be **B**ERT pre-training for **S**ign language recognition with coupling **T**okenization -->
<p><span class="citation" data-cites="Zhao2023BESTPretrainingSignLanguageRecognition">Zhao et al. (<a href="#ref-Zhao2023BESTPretrainingSignLanguageRecognition" role="doc-biblioref">2023</a>)</span> introduce BEST (BERT Pre-training for Sign Language Recognition with Coupling Tokenization), a pre-training method based on masked modeling of pose sequences using a coupled tokenization scheme. This method takes pose triplet units (left hand, right hand, and upper-body with arms) as inputs, each tokenized into discrete codes <span class="citation" data-cites="van_den_Oord_2017NeuralDiscreteRepresentationLearning">(Oord, Vinyals, and Kavukcuoglu <a href="#ref-van_den_Oord_2017NeuralDiscreteRepresentationLearning" role="doc-biblioref">2017</a>)</span> that are then coupled together. Masked modeling is then applied, where any or all components of the triplet (left hand, right hand, or upper-body) may be masked, to learn hierarchical correlations among them. Unlike <span class="citation" data-cites="hu2023SignBertPlus">Hu et al. (<a href="#ref-hu2023SignBertPlus" role="doc-biblioref">2023</a>)</span>, BEST does not mask multi-frame pose sequences or individual joints. The authors validate their pre-training method on isolated sign recognition (ISR) tasks using MS-ASL <span class="citation" data-cites="dataset:joze2018ms">(Joze and Koller <a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>)</span>, WLASL <span class="citation" data-cites="dataset:li2020word">(Li et al. <a href="#ref-dataset:li2020word" role="doc-biblioref">2020</a>)</span>, SLR500 <span class="citation" data-cites="huang2019attention3DCNNsSLR">(Huang et al. <a href="#ref-huang2019attention3DCNNsSLR" role="doc-biblioref">2019</a>)</span>, and NMFs-CSL <span class="citation" data-cites="hu2021NMFAwareSLR">(H. Hu, Zhou, et al. <a href="#ref-hu2021NMFAwareSLR" role="doc-biblioref">2021</a>)</span>. Besides pose-to-gloss, they also experiment with video-to-gloss tasks via fusion with I3D <span class="citation" data-cites="carreira2017quo">(Carreira and Zisserman <a href="#ref-carreira2017quo" role="doc-biblioref">2017</a>)</span>. Results on these datasets demonstrate state-of-the-art performance compared to previous methods and are comparable to those of SignBERT+ <span class="citation" data-cites="hu2023SignBertPlus">(Hu et al. <a href="#ref-hu2023SignBertPlus" role="doc-biblioref">2023</a>)</span>.</p>
<h2 id="annotation-tools">Annotation Tools</h2>
<h5 id="elan---eudico-linguistic-annotator">ELAN - EUDICO Linguistic Annotator</h5>
<p><a href="https://archive.mpi.nl/tla/elan">ELAN</a> <span class="citation" data-cites="wittenburg2006elan">(Wittenburg et al. <a href="#ref-wittenburg2006elan" role="doc-biblioref">2006</a>)</span> is an annotation tool for audio and video recordings. With ELAN, a user can add an unlimited number of textual annotations to audio and/or video recordings. An annotation can be a sentence, word, gloss, comment, translation, or description of any feature observed in the media. Annotations can be created on multiple layers, called tiers, which can be hierarchically interconnected. An annotation can either be time-aligned to the media or refer to other existing annotations. The content of annotations consists of Unicode text, and annotation documents are stored in an XML format (EAF). ELAN is open source (<a href="https://en.wikipedia.org/wiki/GNU_General_Public_License#Version_3">GPLv3</a>), and installation is <a href="https://archive.mpi.nl/tla/elan/download">available</a> for Windows, macOS, and Linux. PyMPI <span class="citation" data-cites="pympi-1.69">(Lubbers and Torreira <a href="#ref-pympi-1.69" role="doc-biblioref">2013</a>)</span> allows for simple python interaction with Elan files.</p>
<h5 id="ilex">iLex</h5>
<p><a href="https://www.sign-lang.uni-hamburg.de/ilex/">iLex</a> <span class="citation" data-cites="hanke2002ilex">(Hanke <a href="#ref-hanke2002ilex" role="doc-biblioref">2002</a>)</span> is a tool for sign language lexicography and corpus analysis, that combines features found in empirical sign language lexicography and sign language discourse transcription. It supports the user in integrated lexicon building while working on the transcription of a corpus and offers several unique features considered essential due to the specific nature of signed languages. iLex binaries are <a href="https://www.sign-lang.uni-hamburg.de/ilex/ilex.xml">available</a> for macOS.</p>
<h5 id="signstream">SignStream</h5>
<p><a href="http://www.bu.edu/asllrp/SignStream/3/">SignStream</a> <span class="citation" data-cites="neidle2001signstream">(Neidle, Sclaroff, and Athitsos <a href="#ref-neidle2001signstream" role="doc-biblioref">2001</a>)</span> is a tool for linguistic annotations and computer vision research on visual-gestural language data SignStream installation is <a href="http://www.bu.edu/asllrp/SignStream/3/download-newSS.html">available</a> for macOS and is distributed under an MIT license.</p>
<h5 id="anvil---the-video-annotation-research-tool">Anvil - The Video Annotation Research Tool</h5>
<p><a href="http://www.anvil-software.de/">Anvil</a> <span class="citation" data-cites="kipp2001anvil">(Kipp <a href="#ref-kipp2001anvil" role="doc-biblioref">2001</a>)</span> is a free video annotation tool, offering multi-layered annotation based on a user-defined coding scheme. In Anvil, the annotator can see color-coded elements on multiple tracks in time alignment. Some special features are cross-level links, non-temporal objects, timepoint tracks, coding agreement analysis, 3D viewing of motion capture data and a project tool for managing whole corpora of annotation files. Anvil installation is <a href="http://www.anvil-software.de/download/index.html">available</a> for Windows, macOS, and Linux.</p>
<h2 id="resources">Resources</h2>
<h6 id="dataset-papers">Dataset Papers</h6>
<p>Research papers which do not necessarily contribute new theory or architectures are actually important and useful enablers of other research. Furthermore, the advancement of the dataset creation process itself is important, and the pipeline of creation and curation is a potential target for improvements and advancements.</p>
<p><span class="citation" data-cites="dataset:joshi-etal-2023-isltranslate">Joshi, Agrawal, and Modi (<a href="#ref-dataset:joshi-etal-2023-isltranslate" role="doc-biblioref">2023</a>)</span> introduce ISLTranslate, a large translation dataset for Indian Sign Language based on publicly available educational videos intended for hard-of-hearing children, which happen to contain both Indian Sign Language and English audio voiceover conveying the same content. They use a speech-to-text model to transcribe the audio content, which they later manually corrected with the help of accompanying books also containing the same content. They also use MediaPipe to extract pose features, and have a certified ISL signer validate a small portion of the sign-text pairs. They provide a baseline based on the architecture proposed in <span class="citation" data-cites="camgoz2020sign">N. C. Camgöz et al. (<a href="#ref-camgoz2020sign" role="doc-biblioref">2020</a><a href="#ref-camgoz2020sign" role="doc-biblioref">b</a>)</span>, and provide code.</p>
<section id="bilingual-dictionaries" class="unnumbered">
<h6 class="unnumbered">Bilingual dictionaries</h6>
<p>for signed language <span class="citation" data-cites="dataset:mesch2012meaning fenlon2015building crasborn2016ngt dataset:gutierrez2016lse">(Mesch and Wallin <a href="#ref-dataset:mesch2012meaning" role="doc-biblioref">2012</a>; Fenlon, Cormier, and Schembri <a href="#ref-fenlon2015building" role="doc-biblioref">2015</a>; Crasborn et al. <a href="#ref-crasborn2016ngt" role="doc-biblioref">2016</a>; Gutierrez-Sigut et al. <a href="#ref-dataset:gutierrez2016lse" role="doc-biblioref">2016</a>)</span> map a spoken language word or short phrase to a signed language video. One notable dictionary, SpreadTheSign is a parallel dictionary containing around 25,000 words with up to 42 different spoken-signed language pairs and more than 600,000 videos in total. Unfortunately, while dictionaries may help create lexical rules between languages, they do not demonstrate the grammar or the usage of signs in context.</p>
</section>
<section id="fingerspelling-corpora" class="unnumbered">
<h6 class="unnumbered">Fingerspelling corpora</h6>
<p>usually consist of videos of words borrowed from spoken languages that are signed letter-by-letter. They can be synthetically created <span class="citation" data-cites="dataset:dreuw2006modeling">(Dreuw et al. <a href="#ref-dataset:dreuw2006modeling" role="doc-biblioref">2006</a>)</span> or mined from online resources <span class="citation" data-cites="dataset:fs18slt dataset:fs18iccv">(Shi et al. <a href="#ref-dataset:fs18slt" role="doc-biblioref">2018</a>, <a href="#ref-dataset:fs18iccv" role="doc-biblioref">2019</a>)</span>. However, they only capture one aspect of signed languages.</p>
</section>
<section id="isolated-sign-corpora" class="unnumbered">
<h6 class="unnumbered">Isolated sign corpora</h6>
<p>are collections of annotated single signs. They are synthesized <span class="citation" data-cites="dataset:ebling2018smile dataset:huang2018video dataset:sincan2020autsl dataset:hassan-etal-2020-isolated">(Ebling et al. <a href="#ref-dataset:ebling2018smile" role="doc-biblioref">2018</a>; Huang et al. <a href="#ref-dataset:huang2018video" role="doc-biblioref">2018</a>; Sincan and Keles <a href="#ref-dataset:sincan2020autsl" role="doc-biblioref">2020</a>; Hassan et al. <a href="#ref-dataset:hassan-etal-2020-isolated" role="doc-biblioref">2020</a>)</span> or mined from online resources <span class="citation" data-cites="dataset:joze2018ms dataset:li2020word">(Joze and Koller <a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>; Li et al. <a href="#ref-dataset:li2020word" role="doc-biblioref">2020</a>)</span>, and can be used for isolated sign language recognition or contrastive analysis of minimal signing pairs <span class="citation" data-cites="dataset:imashev2020dataset">(Imashev et al. <a href="#ref-dataset:imashev2020dataset" role="doc-biblioref">2020</a>)</span>. However, like dictionaries, they do not describe relations between signs, nor do they capture coarticulation during the signing, and are often limited in vocabulary size (20-1000 signs).</p>
</section>
<section id="continuous-sign-corpora" class="unnumbered">
<h6 class="unnumbered">Continuous sign corpora</h6>
<p>contain parallel sequences of signs and spoken language. Available continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken language machine translation <span class="citation" data-cites="arivazhagan2019massively">(Arivazhagan et al. <a href="#ref-arivazhagan2019massively" role="doc-biblioref">2019</a>)</span>. Moreover, while automatic speech recognition (ASR) datasets contain up to 50,000 hours of recordings <span class="citation" data-cites="pratap2020mls">(Pratap et al. <a href="#ref-pratap2020mls" role="doc-biblioref">2020</a>)</span>, the most extensive continuous sign language corpus contains only 1,150 hours, and only 50 of them are publicly available <span class="citation" data-cites="dataset:hanke-etal-2020-extending">(Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span>. These datasets are usually synthesized <span class="citation" data-cites="dataset:databases2007volumes dataset:Crasborn2008TheCN dataset:ko2019neural dataset:hanke-etal-2020-extending">(Databases <a href="#ref-dataset:databases2007volumes" role="doc-biblioref">2007</a>; Crasborn and Zwitserlood <a href="#ref-dataset:Crasborn2008TheCN" role="doc-biblioref">2008</a>; Ko et al. <a href="#ref-dataset:ko2019neural" role="doc-biblioref">2019</a>; Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span> or recorded in studio conditions <span class="citation" data-cites="dataset:forster2014extensions cihan2018neural">(Forster et al. <a href="#ref-dataset:forster2014extensions" role="doc-biblioref">2014</a>; Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span>, which does not account for noise in real-life conditions. Moreover, some contain signed interpretations of spoken language rather than naturally-produced signs, which may not accurately represent native signing since translation is now a part of the discourse event.</p>
</section>
<section id="availability" class="unnumbered">
<h6 class="unnumbered">Availability</h6>
<p>Unlike the vast amount and diversity of available spoken language resources that allow various applications, sign language resources are scarce and, currently only support translation and production. Unfortunately, most of the sign language corpora discussed in the literature are either not available for use or available under heavy restrictions and licensing terms. Furthermore, sign language data is especially challenging to anonymize due to the importance of facial and other physical features in signing videos, limiting its open distribution. Developing anonymization with minimal information loss or accurate anonymous representations is a promising research direction.</p>
<!-- <span style="background-color: red; color: white; padding: 0 2px !important;">**TODO**</span>: a discussion on anonymization methods, including the thoughts of @rust2024PrivacyAwareSign, who mention a few approaches and cite them. They also argue that poses "do not offer meaningful privacy protection either" (Appendix A).  -->
</section>
<h3 id="collect-real-world-data">Collect Real-World Data</h3>
<p>Data is essential to develop any of the core NLP tools previously described, and current efforts in SLP are often limited by the lack of adequate data. We discuss the considerations to keep in mind when building datasets, the challenges of collecting such data, and directions to facilitate data collection.</p>
<section id="what-is-good-signed-language-data" class="unnumbered">
<h6 class="unnumbered">What is Good Signed Language Data?</h6>
<p>For SLP models to be deployable, they must be developed using data that represents the real world accurately. What constitutes an ideal signed language dataset is an open question, we suggest including the following requirements: (1) a broad domain; (2) sufficient data and vocabulary size; (3) real-world conditions; (4) naturally produced signs; (5) a diverse signer demographic; (6) native signers; and when applicable, (7) dense annotations.</p>
<p>To illustrate the importance of data quality during modeling, <span class="citation" data-cites="yin-etal-2021-including">Yin et al. (<a href="#ref-yin-etal-2021-including" role="doc-biblioref">2021</a>)</span> first take as an example a current benchmark for SLP, the RWTH-PHOENIX-Weather 2014T dataset <span class="citation" data-cites="cihan2018neural">(Camgöz et al. <a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span> of German Sign Language, that does not meet most of the above criteria: it is restricted to the weather domain (1); contains only around 8K segments with 1K unique signs (2); filmed in studio conditions (3); interpreted from German utterances (4); and signed by nine Caucasian interpreters (5,6). Although this dataset successfully addressed data scarcity issues at the time and successfully rendered results comparable and fueled competitive research, it does not accurately represent signed languages in the real world. On the other hand, the Public DGS Corpus <span class="citation" data-cites="dataset:hanke-etal-2020-extending">(Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span> is an open-domain (1) dataset consisting of 50 hours of natural signing (4) by 330 native signers from various regions in Germany (5,6), annotated with glosses, HamNoSys and German translations (7), meeting all but two requirements we suggest.</p>
<p>They train a gloss-to-text sign language translation transformer <span class="citation" data-cites="yin-read-2020-better">(Yin and Read <a href="#ref-yin-read-2020-better" role="doc-biblioref">2020</a>)</span> on both datasets. On RWTH-PHOENIX-Weather 2014T, they obtain <strong>22.17</strong> BLEU on testing; on Public DGS Corpus, they obtain a mere  BLEU. Although Transformers achieve encouraging results on RWTH-PHOENIX-Weather 2014T <span class="citation" data-cites="saunders2020progressive camgoz2020multi">(Saunders, Camgöz, and Bowden <a href="#ref-saunders2020progressive" role="doc-biblioref">2020</a><a href="#ref-saunders2020progressive" role="doc-biblioref">b</a>; N. C. Camgöz et al. <a href="#ref-camgoz2020multi" role="doc-biblioref">2020</a><a href="#ref-camgoz2020multi" role="doc-biblioref">a</a>)</span>, they fail on more realistic, open-domain data. These results reveal that, for real-world applications, we need more data to train such models. At the same time, available data is severely limited in size; less data-hungry and more linguistically-informed approaches may be more suitable. This experiment reveals how it is crucial to use data that accurately represent the complexity and diversity of signed languages to precisely assess what types of methods are suitable and how well our models would deploy to the real world.</p>
</section>
<section id="challenges-of-data-collection" class="unnumbered">
<h6 class="unnumbered">Challenges of Data Collection</h6>
<p>Collecting and annotating signed data in line with the ideal requires more resources than speech or text data, taking up to 600 minutes per minute of an annotated signed language video <span class="citation" data-cites="dataset:hanke-etal-2020-extending">(Hanke et al. <a href="#ref-dataset:hanke-etal-2020-extending" role="doc-biblioref">2020</a>)</span>. Moreover, annotation usually requires specific knowledge and skills, which makes recruiting or training qualified annotators challenging. Additionally, there is little existing signed language data in the wild openly licensed for use, especially from native signers that are not interpretations of speech. Therefore, data collection often requires significant efforts and costs of on-site recording.</p>
</section>
<section id="automating-annotation" class="unnumbered">
<h6 class="unnumbered">Automating Annotation</h6>
<p>One helpful research direction for collecting more data that enables the development of deployable SLP models is creating tools that can simplify or automate parts of the collection and annotation process. One of the most significant bottlenecks in obtaining more adequate signed language data is the time and scarcity of experts required to perform annotation. Therefore, tools that perform automatic parsing, detection of frame boundaries, extraction of articulatory features, suggestions for lexical annotations, and allow parts of the annotation process to be crowdsourced to non-experts, to name a few, have a high potential to facilitate and accelerate the availability of good data.</p>
</section>
<h3 id="practice-deaf-collaboration">Practice Deaf Collaboration</h3>
<p>Finally, when working with signed languages, it is vital to keep in mind  this technology should benefit and  they need. Researchers in SLP should acknowledge that signed languages belong to the Deaf community and avoid exploiting their language as a commodity <span class="citation" data-cites="bird-2020-decolonising">(Bird <a href="#ref-bird-2020-decolonising" role="doc-biblioref">2020</a>)</span>.</p>
<section id="solving-real-needs" class="unnumbered">
<h6 class="unnumbered">Solving Real Needs</h6>
<p>Many efforts in SLP have developed intrusive methods (e.g., requiring signers to wear special gloves), which are often rejected by signing communities and therefore have limited real-world value. Such efforts are often marketed to perform “sign language translation” when they, in fact, only identify fingerspelling or recognize a minimal set of isolated signs at best. These approaches oversimplify the rich grammar of signed languages, promote the misconception that signs are solely expressed through the hands, and are considered by the Deaf community as a manifestation of audism, where it is the signers who must make the extra effort to wear additional sensors to be understood by non-signers <span class="citation" data-cites="erard2017sign">(Erard <a href="#ref-erard2017sign" role="doc-biblioref">2017</a>)</span>. To avoid such mistakes, we encourage close Deaf involvement throughout the research process to ensure that we direct our efforts toward applications that will be adopted by signers and do not make false assumptions about signed languages or the needs of signing communities.</p>
</section>
<section id="building-collaboration" class="unnumbered">
<h6 class="unnumbered">Building Collaboration</h6>
<p>Deaf collaborations and leadership are essential for developing signed language technologies to ensure they address the community’s needs and will be adopted, not relying on misconceptions or inaccuracies about signed language <span class="citation" data-cites="harris2009research kusters2017innovations">(Harris, Holmes, and Mertens <a href="#ref-harris2009research" role="doc-biblioref">2009</a>; Annelies Kusters, De Meulder, and O’Brien <a href="#ref-kusters2017innovations" role="doc-biblioref">2017</a>)</span>. Hearing researchers cannot relate to the deaf experience or fully understand the context in which the tools being developed would be used, nor can they speak for the deaf. Therefore, we encourage creating a long-term collaborative environment between signed language researchers and users so that deaf users can identify meaningful challenges and provide insights on the considerations to take while researchers cater to the signers’ needs as the field evolves. We also recommend reaching out to signing communities for reviewing papers on signed languages to ensure an adequate evaluation of this type of research results published at academic venues. There are several ways to connect with Deaf communities for collaboration: one can seek deaf students in their local community, reach out to schools for the deaf, contact deaf linguists, join a network of researchers of sign-related technologies, and/or participate in deaf-led projects.</p>
</section>
<h3 id="downloading">Downloading</h3>
<p>Currently, there is no easy way or agreed-upon format to download and load sign language datasets, and as such, evaluation of these datasets is scarce. As part of this work, we streamlined the loading of available datasets using <a href="https://github.com/tensorflow/datasets">Tensorflow Datasets</a> <span class="citation" data-cites="TFDS">(authors <a href="#ref-TFDS" role="doc-biblioref">2019</a>)</span>. This tool allows researchers to load large and small datasets alike with a simple command and be comparable to other works. We make these datasets available using a custom library, <a href="https://github.com/sign-language-processing/datasets">Sign Language Datasets</a> <span class="citation" data-cites="moryossef2021datasets">(Moryossef and Müller <a href="#ref-moryossef2021datasets" role="doc-biblioref">2021</a>)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> sign_language_datasets.datasets</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># Loading a dataset with default configuration</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>aslg_pc12 <span class="op">=</span> tfds.load(<span class="st">&quot;aslg_pc12&quot;</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Loading a dataset with custom configuration</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">from</span> sign_language_datasets.datasets.config <span class="im">import</span> SignDatasetConfig</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>config <span class="op">=</span> SignDatasetConfig(</span>
<span id="cb1-11"><a href="#cb1-11"></a>    name<span class="op">=</span><span class="st">&quot;videos_and_poses256x256:12&quot;</span>,</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co"># Specific version</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    version<span class="op">=</span><span class="st">&quot;3.0.0&quot;</span>,</span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="co"># Download, and load dataset videos</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>    include_video<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-16"><a href="#cb1-16"></a>    <span class="co"># Load videos at constant, 12 fps</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>    fps<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="co"># Convert videos to a constant resolution, 256x256</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>    resolution<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>),</span>
<span id="cb1-20"><a href="#cb1-20"></a>    <span class="co"># Download and load Holistic pose estimation</span></span>
<span id="cb1-21"><a href="#cb1-21"></a>    include_pose<span class="op">=</span><span class="st">&quot;holistic&quot;</span>)</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a>rwth_phoenix2014_t <span class="op">=</span> tfds.load(</span>
<span id="cb1-24"><a href="#cb1-24"></a>    name<span class="op">=</span><span class="st">&#39;rwth_phoenix2014_t&#39;</span>,</span>
<span id="cb1-25"><a href="#cb1-25"></a>    builder_kwargs<span class="op">=</span><span class="bu">dict</span>(config<span class="op">=</span>config))</span></code></pre></div>
<p>Furthermore, we follow a unified interface when possible, making attributes the same and comparable between datasets:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>{</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="st">&quot;id&quot;</span>: tfds.features.Text(),</span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="st">&quot;signer&quot;</span>: tfds.features.Text() <span class="op">|</span> tf.int32,</span>
<span id="cb2-4"><a href="#cb2-4"></a>    <span class="st">&quot;video&quot;</span>: tfds.features.Video(</span>
<span id="cb2-5"><a href="#cb2-5"></a>        shape<span class="op">=</span>(<span class="va">None</span>, HEIGHT, WIDTH, <span class="dv">3</span>)),</span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="st">&quot;depth_video&quot;</span>: tfds.features.Video(</span>
<span id="cb2-7"><a href="#cb2-7"></a>        shape<span class="op">=</span>(<span class="va">None</span>, HEIGHT, WIDTH, <span class="dv">1</span>)),</span>
<span id="cb2-8"><a href="#cb2-8"></a>    <span class="st">&quot;fps&quot;</span>: tf.int32,</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="st">&quot;pose&quot;</span>: {</span>
<span id="cb2-10"><a href="#cb2-10"></a>        <span class="st">&quot;data&quot;</span>: tfds.features.Tensor(</span>
<span id="cb2-11"><a href="#cb2-11"></a>            shape<span class="op">=</span>(<span class="va">None</span>, <span class="dv">1</span>, POINTS, CHANNELS),</span>
<span id="cb2-12"><a href="#cb2-12"></a>            dtype<span class="op">=</span>tf.float32),</span>
<span id="cb2-13"><a href="#cb2-13"></a>        <span class="st">&quot;conf&quot;</span>: tfds.features.Tensor(</span>
<span id="cb2-14"><a href="#cb2-14"></a>            shape<span class="op">=</span>(<span class="va">None</span>, <span class="dv">1</span>, POINTS),</span>
<span id="cb2-15"><a href="#cb2-15"></a>            dtype<span class="op">=</span>tf.float32)</span>
<span id="cb2-16"><a href="#cb2-16"></a>    },</span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="st">&quot;gloss&quot;</span>: tfds.features.Text(),</span>
<span id="cb2-18"><a href="#cb2-18"></a>    <span class="st">&quot;text&quot;</span>: tfds.features.Text()</span>
<span id="cb2-19"><a href="#cb2-19"></a>}</span></code></pre></div>
<h2 id="list-of-datasets">List of Datasets</h2>
<p>The following table contains a curated list of datasets, including various signed languages and data formats:</p>
<p>🎥 Video | 👋 Pose | 👄 Mouthing | ✍ Notation | 📋 Gloss | 📜 Text | 🔊 Speech</p>
<div id="datasets-table" class="table">
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Publication</th>
<th>Language</th>
<th>Features</th>
<th>#Signs</th>
<th>#Samples</th>
<th>#Signers</th>
<th>License</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://nyu.databrary.org/volume/1062">ASL-100-RGBD</a></td>
<td><span class="citation" data-cites="dataset:hassan-etal-2020-isolated">Hassan et al. (<a href="#ref-dataset:hassan-etal-2020-isolated" role="doc-biblioref">2020</a>)</span></td>
<td>American</td>
<td><span title="video:RGBD">🎥</span><span title="pose:Kinect">👋</span><span title="gloss:ASL">📋</span></td>
<td>100</td>
<td>4,150 Tokens</td>
<td>22</td>
<td><a href="https://nyu.databrary.org/volume/1062">Authorized Academics</a></td>
</tr>
<tr class="even">
<td><a href="https://nyu.databrary.org/volume/1249">ASL-Homework-RGBD</a></td>
<td><span class="citation" data-cites="dataset:hassan-etal-2022-asl-homework">Hassan et al. (<a href="#ref-dataset:hassan-etal-2022-asl-homework" role="doc-biblioref">2022</a>)</span></td>
<td>American</td>
<td><span title="video:RGBD">🎥</span><span title="pose:Kinect">👋</span><span title="gloss:ASL">📋</span></td>
<td></td>
<td>935</td>
<td>45</td>
<td><a href="https://nyu.databrary.org/volume/1249">Authorized Academics</a></td>
</tr>
<tr class="odd">
<td><a href="https://doi.org/10.1093/deafed/enaa038">ASL-LEX</a></td>
<td><span class="citation" data-cites="dataset:sehyr2021asl">Sehyr et al. (<a href="#ref-dataset:sehyr2021asl" role="doc-biblioref">2021</a>)</span></td>
<td>American</td>
<td><span title="gloss:ASL">📋</span></td>
<td>2,723</td>
<td>2723 glosses+linguistic annotations, video downloads not allowed</td>
<td></td>
<td><a href="https://asl-lex.org/download.html">CC BY-NC 4.0</a></td>
</tr>
<tr class="even">
<td><a href="https://achrafothman.net/site/asl-smt/">ASLG-PC12</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/aslg_pc12">💾</a></td>
<td><span class="citation" data-cites="dataset:othman2012english">Othman and Jemni (<a href="#ref-dataset:othman2012english" role="doc-biblioref">2012</a>)</span></td>
<td>American (Synthetic)</td>
<td><span title="gloss:ASL">📋</span><span title="text:English">📜</span></td>
<td></td>
<td>&gt; 100,000,000 Sentences</td>
<td>N/A</td>
<td>Sample Available (<a href="http://www.achrafothman.net/aslsmt/corpus/sample-corpus-asl-en.asl">1</a>, <a href="http://www.achrafothman.net/aslsmt/corpus/sample-corpus-asl-en.en">2</a>)</td>
</tr>
<tr class="odd">
<td><a href="https://crystal.uta.edu/~athitsos/projects/asl_lexicon/">ASLLVD</a></td>
<td><span class="citation" data-cites="dataset:athitsos2008american">Athitsos et al. (<a href="#ref-dataset:athitsos2008american" role="doc-biblioref">2008</a>)</span>,athitsos2010LargeLexiconIndexingRetrieval</td>
<td>American</td>
<td><span title="gloss:ASL">📋</span><span title="video:RGB">🎥</span></td>
<td>3,000</td>
<td>12,000 Samples</td>
<td>4</td>
<td>Attribution</td>
</tr>
<tr class="even">
<td>ATIS</td>
<td><span class="citation" data-cites="dataset:bungeroth2008atis">Bungeroth et al. (<a href="#ref-dataset:bungeroth2008atis" role="doc-biblioref">2008</a>)</span></td>
<td>Multilingual</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>292</td>
<td>595 Sentences</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://elar.soas.ac.uk/Collection/MPI55247">AUSLAN</a></td>
<td><span class="citation" data-cites="dataset:johnston2010archive">Johnston (<a href="#ref-dataset:johnston2010archive" role="doc-biblioref">2008</a>)</span></td>
<td>Australian</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td></td>
<td>1,100 Videos</td>
<td>100</td>
<td></td>
</tr>
<tr class="even">
<td><a href="http://chalearnlap.cvc.uab.es/dataset/40/description/">AUTSL</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/autsl">💾</a></td>
<td><span class="citation" data-cites="dataset:sincan2020autsl">Sincan and Keles (<a href="#ref-dataset:sincan2020autsl" role="doc-biblioref">2020</a>)</span></td>
<td>Turkish</td>
<td><span title="video:RGBD">🎥</span><span title="gloss:TİD">📋</span></td>
<td>226</td>
<td>36,302 Samples</td>
<td>43</td>
<td><a href="https://competitions.codalab.org/competitions/27901#participate">Codalab</a></td>
</tr>
<tr class="odd">
<td><a href="https://www.robots.ox.ac.uk/~vgg/data/bobsl/">BOBSL</a></td>
<td><span class="citation" data-cites="dataset:momeniAutomaticDenseAnnotation2022">Momeni, Bull, Prajwal, et al. (<a href="#ref-dataset:momeniAutomaticDenseAnnotation2022" role="doc-biblioref">2022</a><a href="#ref-dataset:momeniAutomaticDenseAnnotation2022" role="doc-biblioref">b</a>)</span></td>
<td>British</td>
<td><span title="video:RGB">🎥</span><span title="text:English">📜</span></td>
<td>2,281</td>
<td>1.2M Sentences</td>
<td>37</td>
<td><a href="https://www.bbc.co.uk/rd/projects/extol-dataset">non-commercial authorized academics</a></td>
</tr>
<tr class="even">
<td><a href="https://ogulcanozdemir.github.io/bosphorussign22k/">BosphorusSign</a></td>
<td><span class="citation" data-cites="dataset:camgoz-etal-2016-bosphorussign">Camgöz et al. (<a href="#ref-dataset:camgoz-etal-2016-bosphorussign" role="doc-biblioref">2016</a>)</span></td>
<td>Turkish</td>
<td><span title="video:RGBD">🎥</span><span title="pose:Kinectv2">👋</span><span title="gloss">📋</span></td>
<td>855 (595 public)</td>
<td>22k+ (22,670 public)</td>
<td>6</td>
<td><a href="https://ogulcanozdemir.github.io/bosphorussign22k/#description">Research purpose on request</a></td>
</tr>
<tr class="odd">
<td><a href="https://bslcorpusproject.org/">BSL Corpus</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/bsl_corpus">💾</a></td>
<td><span class="citation" data-cites="dataset:schembri2013building">Schembri et al. (<a href="#ref-dataset:schembri2013building" role="doc-biblioref">2013</a>)</span></td>
<td>British</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td></td>
<td>40,000 Lexical Items</td>
<td>249</td>
<td><a href="https://bslcorpusproject.org/cava/restricted-access-data/">Partially Restricted</a></td>
</tr>
<tr class="even">
<td><a href="https://www.slownikpjm.uw.edu.pl/en">CDPSL</a></td>
<td><span class="citation" data-cites="dataset:acheta2014ACD">Łacheta and PawełRutkowski (<a href="#ref-dataset:acheta2014ACD" role="doc-biblioref">2014</a>)</span></td>
<td>Polish</td>
<td><span title="video">🎥</span><span title="writing:HamNoSys">✍</span><span title="text:Polish">📜</span></td>
<td></td>
<td>300 hours</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://ttic.uchicago.edu/~klivescu/ChicagoFSWild.htm">ChicagoFSWild</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/chicago_fs_wild">💾</a></td>
<td><span class="citation" data-cites="dataset:fs18slt">Shi et al. (<a href="#ref-dataset:fs18slt" role="doc-biblioref">2018</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="text:Fingerspelling">📜</span></td>
<td>26</td>
<td>7,304 Sequences</td>
<td>160</td>
<td>Public</td>
</tr>
<tr class="even">
<td><a href="https://ttic.uchicago.edu/~klivescu/ChicagoFSWild.htm">ChicagoFSWild+</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/chicago_fs_wild">💾</a></td>
<td><span class="citation" data-cites="dataset:fs18iccv">Shi et al. (<a href="#ref-dataset:fs18iccv" role="doc-biblioref">2019</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="text:Fingerspelling">📜</span></td>
<td>26</td>
<td>55,232 Sequences</td>
<td>260</td>
<td>Public</td>
</tr>
<tr class="odd">
<td><a href="https://www.cvssp.org/data/c4a-news-corpus/">Content4All</a></td>
<td><span class="citation" data-cites="dataset:camgoz2021content4all">Camgöz et al. (<a href="#ref-dataset:camgoz2021content4all" role="doc-biblioref">2021</a>)</span></td>
<td>Swiss-German, Flemish</td>
<td><span title="video">🎥</span><span title="pose:OpenPose">👋</span><span title="text:Swiss-German">📜</span><span title="text:Flemish">📜</span></td>
<td></td>
<td>190 Hours</td>
<td></td>
<td><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></td>
</tr>
<tr class="even">
<td><a href="http://wearables.cc.gatech.edu/projects/copycat/">CopyCat</a></td>
<td><span class="citation" data-cites="dataset:zafrulla2010novel">Zafrulla et al. (<a href="#ref-dataset:zafrulla2010novel" role="doc-biblioref">2010</a>)</span></td>
<td>American</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>22</td>
<td>420 Phrases</td>
<td>5</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://corpusngt.nl/">Corpus NGT</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/ngt_corpus">💾</a></td>
<td><span class="citation" data-cites="dataset:Crasborn2008TheCN">Crasborn and Zwitserlood (<a href="#ref-dataset:Crasborn2008TheCN" role="doc-biblioref">2008</a>)</span></td>
<td>Netherlands</td>
<td><span title="video:SignerA">🎥</span><span title="video:SignerB">🎥</span><span title="gloss">📋</span><span title="text:Dutch">📜</span></td>
<td>~3k</td>
<td>~2375 sessions</td>
<td>~90</td>
<td><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></td>
</tr>
<tr class="even">
<td><a href="http://vipl.ict.ac.cn/homepage/ksl/data.html">DEVISIGN</a></td>
<td><span class="citation" data-cites="dataset:chai2014devisign">Chai, Wang, and Chen (<a href="#ref-dataset:chai2014devisign" role="doc-biblioref">2014</a>)</span></td>
<td>Chinese</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>2,000</td>
<td>24,000 Samples</td>
<td>8</td>
<td><a href="http://vipl.ict.ac.cn/homepage/ksl/document/Agreement.pdf">Research purpose on request</a></td>
</tr>
<tr class="odd">
<td><a href="https://www.sign-lang.uni-hamburg.de/dicta-sign/portal/">Dicta-Sign</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/dicta_sign">💾</a></td>
<td><span class="citation" data-cites="dataset:matthes2012dicta">Matthes et al. (<a href="#ref-dataset:matthes2012dicta" role="doc-biblioref">2012</a>)</span></td>
<td>Multilingual</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td></td>
<td>6-8 Hours (/Participant)</td>
<td>16-18 /Language</td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://how2sign.github.io/">How2Sign</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/how2sign">💾</a></td>
<td><span class="citation" data-cites="dataset:duarte2020how2sign">Duarte et al. (<a href="#ref-dataset:duarte2020how2sign" role="doc-biblioref">2021</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="pose:OpenPose">👋</span><span title="gloss:ASL">📋</span><span title="text:English">📜</span><span title="speech:English">🔊</span></td>
<td>16,000</td>
<td>79 hours (35,000 sentences)</td>
<td>11</td>
<td><a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></td>
</tr>
<tr class="odd">
<td><a href="https://github.com/marlondcu/ISL">ISL-HS</a></td>
<td><span class="citation" data-cites="dataset:oliveiraDatasetIrishSign2017">Oliveira et al. (<a href="#ref-dataset:oliveiraDatasetIrishSign2017" role="doc-biblioref">2017</a>)</span></td>
<td>Irish</td>
<td><span title="video:RGB">🎥</span><span title="gloss:ISL-HandShapes">📋</span></td>
<td>23</td>
<td>468 videos-&gt;58,114 images-&gt;23 handshapes</td>
<td>6</td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://github.com/Exploration-Lab/ISLTranslate">ISLTranslate</a></td>
<td><span class="citation" data-cites="dataset:joshi-etal-2023-isltranslate">Joshi, Agrawal, and Modi (<a href="#ref-dataset:joshi-etal-2023-isltranslate" role="doc-biblioref">2023</a>)</span></td>
<td>Indian</td>
<td><span title="video">🎥</span><span title="text:English">📜</span><span title="pose:MediaPipe">👋</span></td>
<td>11,000</td>
<td>31k sentences</td>
<td></td>
<td><a href="https://creativecommons.org/licenses/by-nc/4.0/">License: CC BY-NC 4.0</a></td>
</tr>
<tr class="odd">
<td><a href="https://krslproject.github.io/krsl20/">K-RSL</a></td>
<td><span class="citation" data-cites="dataset:imashev2020dataset">Imashev et al. (<a href="#ref-dataset:imashev2020dataset" role="doc-biblioref">2020</a>)</span></td>
<td>Kazakh-Russian</td>
<td><span title="video">🎥</span><span title="pose:OpenPose">👋</span><span title="text:Russian">📜</span></td>
<td>600</td>
<td>28,250 Videos</td>
<td>10</td>
<td>Attribution</td>
</tr>
<tr class="even">
<td>KETI</td>
<td><span class="citation" data-cites="dataset:ko2019neural">Ko et al. (<a href="#ref-dataset:ko2019neural" role="doc-biblioref">2019</a>)</span></td>
<td>Korean</td>
<td><span title="video">🎥</span><span title="pose:OpenPose">👋</span><span title="gloss:KSL">📋</span><span title="text:Korean">📜</span></td>
<td>524</td>
<td>14,672 Videos</td>
<td>14</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span> (emailed Sang-Ki Ko)</td>
</tr>
<tr class="odd">
<td><a href="https://krslproject.github.io/online-school/">KRSL-OnlineSchool</a></td>
<td><span class="citation" data-cites="dataset:mukushev2022towards">Mukushev et al. (<a href="#ref-dataset:mukushev2022towards" role="doc-biblioref">2022</a>)</span></td>
<td>Kazakh-Russian</td>
<td><span title="video">🎥</span><span title="gloss">📋</span><span title="text:Kazakh-Russian">📜</span></td>
<td></td>
<td>890 Hours (1M sentences)</td>
<td>7</td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://link.springer.com/content/pdf/10.3758/s13428-014-0560-1.pdf">LSE-SIGN</a></td>
<td><span class="citation" data-cites="dataset:gutierrez2016lse">Gutierrez-Sigut et al. (<a href="#ref-dataset:gutierrez2016lse" role="doc-biblioref">2016</a>)</span></td>
<td>Spanish</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>2,400</td>
<td>2,400 Samples</td>
<td>2</td>
<td><a href="http://lse-sign.bcbl.eu/web-busqueda/?page_id=8">Custom</a></td>
</tr>
<tr class="odd">
<td><a href="https://www.microsoft.com/en-us/download/details.aspx?id=100121">MS-ASL</a></td>
<td><span class="citation" data-cites="dataset:joze2018ms">Joze and Koller (<a href="#ref-dataset:joze2018ms" role="doc-biblioref">2019</a>)</span></td>
<td>American</td>
<td><span title="video:RGB">🎥</span><span title="gloss">📋</span></td>
<td>1,000</td>
<td>25,513 (~25 hours)</td>
<td>222</td>
<td>Public</td>
</tr>
<tr class="even">
<td><a href="https://www.bu.edu/asllrp/ncslgr.html">NCSLGR</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/ncslgr">💾</a></td>
<td><span class="citation" data-cites="dataset:databases2007volumes">Databases (<a href="#ref-dataset:databases2007volumes" role="doc-biblioref">2007</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="gloss:ASL">📋</span><span title="text:English">📜</span></td>
<td></td>
<td>1,875 sentences</td>
<td>4</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
</tr>
<tr class="odd">
<td><a href="https://ustc-slr.github.io/datasets/2020_nmfs_csl/">NMFs-CSL</a></td>
<td><span class="citation" data-cites="hu2021NMFAwareSLR">H. Hu, Zhou, et al. (<a href="#ref-hu2021NMFAwareSLR" role="doc-biblioref">2021</a>)</span></td>
<td>Chinese</td>
<td><span title="gloss">📋</span><span title="video:RGB">🎥</span></td>
<td>1,067</td>
<td>32,010 videos</td>
<td>10</td>
<td><a href="https://ustc-slr.github.io/datasets/2020_nmfs_csl/Release%20Agreement-NMFs-CSL.pdf">Research purpose on request</a></td>
</tr>
<tr class="even">
<td><a href="https://www.kaggle.com/competitions/asl-signs/data">PopSign ASL v1.0</a></td>
<td><span class="citation" data-cites="dataset:starner_et_al_2023_PopSignASL_v1">Starner et al. (<a href="#ref-dataset:starner_et_al_2023_PopSignASL_v1" role="doc-biblioref">2023</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="gloss">📋</span></td>
<td>250</td>
<td>175023</td>
<td>47</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://www.sign-lang.uni-hamburg.de/dgs-korpus/index.php/welcome.html">Public DGS Corpus</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/dgs_corpus">💾</a></td>
<td><span class="citation" data-cites="dataset:prillwitz2008dgs">Prillwitz et al. (<a href="#ref-dataset:prillwitz2008dgs" role="doc-biblioref">2008</a>)</span></td>
<td>German</td>
<td><span title="video:Front">🎥</span><span title="video:Side">🎥</span><span title="pose:OpenPose">👋</span><span title="mouthing">👄</span><span title="writing:HamNoSys">✍</span><span title="gloss:DGS">📋</span><span title="text:German">📜</span><span title="text:English">📜</span></td>
<td></td>
<td>50 Hours</td>
<td>330</td>
<td><a href="https://www.sign-lang.uni-hamburg.de/meinedgs/ling/license_en.html">Custom</a></td>
</tr>
<tr class="even">
<td><a href="https://engineering.purdue.edu/RVL/Database/ASL/asl-database-front.htm">RVL-SLLL ASL</a></td>
<td><span class="citation" data-cites="dataset:martinez2002purdue">Martínez et al. (<a href="#ref-dataset:martinez2002purdue" role="doc-biblioref">2002</a>)</span></td>
<td>American</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>104</td>
<td>2,576 Videos</td>
<td>14</td>
<td><a href="https://engineering.purdue.edu/RVL/Database/ASL/Agreement.pdf">Research Attribution</a></td>
</tr>
<tr class="odd">
<td><a href="https://www-i6.informatik.rwth-aachen.de/aslr/fingerspelling.php">RWTH Fingerspelling</a></td>
<td><span class="citation" data-cites="dataset:dreuw2006modeling">Dreuw et al. (<a href="#ref-dataset:dreuw2006modeling" role="doc-biblioref">2006</a>)</span></td>
<td>German</td>
<td><span title="video">🎥</span><span title="text:German">📜</span></td>
<td>35</td>
<td>1,400 single-char videos</td>
<td>20</td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-104.php">RWTH-BOSTON-104</a></td>
<td><span class="citation" data-cites="dataset:dreuw2008benchmark">Dreuw et al. (<a href="#ref-dataset:dreuw2008benchmark" role="doc-biblioref">2008</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="text:English">📜</span></td>
<td>104</td>
<td>201 Sentences</td>
<td>3</td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/">RWTH-PHOENIX-Weather T</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/rwth_phoenix2014_t">💾</a></td>
<td><span class="citation" data-cites="dataset:forster2014extensions">Forster et al. (<a href="#ref-dataset:forster2014extensions" role="doc-biblioref">2014</a>)</span>;<span class="citation" data-cites="cihan2018neural">Camgöz et al. (<a href="#ref-cihan2018neural" role="doc-biblioref">2018</a>)</span></td>
<td>German</td>
<td><span title="video">🎥</span><span title="gloss:DGS">📋</span><span title="text:German">📜</span></td>
<td>1,231</td>
<td>8,257 Sentences</td>
<td>9</td>
<td><a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a></td>
</tr>
<tr class="even">
<td><a href="https://research.cs.aalto.fi/cbir/data/s-pot/">S-pot</a></td>
<td><span class="citation" data-cites="dataset:viitaniemi-etal-2014-pot">Viitaniemi et al. (<a href="#ref-dataset:viitaniemi-etal-2014-pot" role="doc-biblioref">2014</a>)</span></td>
<td>Finnish</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>1,211</td>
<td>5,539 Videos</td>
<td>5</td>
<td><a href="mailto:leena.savolainen@kuurojenliitto.fi">Permission</a></td>
</tr>
<tr class="odd">
<td><a href="https://sign2mint.de/">Sign2MINT</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/sign2mint">💾</a></td>
<td>2021</td>
<td>German</td>
<td><span title="video">🎥</span><span title="writing:SignWriting">✍</span><span title="text:German">📜</span></td>
<td>740</td>
<td>1135</td>
<td></td>
<td><a href="https://creativecommons.org/licenses/by-nc-sa/3.0/de/">CC BY-NC-SA 3.0 DE</a></td>
</tr>
<tr class="even">
<td><a href="https://www.signbank.org/signpuddle/">SignBank</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/signbank">💾</a></td>
<td></td>
<td>Multilingual</td>
<td><span title="video">🎥</span><span title="writing:SignWriting">✍</span><span title="text:Multilingual">📜</span></td>
<td></td>
<td>222148</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://zenodo.org/records/6779843">SignBD-Word</a></td>
<td><span class="citation" data-cites="dataset:samsSignBDWordVideoBasedBangla2023">Sams, Akash, and Rahman (<a href="#ref-dataset:samsSignBDWordVideoBasedBangla2023" role="doc-biblioref">2023</a>)</span></td>
<td>Bangla</td>
<td><span title="video:RGB">🎥</span><span title="pose:OpenPose">👋</span></td>
<td>200</td>
<td>6000 videos</td>
<td>16</td>
<td></td>
</tr>
<tr class="even">
<td><a href="http://lojze.lugos.si/signor/">SIGNOR</a></td>
<td><span class="citation" data-cites="dataset:vintar2012compiling">Vintar, Jerko, and Kulovec (<a href="#ref-dataset:vintar2012compiling" role="doc-biblioref">2012</a>)</span></td>
<td>Slovene</td>
<td><span title="video">🎥</span><span title="mouthing">👄</span><span title="writing:HamNoSys">✍</span><span title="gloss:SZJ">📋</span><span title="text:Slovene">📜</span></td>
<td></td>
<td></td>
<td>80</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span> emailed Špela</td>
</tr>
<tr class="odd">
<td><a href="https://www.phonetik.uni-muenchen.de/forschung/Bas/SIGNUM/">SIGNUM</a></td>
<td><span class="citation" data-cites="dataset:von2007towards">Von Agris and Kraiss (<a href="#ref-dataset:von2007towards" role="doc-biblioref">2007</a>)</span></td>
<td>German</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>450</td>
<td>15,600 Sequences</td>
<td>20</td>
<td></td>
</tr>
<tr class="even">
<td>SMILE</td>
<td><span class="citation" data-cites="dataset:ebling2018smile">Ebling et al. (<a href="#ref-dataset:ebling2018smile" role="doc-biblioref">2018</a>)</span></td>
<td>Swiss-German</td>
<td><span style="background-color: red; color: white; padding: 0 2px !important;"><strong>TODO</strong></span></td>
<td>100</td>
<td>9,000 Samples</td>
<td>30</td>
<td><a href="https://zenodo.org/record/7701457">Custom</a></td>
</tr>
<tr class="odd">
<td><a href="https://teckensprakskorpus.su.se">SSL Corpus</a></td>
<td><span class="citation" data-cites="dataset:oqvist-etal-2020-sts">Öqvist, Riemer Kankkonen, and Mesch (<a href="#ref-dataset:oqvist-etal-2020-sts" role="doc-biblioref">2020</a>)</span></td>
<td>Swedish</td>
<td><span title="video">🎥</span><span title="writing:SWL">✍</span><span title="gloss:STS">📋</span><span title="text:Swedish">📜</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://teckensprakslexikon.su.se/">SSL Lexicon</a></td>
<td><span class="citation" data-cites="dataset:mesch2012meaning">Mesch and Wallin (<a href="#ref-dataset:mesch2012meaning" role="doc-biblioref">2012</a>)</span></td>
<td>Swedish</td>
<td><span title="video">🎥</span><span title="gloss:STS">📋</span><span title="text:Swedish">📜</span><span title="text:English">📜</span></td>
<td>20,000</td>
<td></td>
<td></td>
<td><a href="https://creativecommons.org/licenses/by-nc-sa/2.5/se/">CC BY-NC-SA 2.5 SE</a></td>
</tr>
<tr class="odd">
<td><a href="http://home.ustc.edu.cn/~hagjie/">Video-Based CSL</a></td>
<td><span class="citation" data-cites="dataset:huang2018video">Huang et al. (<a href="#ref-dataset:huang2018video" role="doc-biblioref">2018</a>)</span></td>
<td>Chinese</td>
<td><span title="video:RGBD">🎥</span><span title="gloss">📋</span><span title="pose:Kinect">👋</span></td>
<td>500</td>
<td>25,000 Videos</td>
<td>50</td>
<td><a href="https://rec.ustc.edu.cn/share/475ac440-dab7-11ea-963e-ebae3cfe5012">Research Attribution</a></td>
</tr>
<tr class="even">
<td><a href="https://dxli94.github.io/WLASL/">WLASL</a> <a href="https://github.com/sign-language-processing/datasets/tree/master/sign_language_datasets/datasets/wlasl">💾</a></td>
<td><span class="citation" data-cites="dataset:li2020word">Li et al. (<a href="#ref-dataset:li2020word" role="doc-biblioref">2020</a>)</span></td>
<td>American</td>
<td><span title="video">🎥</span><span title="gloss:ASL">📋</span></td>
<td>2,000</td>
<td></td>
<td>100</td>
<td><a href="https://github.com/microsoft/Computational-Use-of-Data-Agreement">C-UDA 1.0</a></td>
</tr>
</tbody>
</table>
</div>
<h2 id="other-resources">Other Resources</h2>
<ul>
<li>iReviews had compiled a list of <a href="https://www.ireviews.com/sign-language-resources">Top Resources for Learning (American) Sign Language</a></li>
</ul>
<h2 id="citation">Citation</h2>
<p>For attribution in academic contexts, please cite this work as:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bibtex"><code class="sourceCode bibtex"><span id="cb3-1"><a href="#cb3-1"></a><span class="va">@misc</span>{<span class="ot">moryossef2021slp</span>, </span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="dt">title</span> = &quot;<span class="st">{S}ign {L}anguage {P}rocessing</span>&quot;, </span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="dt">author</span> = &quot;<span class="st">Moryossef, Amit and Goldberg, Yoav</span>&quot;,</span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="dt">howpublished</span> = &quot;<span class="ch">\url</span><span class="st">{https://sign-language-processing.github.io/}</span>&quot;,</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="dt">year</span> = &quot;<span class="st">2021</span>&quot;</span>
<span id="cb3-6"><a href="#cb3-6"></a>}</span></code></pre></div>
<h2 id="references">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-abeille1991using">
<p>Abeille, Anne, Yves Schabes, and Aravind K. Joshi. 1990. “Using Lexicalized Tags for Machine Translation.” In <em>COLING 1990 Volume 3: Papers Presented to the 13th International Conference on Computational Linguistics</em>. <a href="https://aclanthology.org/C90-3001">https://aclanthology.org/C90-3001</a>.</p>
</div>
<div id="ref-adaloglou2020comprehensive">
<p>Adaloglou, Nikolas, Theocharis Chatzis, Ilias Papastratis, Andreas Stergioulas, Georgios Th Papadopoulos, Vassia Zacharopoulou, George J Xydopoulos, Klimnis Atzakas, Dimitris Papazachariou, and Petros Daras. 2020. “A Comprehensive Study on Sign Language Recognition Methods.” <em>ArXiv Preprint</em> abs/2007.12530. <a href="https://arxiv.org/abs/2007.12530">https://arxiv.org/abs/2007.12530</a>.</p>
</div>
<div id="ref-adeline2013fingerspell">
<p>Adeline, Chloe. 2013. “Fingerspell.net.” <a href="http://fingerspell.net/">http://fingerspell.net/</a>.</p>
</div>
<div id="ref-dataset:albanie2021bobsl">
<p>Albanie, Samuel, Gül Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, et al. 2021. “BOBSL: BBC-Oxford British Sign Language Dataset.” In. <a href="https://www.robots.ox.ac.uk/~vgg/data/bobsl">https://www.robots.ox.ac.uk/~vgg/data/bobsl</a>.</p>
</div>
<div id="ref-arivazhagan2019massively">
<p>Arivazhagan, Naveen, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, et al. 2019. “Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges.” <em>ArXiv Preprint</em> abs/1907.05019. <a href="https://arxiv.org/abs/1907.05019">https://arxiv.org/abs/1907.05019</a>.</p>
</div>
<div id="ref-shalev2022ham2pose">
<p>Arkushin, Rotem Shalev, Amit Moryossef, and Ohad Fried. 2023. “Ham2Pose: Animating Sign Language Notation into Pose Sequences,” 21046–56.</p>
</div>
<div id="ref-athitsos2010LargeLexiconIndexingRetrieval">
<p>Athitsos, Vassilis, Carol Neidle, Stan Sclaroff, Joan Nash, Alexandra Stefan, Ashwin Thangali, Haijing Wang, and Quan Yuan. 2010. “Large Lexicon Project: American Sign Language Video Corpus and Sign Language Indexing/Retrieval Algorithms.” In <em>7th International Conference on Language Resources and Evaluation (LREC 2010)</em>, edited by Philippe Dreuw, Eleni Efthimiou, Thomas Hanke, Trevor Johnston, Gregorio Martínez Ruiz, and Adam Schembri, 11–14. Valletta, Malta: European Language Resources Association (ELRA). <a href="https://www.sign-lang.uni-hamburg.de/lrec/pub/10022.pdf">https://www.sign-lang.uni-hamburg.de/lrec/pub/10022.pdf</a>.</p>
</div>
<div id="ref-dataset:athitsos2008american">
<p>Athitsos, Vassilis, Carol Neidle, Stan Sclaroff, Joan Nash, Alexandra Stefan, Quan Yuan, and Ashwin Thangali. 2008. “The American Sign Language Lexicon Video Dataset.” In <em>2008 Ieee Computer Society Conference on Computer Vision and Pattern Recognition Workshops</em>, 1–8. IEEE.</p>
</div>
<div id="ref-TFDS">
<p>authors, TensorFlow. 2019. “TensorFlow Datasets, a Collection of Ready-to-Use Datasets.” <em>GitHub Repository</em>. <a href="https://github.com/tensorflow/datasets">https://github.com/tensorflow/datasets</a>; GitHub.</p>
</div>
<div id="ref-bahdanau2014neural">
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In <em>3rd International Conference on Learning Representations, ICLR 2015, San Diego, ca, Usa, May 7-9, 2015, Conference Track Proceedings</em>, edited by Yoshua Bengio and Yann LeCun. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.</p>
</div>
<div id="ref-bangham2000virtual">
<p>Bangham, J Andrew, SJ Cox, Ralph Elliott, John RW Glauert, Ian Marshall, Sanja Rankov, and Mark Wells. 2000. “Virtual Signing: Capture, Animation, Storage and Transmission-an Overview of the Visicast Project.” In <em>IEE Seminar on Speech and Language Processing for Disabled and Elderly People (Ref. No. 2000/025)</em>, 6–1. IET.</p>
</div>
<div id="ref-paula:baowidan2021improving">
<p>Baowidan, Souad. 2021. “Improving Realism in Automated Fingerspelling of American Sign Language.” <em>Machine Translation</em> 35 (3): 387–404.</p>
</div>
<div id="ref-battison1978lexical">
<p>Battison, Robbin. 1978. “Lexical Borrowing in American Sign Language.”</p>
</div>
<div id="ref-bellugi1972comparison">
<p>Bellugi, Ursula, and Susan Fischer. 1972. “A Comparison of Sign Language and Spoken Language.” <em>Cognition</em> 1 (2-3): 173–200.</p>
</div>
<div id="ref-writing:bergman1977tecknad">
<p>Bergman, Brita. 1977. <em>Tecknad Svenska:[Signed Swedish]</em>. LiberLäromedel/Utbildningsförl.:</p>
</div>
<div id="ref-Berndt1994UsingDT">
<p>Berndt, Donald J., and James Clifford. 1994. “Using Dynamic Time Warping to Find Patterns in Time Series.” In <em>KDD Workshop</em>.</p>
</div>
<div id="ref-de2008pointing">
<p>Beuzeville, Louise de. 2008. “Pointing and Verb Modification: The Expression of Semantic Roles in the Auslan Corpus.” In <em>Workshop Programme</em>, 13. Citeseer.</p>
</div>
<div id="ref-bird-2020-decolonising">
<p>Bird, Steven. 2020. “Decolonising Speech and Language Technology.” In <em>Proceedings of the 28th International Conference on Computational Linguistics</em>, 3504–19. Barcelona, Spain (Online): International Committee on Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.coling-main.313">https://doi.org/10.18653/v1/2020.coling-main.313</a>.</p>
</div>
<div id="ref-bishop1994mixture">
<p>Bishop, Christopher M. 1994. “Mixture Density Networks.”</p>
</div>
<div id="ref-text-to-video:blattmann2023videoldm">
<p>Blattmann, Andreas, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. “Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.” In <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.</p>
</div>
<div id="ref-detection:borg2019sign">
<p>Borg, Mark, and Kenneth P. Camilleri. 2019. “Sign Language Detection "in the Wild" with Recurrent Neural Networks.” In <em>IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>, 1637–41. IEEE. <a href="https://doi.org/10.1109/ICASSP.2019.8683257">https://doi.org/10.1109/ICASSP.2019.8683257</a>.</p>
</div>
<div id="ref-bragg2019sign">
<p>Bragg, Danielle, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, et al. 2019. “Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective.” In <em>The 21st International Acm Sigaccess Conference on Computers and Accessibility</em>, 16–31.</p>
</div>
<div id="ref-brentari2011sign">
<p>Brentari, Diane. 2011. “Sign Language Phonology.” <em>The Handbook of Phonological Theory</em>, 691–721.</p>
</div>
<div id="ref-brentari2001language">
<p>Brentari, Diane, and Carol Padden. 2001. “A Language with Multiple Origins: Native and Foreign Vocabulary in American Sign Language.” <em>Foreign Vocabulary in Sign Language: A Cross-Linguistic Investigation of Word Formation</em>, 87–119.</p>
</div>
<div id="ref-segmentation:bull2021aligning">
<p>Bull, Hannah, Triantafyllos Afouras, Gül Varol, Samuel Albanie, Liliane Momeni, and Andrew Zisserman. 2021. “Aligning Subtitles in Sign Language Videos.” In <em>2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, Qc, Canada, October 10-17, 2021</em>, 11532–41. IEEE. <a href="https://doi.org/10.1109/ICCV48922.2021.01135">https://doi.org/10.1109/ICCV48922.2021.01135</a>.</p>
</div>
<div id="ref-segmentation:bull2020automatic">
<p>Bull, Hannah, Michèle Gouiffès, and Annelies Braffort. 2020. “Automatic Segmentation of Sign Language into Subtitle-Units.” In <em>European Conference on Computer Vision</em>, 186–98. Springer.</p>
</div>
<div id="ref-dataset:bungeroth2008atis">
<p>Bungeroth, Jan, Daniel Stein, Philippe Dreuw, Hermann Ney, Sara Morrissey, Andy Way, and Lynette van Zijl. 2008. “The ATIS Sign Language Corpus.” In <em>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08)</em>. Marrakech, Morocco: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/748_paper.pdf">http://www.lrec-conf.org/proceedings/lrec2008/pdf/748_paper.pdf</a>.</p>
</div>
<div id="ref-camgoz2017subunets">
<p>Camgöz, Necati Cihan, Simon Hadfield, Oscar Koller, and Richard Bowden. 2017. “SubUNets: End-to-End Hand Shape and Continuous Sign Language Recognition.” In <em>IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017</em>, 3075–84. IEEE Computer Society. <a href="https://doi.org/10.1109/ICCV.2017.332">https://doi.org/10.1109/ICCV.2017.332</a>.</p>
</div>
<div id="ref-cihan2018neural">
<p>Camgöz, Necati Cihan, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. 2018. “Neural Sign Language Translation.” In <em>2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, Ut, Usa, June 18-22, 2018</em>, 7784–93. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2018.00812">https://doi.org/10.1109/CVPR.2018.00812</a>.</p>
</div>
<div id="ref-dataset:camgoz-etal-2016-bosphorussign">
<p>Camgöz, Necati Cihan, Ahmet Alp Kındıroğlu, Serpil Karabüklü, Meltem Kelepir, Ayşe Sumru Özsoy, and Lale Akarun. 2016. “BosphorusSign: A Turkish Sign Language Recognition Corpus in Health and Finance Domains.” In <em>Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</em>, 1383–8. Portorož, Slovenia: European Language Resources Association (ELRA). <a href="https://aclanthology.org/L16-1220">https://aclanthology.org/L16-1220</a>.</p>
</div>
<div id="ref-camgoz2020multi">
<p>Camgöz, Necati Cihan, Oscar Koller, Simon Hadfield, and Richard Bowden. 2020a. “Multi-Channel Transformers for Multi-Articulatory Sign Language Translation.” In <em>European Conference on Computer Vision</em>, 301–19.</p>
</div>
<div id="ref-camgoz2020sign">
<p>———. 2020b. “Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation.” In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, Wa, Usa, June 13-19, 2020</em>, 10020–30. IEEE. <a href="https://doi.org/10.1109/CVPR42600.2020.01004">https://doi.org/10.1109/CVPR42600.2020.01004</a>.</p>
</div>
<div id="ref-dataset:camgoz2021content4all">
<p>Camgöz, Necati Cihan, Ben Saunders, Guillaume Rochette, Marco Giovanelli, Giacomo Inches, Robin Nachtrab-Ribback, and Richard Bowden. 2021. “Content4all Open Research Sign Language Translation Datasets.” In <em>2021 16th Ieee International Conference on Automatic Face and Gesture Recognition (Fg 2021)</em>, 1–5. IEEE.</p>
</div>
<div id="ref-pose:cao2017realtime">
<p>Cao, Zhe, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. “Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.” In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, Hi, Usa, July 21-26, 2017</em>, 1302–10. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.143">https://doi.org/10.1109/CVPR.2017.143</a>.</p>
</div>
<div id="ref-pose:cao2018openpose">
<p>Cao, Z., G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. 2019. “OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</div>
<div id="ref-carreira2017quo">
<p>Carreira, Joao, and Andrew Zisserman. 2017. “Quo Vadis, Action Recognition.” <em>ArXiv Preprint</em> abs/1705.07750. <a href="https://arxiv.org/abs/1705.07750">https://arxiv.org/abs/1705.07750</a>.</p>
</div>
<div id="ref-dataset:chai2014devisign">
<p>Chai, Xiujuan, Hanjie Wang, and Xilin Chen. 2014. “The Devisign Large Vocabulary of Chinese Sign Language Database and Baseline Evaluations.” <em>Technical Report VIPL-TR-14-SLR-001. Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS</em>.</p>
</div>
<div id="ref-pose:chan2019everybody">
<p>Chan, Caroline, Shiry Ginosar, Tinghui Zhou, and Alexei A. Efros. 2019. “Everybody Dance Now.” In <em>2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, 5932–41. IEEE. <a href="https://doi.org/10.1109/ICCV.2019.00603">https://doi.org/10.1109/ICCV.2019.00603</a>.</p>
</div>
<div id="ref-Chen2023SpeedIA">
<p>Chen, Yu-Hui, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann. 2023. “Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via Gpu-Aware Optimizations.” In.</p>
</div>
<div id="ref-pose:chen2017adversarial">
<p>Chen, Yu, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, and Jian Yang. 2017. “Adversarial Posenet: A Structure-Aware Convolutional Network for Human Pose Estimation.” In <em>IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017</em>, 1221–30. IEEE Computer Society. <a href="https://doi.org/10.1109/ICCV.2017.137">https://doi.org/10.1109/ICCV.2017.137</a>.</p>
</div>
<div id="ref-chenSimpleMultiModalityTransfer2022a">
<p>Chen, Yutong, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. 2022. “A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation.” In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, La, Usa, June 18-24, 2022</em>, 5110–20. IEEE. <a href="https://doi.org/10.1109/CVPR52688.2022.00506">https://doi.org/10.1109/CVPR52688.2022.00506</a>.</p>
</div>
<div id="ref-chen2022TwoStreamNetworkSign">
<p>Chen, Yutong, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie LIU, and Brian Mak. 2022. “Two-Stream Network for Sign Language Recognition and Translation.” In <em>Advances in Neural Information Processing Systems</em>, edited by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, 35:17043–56. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6cd3ac24cdb789beeaa9f7145670fcae-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/6cd3ac24cdb789beeaa9f7145670fcae-Paper-Conference.pdf</a>.</p>
</div>
<div id="ref-Cheng2023CiCoSignLanguageRetrieval">
<p>Cheng, Yiting, Fangyun Wei, Jianmin Bao, Dong Chen, and Wenqiang Zhang. 2023. “CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning.” In <em>2023 Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr)</em>. <a href="https://doi.org/10.1109/CVPR52729.2023.01823">https://doi.org/10.1109/CVPR52729.2023.01823</a>.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, Kyunghyun, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1724–34. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1179">https://doi.org/10.3115/v1/D14-1179</a>.</p>
</div>
<div id="ref-mmpose2020">
<p>Contributors, MMPose. 2020. “OpenMMLab Pose Estimation Toolbox and Benchmark.” <a href="https://github.com/open-mmlab/mmpose">https://github.com/open-mmlab/mmpose</a>.</p>
</div>
<div id="ref-cormier2015rethinking">
<p>Cormier, Kearsy, Sandra Smith, and Zed Sevcikova-Sehyr. 2015. “Rethinking Constructed Action.” <em>Sign Language &amp; Linguistics</em> 18 (2): 167–204.</p>
</div>
<div id="ref-costerQueryingSignLanguage2023">
<p>Coster, Mathieu De, and Joni Dambre. 2023. “Querying a Sign Language Dictionary with Videos Using Dense Vector Search.” In <em>2023 Ieee International Conference on Acoustics, Speech, and Signal Processing Workshops (Icasspw)</em>, 1–5. <a href="https://doi.org/10.1109/ICASSPW59220.2023.10193531">https://doi.org/10.1109/ICASSPW59220.2023.10193531</a>.</p>
</div>
<div id="ref-crasborn2016ngt">
<p>Crasborn, Onno, Richard Bank, Inge Zwitserlood, Els van der Kooij, Anique Schüller, Ellen Ormel, Ellen Nauta, Merel van Zuilen, Frouke van Winsum, and Johan Ros. 2016. “NGT Signbank.” <em>Nijmegen: Radboud University, Centre for Language Studies</em>.</p>
</div>
<div id="ref-dataset:Crasborn2008TheCN">
<p>Crasborn, Onno, and Inge Zwitserlood. 2008. “The Corpus NGT: An Online Corpus for Professionals and Laymen.” In.</p>
</div>
<div id="ref-cui2017recurrent">
<p>Cui, Runpeng, Hu Liu, and Changshui Zhang. 2017. “Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization.” In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, Hi, Usa, July 21-26, 2017</em>, 1610–8. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.175">https://doi.org/10.1109/CVPR.2017.175</a>.</p>
</div>
<div id="ref-cui2019deep">
<p>———. 2019. “A Deep Neural Framework for Continuous Sign Language Recognition by Iterative Training.” <em>IEEE Transactions on Multimedia</em> 21 (7): 1880–91.</p>
</div>
<div id="ref-dafnis2022bidirectional">
<p>Dafnis, Konstantinos M, Evgenia Chroni, Carol Neidle, and Dimitris N Metaxas. 2022. “Bidirectional Skeleton-Based Isolated Sign Recognition Using Graph Convolution Networks.” In <em>Proceedings of the 13th Conference on Language Resources and Evaluation (Lrec 2022), Marseille, 20-25 June 2022.</em></p>
</div>
<div id="ref-dataset:databases2007volumes">
<p>Databases, NCSLGR. 2007. “Volumes 2–7.” American Sign Language Linguistic Research Project (Distributed on CD-ROM ….</p>
</div>
<div id="ref-paula:davidson2006paula">
<p>Davidson, Mary Jo. 2006. “PAULA: A Computer-Based Sign Language Tutor for Hearing Adults.” In <em>Intelligent Tutoring Systems 2006 Workshop on Teaching with Robots, Agents, and Natural Language Processing</em>, 66–72. Citeseer.</p>
</div>
<div id="ref-de2022machine">
<p>De Coster, Mathieu, Dimitar Shterionov, Mieke Van Herreweghe, and Joni Dambre. 2022. “Machine Translation from Signed to Spoken Languages: State of the Art and Challenges.” <em>ArXiv Preprint</em> abs/2202.03086. <a href="https://arxiv.org/abs/2202.03086">https://arxiv.org/abs/2202.03086</a>.</p>
</div>
<div id="ref-deng2009imagenet">
<p>Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In <em>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA</em>, 248–55. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.</p>
</div>
<div id="ref-segmentation:de-sisto-etal-2021-defining">
<p>De Sisto, Mirella, Dimitar Shterionov, Irene Murtagh, Myriam Vermeerbergen, and Lorraine Leeson. 2021. “Defining Meaningful Units. Challenges in Sign Segmentation and Segment-Meaning Mapping (Short Paper).” In <em>Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (At4ssl)</em>, 98–103. Virtual: Association for Machine Translation in the Americas. <a href="https://aclanthology.org/2021.mtsummit-at4ssl.11">https://aclanthology.org/2021.mtsummit-at4ssl.11</a>.</p>
</div>
<div id="ref-devlin-etal-2019-bert">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div id="ref-dataset:dreuw2006modeling">
<p>Dreuw, Philippe, Thomas Deselaers, Daniel Keysers, and Hermann Ney. 2006. “Modeling Image Variability in Appearance-Based Gesture Recognition.” In <em>ECCV Workshop on Statistical Methods in Multi-Image and Video Processing</em>, 7–18.</p>
</div>
<div id="ref-dataset:dreuw2008benchmark">
<p>Dreuw, Philippe, Carol Neidle, Vassilis Athitsos, Stan Sclaroff, and Hermann Ney. 2008. “Benchmark Databases for Video-Based Automatic Sign Language Recognition.” In <em>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08)</em>. Marrakech, Morocco: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2008/pdf/287_paper.pdf">http://www.lrec-conf.org/proceedings/lrec2008/pdf/287_paper.pdf</a>.</p>
</div>
<div id="ref-Duarte2022SignVideoRetrivalWithTextQueries">
<p>Duarte, Amanda Cardoso, Samuel Albanie, Xavier Giró-i-Nieto, and Gül Varol. 2022. “Sign Language Video Retrieval with Free-Form Textual Queries.” In <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, La, Usa, June 18-24, 2022</em>, 14074–84. IEEE. <a href="https://doi.org/10.1109/CVPR52688.2022.01370">https://doi.org/10.1109/CVPR52688.2022.01370</a>.</p>
</div>
<div id="ref-dataset:duarte2020how2sign">
<p>Duarte, Amanda Cardoso, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Giró-i-Nieto. 2021. “How2Sign: A Large-Scale Multimodal Dataset for Continuous American Sign Language.” In <em>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual, June 19-25, 2021</em>, 2735–44. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPR46437.2021.00276">https://doi.org/10.1109/CVPR46437.2021.00276</a>.</p>
</div>
<div id="ref-dudis2004body">
<p>Dudis, Paul G. 2004. “Body Partitioning and Real-Space Blends.” <em>Cognitive Linguistics</em> 15 (2): 223–38.</p>
</div>
<div id="ref-dataset:ebling2018smile">
<p>Ebling, Sarah, Necati Cihan Camgöz, Penny Boyes Braem, Katja Tissi, Sandra Sidler-Miserez, Stephanie Stoll, Simon Hadfield, et al. 2018. “SMILE Swiss German Sign Language Dataset.” In <em>Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>. Miyazaki, Japan: European Language Resources Association (ELRA). <a href="https://aclanthology.org/L18-1666">https://aclanthology.org/L18-1666</a>.</p>
</div>
<div id="ref-dataset:efthimiou2012sign">
<p>Efthimiou, Eleni, Stavroula-Evita Fotinea, Thomas Hanke, John Glauert, Richard Bowden, Annelies Braffort, Christophe Collet, Petros Maragos, and François Lefebvre-Albaret. 2012. “Sign Language Technologies and Resources of the Dicta-Sign Project.” In <em>5th Workshop on the Representation and Processing of Sign Languages: Interactions Between Corpus and Lexicon. Satellite Workshop to the Eighth International Conference on Language Resources and Evaluation (Lrec-2012)</em>.</p>
</div>
<div id="ref-egea-gomez-etal-2021-syntax">
<p>Egea Gómez, Santiago, Euan McGill, and Horacio Saggion. 2021. “Syntax-Aware Transformers for Neural Machine Translation: The Case of Text to Sign Gloss Translation.” In <em>Proceedings of the 14th Workshop on Building and Using Comparable Corpora (Bucc 2021)</em>, 18–27. Online (Virtual Mode): INCOMA Ltd. <a href="https://aclanthology.org/2021.bucc-1.4">https://aclanthology.org/2021.bucc-1.4</a>.</p>
</div>
<div id="ref-elliott2004overview">
<p>Elliott, Ralph, John Glauert, Vince Jennings, and Richard Kennaway. 2004. “An Overview of the Sigml Notation and Sigmlsigning Software System.” <em>Sign-Lang LREC 2004</em>, 98–104.</p>
</div>
<div id="ref-elliott2000development">
<p>Elliott, Ralph, John RW Glauert, JR Kennaway, and Ian Marshall. 2000. “The Development of Language Processing Support for the Visicast Project.” In <em>Proceedings of the Fourth International Acm Conference on Assistive Technologies</em>, 101–8.</p>
</div>
<div id="ref-erard2017sign">
<p>Erard, Michael. 2017. “Why Sign-Language Gloves Don’t Help Deaf People.” <em>The Atlantic</em> 9.</p>
</div>
<div id="ref-segmentation:farag2019learning">
<p>Farag, Iva, and Heike Brock. 2019. “Learning Motion Disfluencies for Automatic Sign Language Segmentation.” In <em>IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>, 7360–4. IEEE. <a href="https://doi.org/10.1109/ICASSP.2019.8683523">https://doi.org/10.1109/ICASSP.2019.8683523</a>.</p>
</div>
<div id="ref-fenlon2015building">
<p>Fenlon, Jordan, Kearsy Cormier, and Adam Schembri. 2015. “Building BSL Signbank: The Lemma Dilemma Revisited.” <em>International Journal of Lexicography</em> 28 (2): 169–206.</p>
</div>
<div id="ref-fenlon2018modification">
<p>Fenlon, Jordan, Adam Schembri, and Kearsy Cormier. 2018. “Modification of Indicating Verbs in British Sign Language: A Corpus-Based Study.” <em>Language</em> 94 (1): 84–118.</p>
</div>
<div id="ref-ferreira1984similarities">
<p>Ferreira-Brito, Lucinda. 1984. “Similarities &amp; Differences in Two Brazilian Sign Languages.” <em>Sign Language Studies</em> 42: 45–56.</p>
</div>
<div id="ref-paula:filhol2022representation">
<p>Filhol, Michael, and John McDonald. 2022. “Representation and Synthesis of Geometric Relocations.” In <em>Proceedings of the Lrec2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</em>, 53–58. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.signlang-1.9">https://aclanthology.org/2022.signlang-1.9</a>.</p>
</div>
<div id="ref-paula:filhol2020synthesis">
<p>Filhol, Michael, and John C. McDonald. 2020. “The Synthesis of Complex Shape Deployments in Sign Language.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 61–68. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.10">https://aclanthology.org/2020.signlang-1.10</a>.</p>
</div>
<div id="ref-paula:filhol2017synthesizing">
<p>Filhol, Michael, John C McDonald, and Rosalee J Wolfe. 2017. “Synthesizing Sign Language by Connecting Linguistically Structured Descriptions to a Multi-Track Animation System.” In <em>Universal Access in Human–Computer Interaction. Designing Novel Interactions: 11th International Conference, Uahci 2017, Held as Part of Hci International 2017, Vancouver, Bc, Canada, July 9–14, 2017, Proceedings, Part Ii 11</em>, 27–40. Springer.</p>
</div>
<div id="ref-dataset:forster2014extensions">
<p>Forster, Jens, Christoph Schmidt, Oscar Koller, Martin Bellgardt, and Hermann Ney. 2014. “Extensions of the Sign Language Recognition and Translation Corpus RWTH-PHOENIX-Weather.” In <em>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</em>, 1911–6. Reykjavik, Iceland: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/585_Paper.pdf">http://www.lrec-conf.org/proceedings/lrec2014/pdf/585_Paper.pdf</a>.</p>
</div>
<div id="ref-identification:gebre2013automatic">
<p>Gebre, Binyam Gebrekidan, Peter Wittenburg, and Tom Heskes. 2013. “Automatic Sign Language Identification.” In <em>2013 Ieee International Conference on Image Processing</em>, 2626–30. IEEE.</p>
</div>
<div id="ref-pose:girocan2020slrtp">
<p>Giró-i-Nieto, Xavier. 2020. “Can Everybody Sign Now? Exploring Sign Language Video Generation from 2D Poses.” <em>SLRTP 2020: The Sign Language Recognition, Translation &amp; Production Workshop</em>.</p>
</div>
<div id="ref-glickman2018language">
<p>Glickman, Neil S, and Wyatte C Hall. 2018. <em>Language Deprivation and Deaf Mental Health</em>. Routledge.</p>
</div>
<div id="ref-gongLLMsAreGood2024">
<p>Gong, Jia, Lin Geng Foo, Yixuan He, Hossein Rahmani, and Jun Liu. 2024. “LLMs Are Good Sign Language Translators.” <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/2404.00925">https://arxiv.org/abs/2404.00925</a>.</p>
</div>
<div id="ref-graves2006connectionist">
<p>Graves, Alex, Santiago Fernández, Faustino J. Gomez, and Jürgen Schmidhuber. 2006. “Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.” In <em>Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, Usa, June 25-29, 2006</em>, edited by William W. Cohen and Andrew W. Moore, 148:369–76. ACM International Conference Proceeding Series. ACM. <a href="https://doi.org/10.1145/1143844.1143891">https://doi.org/10.1145/1143844.1143891</a>.</p>
</div>
<div id="ref-mediapipe2020holistic">
<p>Grishchenko, Ivan, and Valentin Bazarevsky. 2020. “MediaPipe Holistic.” <a href="https://google.github.io/mediapipe/solutions/holistic.html">https://google.github.io/mediapipe/solutions/holistic.html</a>.</p>
</div>
<div id="ref-dataset:gutierrez2016lse">
<p>Gutierrez-Sigut, Eva, Brendan Costello, Cristina Baus, and Manuel Carreiras. 2016. “LSE-Sign: A Lexical Database for Spanish Sign Language.” <em>Behavior Research Methods</em> 48 (1): 123–37.</p>
</div>
<div id="ref-pose:alp2018densepose">
<p>Güler, Riza Alp, Natalia Neverova, and Iasonas Kokkinos. 2018. “DensePose: Dense Human Pose Estimation in the Wild.” In <em>2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, Ut, Usa, June 18-22, 2018</em>, 7297–7306. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2018.00762">https://doi.org/10.1109/CVPR.2018.00762</a>.</p>
</div>
<div id="ref-hall2017language">
<p>Hall, Wyatte C, Leonard L Levin, and Melissa L Anderson. 2017. “Language Deprivation Syndrome: A Possible Neurodevelopmental Disorder with Sociocultural Origins.” <em>Social Psychiatry and Psychiatric Epidemiology</em> 52 (6): 761–76.</p>
</div>
<div id="ref-hanke2002ilex">
<p>Hanke, Thomas. 2002. “ILex - a Tool for Sign Language Lexicography and Corpus Analysis.” In <em>Proceedings of the Third International Conference on Language Resources and Evaluation (LREC’02)</em>. Las Palmas, Canary Islands - Spain: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2002/pdf/330.pdf">http://www.lrec-conf.org/proceedings/lrec2002/pdf/330.pdf</a>.</p>
</div>
<div id="ref-dataset:hanke-etal-2020-extending">
<p>Hanke, Thomas, Marc Schulder, Reiner Konrad, and Elena Jahn. 2020. “Extending the Public DGS Corpus in Size and Depth.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 75–82. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.12">https://aclanthology.org/2020.signlang-1.12</a>.</p>
</div>
<div id="ref-harris2009research">
<p>Harris, Raychelle, Heidi M Holmes, and Donna M Mertens. 2009. “Research Ethics in Sign Language Communities.” <em>Sign Language Studies</em> 9 (2): 104–31.</p>
</div>
<div id="ref-dataset:hassan-etal-2020-isolated">
<p>Hassan, Saad, Larwan Berke, Elahe Vahdani, Longlong Jing, Yingli Tian, and Matt Huenerfauth. 2020. “An Isolated-Signing RGBD Dataset of 100 American Sign Language Signs Produced by Fluent ASL Signers.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 89–94. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.14">https://aclanthology.org/2020.signlang-1.14</a>.</p>
</div>
<div id="ref-dataset:hassan-etal-2022-asl-homework">
<p>Hassan, Saad, Matthew Seita, Larwan Berke, Yingli Tian, Elaine Gale, Sooyeon Lee, and Matt Huenerfauth. 2022. “ASL-Homework-RGBD Dataset: An Annotated Dataset of 45 Fluent and Non-Fluent Signers Performing American Sign Language Homeworks.” In <em>Proceedings of the Lrec2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</em>, 67–72. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.signlang-1.11">https://aclanthology.org/2022.signlang-1.11</a>.</p>
</div>
<div id="ref-text-to-video:Ho2022ImagenVH">
<p>Ho, Jonathan, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, et al. 2022. “Imagen Video: High Definition Video Generation with Diffusion Models.” <em>ArXiv Preprint</em> abs/2210.02303. <a href="https://arxiv.org/abs/2210.02303">https://arxiv.org/abs/2210.02303</a>.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80.</p>
</div>
<div id="ref-hu2023SignBertPlus">
<p>Hu, Hezhen, Weichao Zhao, Wengang Zhou, and Houqiang Li. 2023. “SignBERT+: Hand-Model-Aware Self-Supervised Pre-Training for Sign Language Understanding.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 45 (9): 11221–39. <a href="https://doi.org/10.1109/TPAMI.2023.3269220">https://doi.org/10.1109/TPAMI.2023.3269220</a>.</p>
</div>
<div id="ref-hu2021SignBert">
<p>Hu, Hezhen, Weichao Zhao, Wengang Zhou, Yuechen Wang, and Houqiang Li. 2021. “SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition.” In <em>2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, Qc, Canada, October 10-17, 2021</em>, 11067–76. IEEE. <a href="https://doi.org/10.1109/ICCV48922.2021.01090">https://doi.org/10.1109/ICCV48922.2021.01090</a>.</p>
</div>
<div id="ref-hu2021NMFAwareSLR">
<p>Hu, Hezhen, Wengang Zhou, Junfu Pu, and Houqiang Li. 2021. “Global-Local Enhancement Network for NMF-Aware Sign Language Recognition.” <em>ACM Trans. Multimedia Comput. Commun. Appl.</em> 17 (3). <a href="https://doi.org/10.1145/3436754">https://doi.org/10.1145/3436754</a>.</p>
</div>
<div id="ref-huang2019attention3DCNNsSLR">
<p>Huang, Jie, Wengang Zhou, Houqiang Li, and Weiping Li. 2019. “Attention-Based 3D-CNNs for Large-Vocabulary Sign Language Recognition.” <em>IEEE Transactions on Circuits and Systems for Video Technology</em> 29 (9): 2822–32. <a href="https://doi.org/10.1109/TCSVT.2018.2870740">https://doi.org/10.1109/TCSVT.2018.2870740</a>.</p>
</div>
<div id="ref-dataset:huang2018video">
<p>Huang, Jie, Wengang Zhou, Qilin Zhang, Houqiang Li, and Weiping Li. 2018. “Video-Based Sign Language Recognition Without Temporal Segmentation.” In <em>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (Aaai-18), the 30th Innovative Applications of Artificial Intelligence (Iaai-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (Eaai-18), New Orleans, Louisiana, Usa, February 2-7, 2018</em>, edited by Sheila A. McIlraith and Kilian Q. Weinberger, 2257–64. AAAI Press. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17137">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17137</a>.</p>
</div>
<div id="ref-huang2021towards">
<p>Huang, Wencan, Wenwen Pan, Zhou Zhao, and Qi Tian. 2021. “Towards Fast and High-Quality Sign Language Production.” In <em>Proceedings of the 29th Acm International Conference on Multimedia</em>, 3172–81.</p>
</div>
<div id="ref-humphries2016avoiding">
<p>Humphries, Tom, Poorna Kushalnagar, Gaurav Mathur, Donna Jo Napoli, Carol Padden, Christian Rathmann, and Scott Smith. 2016. “Avoiding Linguistic Neglect of Deaf Children.” <em>Social Service Review</em> 90 (4): 589–619.</p>
</div>
<div id="ref-dataset:imashev2020dataset">
<p>Imashev, Alfarabi, Medet Mukushev, Vadim Kimmelman, and Anara Sandygulova. 2020. “A Dataset for Linguistic Understanding, Visual Evaluation, and Recognition of Sign Languages: The K-RSL.” In <em>Proceedings of the 24th Conference on Computational Natural Language Learning</em>, 631–40. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.conll-1.51">https://doi.org/10.18653/v1/2020.conll-1.51</a>.</p>
</div>
<div id="ref-isard2020approaches">
<p>Isard, Amy. 2020. “Approaches to the Anonymisation of Sign Language Corpora.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 95–100. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.15">https://aclanthology.org/2020.signlang-1.15</a>.</p>
</div>
<div id="ref-isola2017image">
<p>Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. “Image-to-Image Translation with Conditional Adversarial Networks.” In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, Hi, Usa, July 21-26, 2017</em>, 5967–76. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.632">https://doi.org/10.1109/CVPR.2017.632</a>.</p>
</div>
<div id="ref-jiang2021sign">
<p>Jiang, Songyao, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu. 2021. “Sign Language Recognition via Skeleton-Aware Multi-Model Ensemble.” <em>ArXiv Preprint</em> abs/2110.06161. <a href="https://arxiv.org/abs/2110.06161">https://arxiv.org/abs/2110.06161</a>.</p>
</div>
<div id="ref-jiang2022machine">
<p>Jiang, Zifan, Amit Moryossef, Mathias Müller, and Sarah Ebling. 2023. “Machine Translation Between Spoken Languages and Signed Languages Represented in SignWriting.” In <em>Findings of the Association for Computational Linguistics: EACL 2023</em>, 1706–24. Dubrovnik, Croatia: Association for Computational Linguistics. <a href="https://aclanthology.org/2023.findings-eacl.127">https://aclanthology.org/2023.findings-eacl.127</a>.</p>
</div>
<div id="ref-johnson2011toward">
<p>Johnson, Robert E, and Scott K Liddell. 2011. “Toward a Phonetic Representation of Signs: Sequentiality and Contrast.” <em>Sign Language Studies</em> 11 (2): 241–74.</p>
</div>
<div id="ref-paula:johnson2018improved">
<p>Johnson, Ronan, Maren Brumm, and Rosalee J Wolfe. 2018. “An Improved Avatar for Automatic Mouth Gesture Recognition.” In <em>Language Resources and Evaluation Conference</em>, 107–8.</p>
</div>
<div id="ref-dataset:johnston2010archive">
<p>Johnston, Trevor. 2008. “From Archive to Corpus: Transcription and Annotation in the Creation of Signed Language Corpora.” In <em>Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation</em>, 16–29. The University of the Philippines Visayas Cebu College, Cebu City, Philippines: De La Salle University, Manila, Philippines. <a href="https://aclanthology.org/Y08-1002">https://aclanthology.org/Y08-1002</a>.</p>
</div>
<div id="ref-johnston2016auslan">
<p>Johnston, Trevor, and Louise De Beuzeville. 2016. “Auslan Corpus Annotation Guidelines.” <em>Auslan Corpus</em>.</p>
</div>
<div id="ref-johnston2007australian">
<p>Johnston, Trevor, and Adam Schembri. 2007. <em>Australian Sign Language (Auslan): An Introduction to Sign Language Linguistics</em>. Cambridge University Press.</p>
</div>
<div id="ref-dataset:joshi-etal-2023-isltranslate">
<p>Joshi, Abhinav, Susmit Agrawal, and Ashutosh Modi. 2023. “ISLTranslate: Dataset for Translating Indian Sign Language.” In <em>Findings of the Association for Computational Linguistics: ACL 2023</em>, 10466–75. Toronto, Canada: Association for Computational Linguistics. <a href="https://aclanthology.org/2023.findings-acl.665">https://aclanthology.org/2023.findings-acl.665</a>.</p>
</div>
<div id="ref-dataset:joze2018ms">
<p>Joze, Hamid Reza Vaezi, and Oscar Koller. 2019. “MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language.” In <em>30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, Uk, September 9-12, 2019</em>, 100. BMVA Press. <a href="https://bmvc2019.org/wp-content/uploads/papers/0254-paper.pdf">https://bmvc2019.org/wp-content/uploads/papers/0254-paper.pdf</a>.</p>
</div>
<div id="ref-jui-etal-2022-machine">
<p>Jui, Tonni Das, Gissella Bejarano, and Pablo Rivas. 2022. “A Machine Learning-Based Segmentation Approach for Measuring Similarity Between Sign Languages.” In <em>Proceedings of the Lrec2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</em>, 94–101. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.signlang-1.15">https://aclanthology.org/2022.signlang-1.15</a>.</p>
</div>
<div id="ref-writing:kakumasu1968urubu">
<p>Kakumasu, Jim. 1968. “Urubu Sign Language.” <em>International Journal of American Linguistics</em> 34 (4): 275–81.</p>
</div>
<div id="ref-style-to-image:Karras2021">
<p>Karras, Tero, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. “Alias-Free Generative Adversarial Networks.” In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, Neurips 2021, December 6-14, 2021, Virtual</em>, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 852–63. <a href="https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html</a>.</p>
</div>
<div id="ref-style-to-image:Karras2018ASG">
<p>Karras, Tero, Samuli Laine, and Timo Aila. 2019. “A Style-Based Generator Architecture for Generative Adversarial Networks.” In <em>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, ca, Usa, June 16-20, 2019</em>, 4401–10. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPR.2019.00453">https://doi.org/10.1109/CVPR.2019.00453</a>.</p>
</div>
<div id="ref-style-to-image:Karras2019stylegan2">
<p>Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. “Analyzing and Improving the Image Quality of Stylegan.” In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, Wa, Usa, June 13-19, 2020</em>, 8107–16. IEEE. <a href="https://doi.org/10.1109/CVPR42600.2020.00813">https://doi.org/10.1109/CVPR42600.2020.00813</a>.</p>
</div>
<div id="ref-kezar2023improving">
<p>Kezar, Lee, Jesse Thomason, and Zed Sehyr. 2023. “Improving Sign Recognition with Phonology.” In <em>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2732–7. Dubrovnik, Croatia: Association for Computational Linguistics. <a href="https://aclanthology.org/2023.eacl-main.200">https://aclanthology.org/2023.eacl-main.200</a>.</p>
</div>
<div id="ref-kimmelman2014information">
<p>Kimmelman, Vadim. 2014. “Information Structure in Russian Sign Language and Sign Language of the Netherlands.” <em>Sign Language &amp; Linguistics</em> 18 (1): 142–50.</p>
</div>
<div id="ref-text-to-image:Kingma2021VariationalDM">
<p>Kingma, Diederik P., Tim Salimans, Ben Poole, and Jonathan Ho. 2021. “Variational Diffusion Models.” <em>ArXiv Preprint</em> abs/2107.00630. <a href="https://arxiv.org/abs/2107.00630">https://arxiv.org/abs/2107.00630</a>.</p>
</div>
<div id="ref-kipp2001anvil">
<p>Kipp, Michael. 2001. “Anvil-a Generic Annotation Tool for Multimodal Dialogue.” In <em>Seventh European Conference on Speech Communication and Technology</em>.</p>
</div>
<div id="ref-dataset:ko2019neural">
<p>Ko, Sang-Ki, Chang Jo Kim, Hyedong Jung, and Choongsang Cho. 2019. “Neural Sign Language Translation Based on Human Keypoint Estimation.” <em>Applied Sciences</em> 9 (13): 2683.</p>
</div>
<div id="ref-koller2019weakly">
<p>Koller, Oscar, Cihan Camgöz, Hermann Ney, and Richard Bowden. 2019. “Weakly Supervised Learning with Multi-Stream Cnn-Lstm-Hmms to Discover Sequential Parallelism in Sign Language Videos.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.</p>
</div>
<div id="ref-koller2015ContinuousSLR">
<p>Koller, Oscar, Jens Forster, and Hermann Ney. 2015. “Continuous Sign Language Recognition: Towards Large Vocabulary Statistical Recognition Systems Handling Multiple Signers.” <em>Computer Vision and Image Understanding</em> 141: 108–25. <a href="https://doi.org/https://doi.org/10.1016/j.cviu.2015.09.013">https://doi.org/https://doi.org/10.1016/j.cviu.2015.09.013</a>.</p>
</div>
<div id="ref-konrad2018public">
<p>Konrad, Reiner, Thomas Hanke, Gabriele Langer, Susanne König, Lutz König, Rie Nishio, and Anja Regen. 2018. “Public DGS Corpus: Annotation Conventions.” Project Note AP03-2018-01, DGS-Korpus project, IDGS, Hamburg University.</p>
</div>
<div id="ref-krizhevsky2012imagenet">
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1106–14. <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>.</p>
</div>
<div id="ref-kusters2017innovations">
<p>Kusters, Annelies, Maartje De Meulder, and Dai O’Brien. 2017. <em>Innovations in Deaf Studies: The Role of Deaf Scholars</em>. Oxford University Press.</p>
</div>
<div id="ref-kusters-et-al-2017">
<p>Kusters, Annelies Maria Jozef, Dai O’Brien, and Maartje De Meulder. 2017. “Innovations in Deaf Studies: Critically Mapping the Field.” In <em>Innovations in Deaf Studies</em>, edited by Annelies Kusters, Maartje De Meulder, and Dai O’Brien, 1–53. United Kingdom: Oxford University Press.</p>
</div>
<div id="ref-lebert2008project">
<p>Lebert, Marie. 2008. “Project Gutenberg (1971-2008).” Project Gutenberg.</p>
</div>
<div id="ref-dataset:li2020word">
<p>Li, Dongxu, Cristian Rodriguez, Xin Yu, and Hongdong Li. 2020. “Word-Level Deep Sign Language Recognition from Video: A New Large-Scale Dataset and Methods Comparison.” In <em>The Ieee Winter Conference on Applications of Computer Vision</em>, 1459–69.</p>
</div>
<div id="ref-liddell1989american">
<p>Liddell, Scott K, and Robert E Johnson. 1989. “American Sign Language: The Phonological Base.” <em>Sign Language Studies</em> 64 (1): 195–277.</p>
</div>
<div id="ref-liddell1998gesture">
<p>Liddell, Scott K, and Melanie Metzger. 1998. “Gesture in Sign Language Discourse.” <em>Journal of Pragmatics</em> 30 (6): 657–97.</p>
</div>
<div id="ref-liddell2003grammar">
<p>Liddell, Scott K, and others. 2003. <em>Grammar, Gesture, and Meaning in American Sign Language</em>. Cambridge University Press.</p>
</div>
<div id="ref-liu-etal-2020-multilingual-denoising">
<p>Liu, Yinhan, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. “Multilingual Denoising Pre-Training for Neural Machine Translation.” <em>Transactions of the Association for Computational Linguistics</em> 8: 726–42. <a href="https://doi.org/10.1162/tacl_a_00343">https://doi.org/10.1162/tacl_a_00343</a>.</p>
</div>
<div id="ref-pympi-1.69">
<p>Lubbers, Mart, and Francisco Torreira. 2013. “Pympi-Ling: A Python Module for Processing ELANs EAF and Praats TextGrid Annotation Files.” <a href="https://pypi.python.org/pypi/pympi-ling">https://pypi.python.org/pypi/pympi-ling</a>.</p>
</div>
<div id="ref-luong2015effective">
<p>Luong, Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” In <em>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, 1412–21. Lisbon, Portugal: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D15-1166">https://doi.org/10.18653/v1/D15-1166</a>.</p>
</div>
<div id="ref-pose:hidalgo2019singlenetwork">
<p>Martinez, Gines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul Joo, Tomas Simon, and Yaser Sheikh. 2019. “Single-Network Whole-Body Pose Estimation.” In <em>2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, 6981–90. IEEE. <a href="https://doi.org/10.1109/ICCV.2019.00708">https://doi.org/10.1109/ICCV.2019.00708</a>.</p>
</div>
<div id="ref-dataset:martinez2002purdue">
<p>Martínez, Aleix M, Ronnie B Wilbur, Robin Shay, and Avinash C Kak. 2002. “Purdue Rvl-Slll Asl Database for Automatic Recognition of American Sign Language.” In <em>Proceedings. Fourth Ieee International Conference on Multimodal Interfaces</em>, 167–72. IEEE.</p>
</div>
<div id="ref-dataset:matthes2012dicta">
<p>Matthes, Silke, Thomas Hanke, Anja Regen, Jakob Storz, Satu Worseck, Eleni Efthimiou, Athanasia-Lida Dimou, Annelies Braffort, John Glauert, and Eva Safar. 2012. “Dicta-Sign–Building a Multilingual Sign Language Corpus.” In <em>Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions Between Corpus and Lexicon (Lrec 2012)</em>.</p>
</div>
<div id="ref-paula:mcdonald2021natural">
<p>McDonald, John C, and Michael Filhol. 2021. “Natural Synthesis of Productive Forms from Structured Descriptions of Sign Language.” <em>Machine Translation</em> 35 (3): 363–86.</p>
</div>
<div id="ref-paula:mcdonald2017improved">
<p>McDonald, John C, Rosalee J Wolfe, Sarah Johnson, Souad Baowidan, Robyn Moncrief, and Ningshan Guo. 2017. “An Improved Framework for Layering Linguistic Processes in Sign Language Generation: Why There Should Never Be a ‘Brows’ Tier.” In <em>Universal Access in Human–Computer Interaction. Designing Novel Interactions: 11th International Conference, Uahci 2017, Held as Part of Hci International 2017, Vancouver, Bc, Canada, July 9–14, 2017, Proceedings, Part Ii 11</em>, 41–54. Springer.</p>
</div>
<div id="ref-paula:mcdonald2016automated">
<p>McDonald, John C, Rosalee J Wolfe, Jerry C Schnepp, Julie Hochgesang, Diana Gorman Jamrozik, Marie Stumbo, Larwan Berke, Melissa Bialek, and Farah Thomas. 2016. “An Automated Technique for Real-Time Production of Lifelike Animations of American Sign Language.” <em>Universal Access in the Information Society</em> 15: 551–66.</p>
</div>
<div id="ref-paula:mcdonald2022novel">
<p>McDonald, John, Ronan Johnson, and Rosalee Wolfe. 2022. “A Novel Approach to Managing Lower Face Complexity in Signing Avatars.” In <em>Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives</em>, 67–72. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.sltat-1.10">https://aclanthology.org/2022.sltat-1.10</a>.</p>
</div>
<div id="ref-mckee2000lexical">
<p>McKee, David, and Graeme Kennedy. 2000. “Lexical Comparison of Signs from American, Australian, British and New Zealand Sign Languages.” <em>The Signs of Language Revisited: An Anthology to Honor Ursula Bellugi and Edward Klima</em>, 49–76.</p>
</div>
<div id="ref-dataset:mesch2012meaning">
<p>Mesch, Johanna, and Lars Wallin. 2012. “From Meaning to Signs and Back: Lexicography and the Swedish Sign Language Corpus.” In <em>Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions Between Corpus and Lexicon [Language Resources and Evaluation Conference (Lrec)]</em>, 123–26.</p>
</div>
<div id="ref-mesch2015gloss">
<p>———. 2015. “Gloss Annotations in the Swedish Sign Language Corpus.” <em>International Journal of Corpus Linguistics</em> 20 (1): 102–20.</p>
</div>
<div id="ref-min2012motion">
<p>Min, Jianyuan, and Jinxiang Chai. 2012. “Motion Graphs++ a Compact Generative Model for Semantic Motion Analysis and Synthesis.” <em>ACM Transactions on Graphics (TOG)</em> 31 (6): 1–12.</p>
</div>
<div id="ref-momeni2022automatic">
<p>Momeni, Liliane, Hannah Bull, KR Prajwal, Samuel Albanie, Gül Varol, and Andrew Zisserman. 2022a. “Automatic Dense Annotation of Large-Vocabulary Sign Language Videos.” In <em>Computer Vision–Eccv 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part Xxxv</em>, 671–90. Springer.</p>
</div>
<div id="ref-dataset:momeniAutomaticDenseAnnotation2022">
<p>Momeni, Liliane, Hannah Bull, K. R. Prajwal, Samuel Albanie, Gül Varol, and Andrew Zisserman. 2022b. “Automatic Dense Annotation of Large-Vocabulary Sign Language Videos.” In <em>Computer Vision – ECCV 2022</em>, edited by Shai Avidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, 671–90. Cham: Springer Nature Switzerland. <a href="https://doi.org/10.1007/978-3-031-19833-5_39">https://doi.org/10.1007/978-3-031-19833-5_39</a>.</p>
</div>
<div id="ref-paula:moncrief2020extending">
<p>Moncrief, Robyn. 2020. “Extending a Model for Animating Adverbs of Manner in American Sign Language.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 151–56. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.25">https://aclanthology.org/2020.signlang-1.25</a>.</p>
</div>
<div id="ref-paula:moncrief2021generalizing">
<p>———. 2021. “Generalizing a Model for Animating Adverbs of Manner in American Sign Language.” <em>Machine Translation</em> 35 (3): 345–62.</p>
</div>
<div id="ref-identification:monteiro2016detecting">
<p>Monteiro, Caio DD, Christy Maria Mathew, Ricardo Gutierrez-Osuna, and Frank Shipman. 2016. “Detecting and Identifying Sign Languages Through Visual Features.” In <em>2016 Ieee International Symposium on Multimedia (Ism)</em>, 287–90. IEEE.</p>
</div>
<div id="ref-montemurro2018emphatic">
<p>Montemurro, Kathryn, and Diane Brentari. 2018. “Emphatic Fingerspelling as Code-Mixing in American Sign Language.” <em>Proceedings of the Linguistic Society of America</em> 3 (1): 61–61.</p>
</div>
<div id="ref-segmentation:moryossef-etal-2023-linguistically">
<p>Moryossef, Amit, Zifan Jiang, Mathias Müller, Sarah Ebling, and Yoav Goldberg. 2023. “Linguistically Motivated Sign Language Segmentation.” In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em>, edited by Houda Bouamor, Juan Pino, and Kalika Bali, 12703–24. Singapore: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.findings-emnlp.846">https://doi.org/10.18653/v1/2023.findings-emnlp.846</a>.</p>
</div>
<div id="ref-moryossef2021datasets">
<p>Moryossef, Amit, and Mathias Müller. 2021. “Sign Language Datasets.” <a href="https://github.com/sign-language-processing/datasets">https://github.com/sign-language-processing/datasets</a>.</p>
</div>
<div id="ref-moryossef2023baseline">
<p>Moryossef, Amit, Mathias Müller, Anne Göhring, Zifan Jiang, Yoav Goldberg, and Sarah Ebling. 2023. “An Open-Source Gloss-Based Baseline for Spoken to Signed Language Translation.” In <em>ArXiv Preprint</em>. Vol. abs/2305.17714. <a href="https://arxiv.org/abs/2305.17714">https://arxiv.org/abs/2305.17714</a>.</p>
</div>
<div id="ref-detection:moryossef2020real">
<p>Moryossef, Amit, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, and Srini Narayanan. 2020. “Real-Time Sign-Language Detection Using Human Pose Estimation.” In <em>Computer Vision–Eccv 2020 Workshops: Glasgow, Uk, August 23–28, 2020, Proceedings, Part Ii 16, Slrtp 2020: The Sign Language Recognition, Translation and Production Workshop</em>, 237–48.</p>
</div>
<div id="ref-moryossef-etal-2021-data">
<p>Moryossef, Amit, Kayo Yin, Graham Neubig, and Yoav Goldberg. 2021. “Data Augmentation for Sign Language Gloss Translation.” In <em>Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (At4ssl)</em>, 1–11. Virtual: Association for Machine Translation in the Americas. <a href="https://aclanthology.org/2021.mtsummit-at4ssl.1">https://aclanthology.org/2021.mtsummit-at4ssl.1</a>.</p>
</div>
<div id="ref-dataset:mukushev2022towards">
<p>Mukushev, Medet, Aigerim Kydyrbekova, Vadim Kimmelman, and Anara Sandygulova. 2022. “Towards Large Vocabulary Kazakh-Russian Sign Language Dataset: KRSL-OnlineSchool.” In <em>Proceedings of the Lrec2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</em>, 154–58. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.signlang-1.24">https://aclanthology.org/2022.signlang-1.24</a>.</p>
</div>
<div id="ref-murray2020importance">
<p>Murray, Joseph J, Wyatte C Hall, and Kristin Snoddon. 2020. “The Importance of Signed Languages for Deaf Children and Their Families.” <em>The Hearing Journal</em> 73 (3): 30–32.</p>
</div>
<div id="ref-muller-etal-2022-findings">
<p>Müller, Mathias, Sarah Ebling, Eleftherios Avramidis, Alessia Battisti, Michèle Berger, Richard Bowden, Annelies Braffort, et al. 2022. “Findings of the First WMT Shared Task on Sign Language Translation (WMT-SLT22).” In <em>Proceedings of the Seventh Conference on Machine Translation (Wmt)</em>, 744–72. Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics. <a href="https://aclanthology.org/2022.wmt-1.71">https://aclanthology.org/2022.wmt-1.71</a>.</p>
</div>
<div id="ref-muller-etal-2023-considerations">
<p>Müller, Mathias, Zifan Jiang, Amit Moryossef, Annette Rios, and Sarah Ebling. 2023. “Considerations for Meaningful Sign Language Machine Translation Based on Glosses.” In <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 682–93. Toronto, Canada: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.acl-short.60">https://doi.org/10.18653/v1/2023.acl-short.60</a>.</p>
</div>
<div id="ref-napier-leeson-2016">
<p>Napier, Jemina, and Lorraine Leeson. 2016. <em>Sign Language in Action</em>. London: Palgrave Macmillan.</p>
</div>
<div id="ref-neidle2001signstream">
<p>Neidle, Carol, Stan Sclaroff, and Vassilis Athitsos. 2001. “SignStream: A Tool for Linguistic and Computer Vision Research on Visual-Gestural Language Data.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 33 (3): 311–20.</p>
</div>
<div id="ref-dataset:oliveiraDatasetIrishSign2017">
<p>Oliveira, Marlon, Houssem Chatbri, Ylva Ferstl, Mohamed Farouk, Suzanne Little, Noel O’Connor, and A. Sutherland. 2017. “A Dataset for Irish Sign Language Recognition.” In <em>Proceedings of the Irish Machine Vision and Image Processing Conference (IMVIP)</em>.</p>
</div>
<div id="ref-van_den_Oord_2017NeuralDiscreteRepresentationLearning">
<p>Oord, Aäron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. “Neural Discrete Representation Learning.” In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, ca, USA</em>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 6306–15. <a href="https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html</a>.</p>
</div>
<div id="ref-ormel2012prosodic">
<p>Ormel, Ellen, and Onno Crasborn. 2012. “Prosodic Correlates of Sentences in Signed Languages: A Literature Review and Suggestions for New Types of Studies.” <em>Sign Language Studies</em> 12 (2): 279–315.</p>
</div>
<div id="ref-dataset:othman2012english">
<p>Othman, Achraf, and Mohamed Jemni. 2012. “English-Asl Gloss Parallel Corpus 2012: Aslg-Pc12.” In <em>5th Workshop on the Representation and Processing of Sign Languages: Interactions Between Corpus and Lexicon Lrec</em>.</p>
</div>
<div id="ref-dataset:oqvist-etal-2020-sts">
<p>Öqvist, Zrajm, Nikolaus Riemer Kankkonen, and Johanna Mesch. 2020. “STS-Korpus: A Sign Language Web Corpus Tool for Teaching and Public Use.” In <em>Proceedings of the Lrec2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</em>, 177–80. Marseille, France: European Language Resources Association (ELRA). <a href="https://aclanthology.org/2020.signlang-1.29">https://aclanthology.org/2020.signlang-1.29</a>.</p>
</div>
<div id="ref-padden1988interaction">
<p>Padden, C. 1988. <em>Interaction of Morphology and Syntax in American Sign Language</em>. Outstanding Disc Linguistics Series. Garland. <a href="https://books.google.com/books?id=Mea7AAAAIAAJ">https://books.google.com/books?id=Mea7AAAAIAAJ</a>.</p>
</div>
<div id="ref-padden1998asl">
<p>Padden, Carol A. 1998. “The ASL Lexicon.” <em>Sign Language &amp; Linguistics</em> 1 (1): 39–60.</p>
</div>
<div id="ref-padden2003alphabet">
<p>Padden, Carol A, and Darline Clark Gunsauls. 2003. “How the Alphabet Came to Be Used in a Sign Language.” <em>Sign Language Studies</em>, 10–33.</p>
</div>
<div id="ref-padden1988deaf">
<p>Padden, Carol A, and Tom Humphries. 1988. <em>Deaf in America</em>. Harvard University Press.</p>
</div>
<div id="ref-detection:pal2023importance">
<p>Pal, Abhilash, Stephan Huber, Cyrine Chaabani, Alessandro Manzotti, and Oscar Koller. 2023. “On the Importance of Signer Overlap for Sign Language Detection.” <em>ArXiv Preprint</em> abs/2303.10782. <a href="https://arxiv.org/abs/2303.10782">https://arxiv.org/abs/2303.10782</a>.</p>
</div>
<div id="ref-pose:panteleris2018using">
<p>Panteleris, Paschalis, Iason Oikonomidis, and Antonis Argyros. 2018. “Using a Single Rgb Frame for Real Time 3d Hand Pose Estimation in the Wild.” In <em>2018 Ieee Winter Conference on Applications of Computer Vision (Wacv)</em>, 436–45. IEEE.</p>
</div>
<div id="ref-papineni-etal-2002-bleu">
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, 311–18. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. <a href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a>.</p>
</div>
<div id="ref-patrie2011fingerspelled">
<p>Patrie, Carol J, and Robert E Johnson. 2011. <em>Fingerspelled Word Recognition Through Rapid Serial Visual Presentation: RSVP</em>. DawnSignPress.</p>
</div>
<div id="ref-pose:pavllo20193d">
<p>Pavllo, Dario, Christoph Feichtenhofer, David Grangier, and Michael Auli. 2019. “3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training.” In <em>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, ca, Usa, June 16-20, 2019</em>, 7753–62. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPR.2019.00794">https://doi.org/10.1109/CVPR.2019.00794</a>.</p>
</div>
<div id="ref-pose:pishchulin2012articulated">
<p>Pishchulin, Leonid, Arjun Jain, Mykhaylo Andriluka, Thorsten Thormählen, and Bernt Schiele. 2012. “Articulated People Detection and Pose Estimation: Reshaping the Future.” In <em>2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, Ri, Usa, June 16-21, 2012</em>, 3178–85. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2012.6248052">https://doi.org/10.1109/CVPR.2012.6248052</a>.</p>
</div>
<div id="ref-pizzuto:06001:sign-lang:lrec">
<p>Pizzuto, Elena Antinoro, Paolo Rossini, and Tommaso Russo. 2006. “Representing Signed Languages in Written Form: Questions That Need to Be Posed.” In <em>5th International Conference on Language Resources and Evaluation (LREC 2006)</em>, edited by Chiara Vettori, 1–6. Genoa, Italy: European Language Resources Association (ELRA). <a href="https://www.sign-lang.uni-hamburg.de/lrec/pub/06001.pdf">https://www.sign-lang.uni-hamburg.de/lrec/pub/06001.pdf</a>.</p>
</div>
<div id="ref-post-2018-call-sacrebleu">
<p>Post, Matt. 2018. “A Call for Clarity in Reporting BLEU Scores.” In <em>Proceedings of the Third Conference on Machine Translation: Research Papers</em>, 186–91. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/W18-6319">https://doi.org/10.18653/v1/W18-6319</a>.</p>
</div>
<div id="ref-pratap2020mls">
<p>Pratap, Vineel, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. “MLS: A Large-Scale Multilingual Dataset for Speech Research.” In <em>Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020</em>, edited by Helen Meng, Bo Xu, and Thomas Fang Zheng, 2757–61. ISCA. <a href="https://doi.org/10.21437/Interspeech.2020-2826">https://doi.org/10.21437/Interspeech.2020-2826</a>.</p>
</div>
<div id="ref-dataset:prillwitz2008dgs">
<p>Prillwitz, Siegmund, Thomas Hanke, Susanne König, Reiner Konrad, Gabriele Langer, and Arvid Schwarz. 2008. “DGS Corpus Project–Development of a Corpus Based Electronic Dictionary German Sign Language/German.” In <em>Sign-Lang at Lrec 2008</em>, 159–64. European Language Resources Association (ELRA).</p>
</div>
<div id="ref-writing:prillwitz1990hamburg">
<p>Prillwitz, Siegmund, and Heiko Zienert. 1990. “Hamburg Notation System for Sign Language: Development of a Sign Writing with Computer Application.” In <em>Current Trends in European Sign Language Research. Proceedings of the 3rd European Congress on Sign Language Research</em>, 355–79.</p>
</div>
<div id="ref-Radford2021LearningTV">
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:8748–63. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/radford21a.html">http://proceedings.mlr.press/v139/radford21a.html</a>.</p>
</div>
<div id="ref-raffel2020T5Transformer">
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <em>J. Mach. Learn. Res.</em> 21: 140:1–140:67. <a href="http://jmlr.org/papers/v21/20-074.html">http://jmlr.org/papers/v21/20-074.html</a>.</p>
</div>
<div id="ref-ramirez2004efficient">
<p>Ramırez, Javier, José C Segura, Carmen Benıtez, Angel De La Torre, and Antonio Rubio. 2004. “Efficient Voice Activity Detection Algorithms Using Long-Term Speech Information.” <em>Speech Communication</em> 42 (3-4): 271–87.</p>
</div>
<div id="ref-rathmann2011featural">
<p>Rathmann, Christian, and Gaurav Mathur. 2011. “A Featural Approach to Verb Agreement in Signed Languages.” <em>Theoretical Linguistics</em> 37 (3-4): 197–208.</p>
</div>
<div id="ref-segmentation:renz2021signa">
<p>Renz, Katrin, Nicolaj C Stache, Samuel Albanie, and Gül Varol. 2021a. “Sign Language Segmentation with Temporal Convolutional Networks.” In <em>ICASSP 2021-2021 Ieee International Conference on Acoustics, Speech and Signal Processing (Icassp)</em>, 2135–9. IEEE.</p>
</div>
<div id="ref-segmentation:renz2021signb">
<p>Renz, Katrin, Nicolaj C. Stache, Neil Fox, Gül Varol, and Samuel Albanie. 2021b. “Sign Segmentation with Changepoint-Modulated Pseudo-Labelling.” In <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2021, Virtual, June 19-25, 2021</em>, 3403–12. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPRW53098.2021.00379">https://doi.org/10.1109/CVPRW53098.2021.00379</a>.</p>
</div>
<div id="ref-text-to-image:Rombach2021HighResolutionIS">
<p>Rombach, Robin, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2021. “High-Resolution Image Synthesis with Latent Diffusion Models.” <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10674–85.</p>
</div>
<div id="ref-romero2017MANOHandModel">
<p>Romero, Javier, Dimitrios Tzionas, and Michael J. Black. 2017. “Embodied Hands: Modeling and Capturing Hands and Bodies Together.” <em>ACM Trans. Graph.</em> 36 (6). <a href="https://doi.org/10.1145/3130800.3130883">https://doi.org/10.1145/3130800.3130883</a>.</p>
</div>
<div id="ref-roy2011discourse">
<p>Roy, Cynthia B. 2011. <em>Discourse in Signed Languages</em>. Gallaudet University Press.</p>
</div>
<div id="ref-rust2024PrivacyAwareSign">
<p>Rust, Phillip, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, and Jean Maillard. 2024. “Towards Privacy-Aware Sign Language Translation at Scale.” <a href="http://arxiv.org/abs/2402.09611">http://arxiv.org/abs/2402.09611</a>.</p>
</div>
<div id="ref-ryali2023HieraVisionTransformer">
<p>Ryali, Chaitanya, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, et al. 2023. “Hiera: A Hierarchical Vision Transformer Without the Bells-and-Whistles.” In <em>Proceedings of the 40th International Conference on Machine Learning</em>, edited by Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, 202:29441–54. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v202/ryali23a.html">https://proceedings.mlr.press/v202/ryali23a.html</a>.</p>
</div>
<div id="ref-dataset:samsSignBDWordVideoBasedBangla2023">
<p>Sams, Ataher, Ahsan Habib Akash, and S. M. Mahbubur Rahman. 2023. “SignBD-Word: Video-Based Bangla Word-Level Sign Language and Pose Translation.” In <em>2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)</em>, 1–7. <a href="https://doi.org/10.1109/ICCCNT56998.2023.10306914">https://doi.org/10.1109/ICCCNT56998.2023.10306914</a>.</p>
</div>
<div id="ref-sandler2010prosody">
<p>Sandler, Wendy. 2010. “Prosody and Syntax in Sign Languages.” <em>Transactions of the Philological Society</em> 108 (3): 298–328.</p>
</div>
<div id="ref-sandler2012phonological">
<p>———. 2012. “The Phonological Organization of Sign Languages.” <em>Language and Linguistics Compass</em> 6 (3): 162–82.</p>
</div>
<div id="ref-sandler2006sign">
<p>Sandler, Wendy, and Diane Lillo-Martin. 2006. <em>Sign Language and Linguistic Universals</em>. Cambridge University Press.</p>
</div>
<div id="ref-segmentation:santemiz2009automatic">
<p>Santemiz, Pinar, Oya Aran, Murat Saraclar, and Lale Akarun. 2009. “Automatic Sign Segmentation from Continuous Signing via Multiple Sequence Alignment.” In <em>2009 Ieee 12th International Conference on Computer Vision Workshops, Iccv Workshops</em>, 2001–8. IEEE.</p>
</div>
<div id="ref-saunders2020adversarial">
<p>Saunders, Ben, Richard Bowden, and Necati Cihan Camgöz. 2020. “Adversarial Training for Multi-Channel Sign Language Production.” In <em>31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event, Uk, September 7-10, 2020</em>. BMVA Press. <a href="https://www.bmvc2020-conference.com/assets/papers/0223.pdf">https://www.bmvc2020-conference.com/assets/papers/0223.pdf</a>.</p>
</div>
<div id="ref-saunders2020everybody">
<p>Saunders, Ben, Necati Cihan Camgöz, and Richard Bowden. 2020a. “Everybody Sign Now: Translating Spoken Language to Photo Realistic Sign Language Video.” <em>ArXiv Preprint</em> abs/2011.09846. <a href="https://arxiv.org/abs/2011.09846">https://arxiv.org/abs/2011.09846</a>.</p>
</div>
<div id="ref-saunders2020progressive">
<p>———. 2020b. “Progressive Transformers for End-to-End Sign Language Production.” In <em>European Conference on Computer Vision</em>, 687–705.</p>
</div>
<div id="ref-anonysign">
<p>———. 2021. “Anonysign: Novel Human Appearance Synthesis for Sign Language Video Anonymisation.” In <em>2021 16th Ieee International Conference on Automatic Face and Gesture Recognition (Fg 2021)</em>, 1–8. <a href="https://doi.org/10.1109/FG52635.2021.9666984">https://doi.org/10.1109/FG52635.2021.9666984</a>.</p>
</div>
<div id="ref-savitzky1964smoothing">
<p>Savitzky, Abraham, and Marcel JE Golay. 1964. “Smoothing and Differentiation of Data by Simplified Least Squares Procedures.” <em>Analytical Chemistry</em> 36 (8): 1627–39.</p>
</div>
<div id="ref-schembri2018indicating">
<p>Schembri, Adam, Kearsy Cormier, and Jordan Fenlon. 2018. “Indicating Verbs as Typologically Unique Constructions: Reconsidering Verb ‘Agreement’in Sign Languages.” <em>Glossa: A Journal of General Linguistics</em> 3 (1).</p>
</div>
<div id="ref-dataset:schembri2013building">
<p>Schembri, Adam, Jordan Fenlon, Ramas Rentelis, Sally Reynolds, and Kearsy Cormier. 2013. “Building the British Sign Language Corpus.” <em>Language Documentation &amp; Conservation</em> 7: 136–54.</p>
</div>
<div id="ref-paula:schnepp2012combining">
<p>Schnepp, Jerry C, Rosalee J Wolfe, John C McDonald, and Jorge A Toro. 2012. “Combining Emotion and Facial Nonmanual Signals in Synthesized American Sign Language.” In <em>Proceedings of the 14th International Acm Sigaccess Conference on Computers and Accessibility</em>, 249–50.</p>
</div>
<div id="ref-paula:schnepp2013generating">
<p>Schnepp, Jerry C., Rosalee J. Wolfe, John C. McDonald, and Jorge A. Toro. 2013. “Generating Co-Occurring Facial Nonmanual Signals in Synthesized American Sign Language.” In <em>GRAPP/Ivapp</em>.</p>
</div>
<div id="ref-dataset:sehyr2021asl">
<p>Sehyr, Zed Sevcikova, Naomi Caselli, Ariel M Cohen-Goldberg, and Karen Emmorey. 2021. “The Asl-Lex 2.0 Project: A Database of Lexical and Phonological Properties for 2,723 Signs in American Sign Language.” <em>The Journal of Deaf Studies and Deaf Education</em> 26 (2): 263–77.</p>
</div>
<div id="ref-selvaraj-etal-2022-openhands">
<p>Selvaraj, Prem, Gokul Nc, Pratyush Kumar, and Mitesh Khapra. 2022. “OpenHands: Making Sign Language Recognition Accessible with Pose-Based Pretrained Models Across Languages.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2114–33. Dublin, Ireland: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2022.acl-long.150">https://doi.org/10.18653/v1/2022.acl-long.150</a>.</p>
</div>
<div id="ref-sennrich-etal-2016-neural">
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1715–25. Berlin, Germany: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P16-1162">https://doi.org/10.18653/v1/P16-1162</a>.</p>
</div>
<div id="ref-shi-etal-2022-open">
<p>Shi, Bowen, Diane Brentari, Gregory Shakhnarovich, and Karen Livescu. 2022. “Open-Domain Sign Language Translation Learned from Online Video.” In <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 6365–79. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics. <a href="https://aclanthology.org/2022.emnlp-main.427">https://aclanthology.org/2022.emnlp-main.427</a>.</p>
</div>
<div id="ref-dataset:fs18iccv">
<p>Shi, Bowen, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. 2019. “Fingerspelling Recognition in the Wild with Iterative Visual Attention.” In <em>2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, 5399–5408. IEEE. <a href="https://doi.org/10.1109/ICCV.2019.00550">https://doi.org/10.1109/ICCV.2019.00550</a>.</p>
</div>
<div id="ref-dataset:fs18slt">
<p>Shi, B., A. Martinez Del Rio, J. Keane, J. Michaux, G. Shakhnarovich D. Brentari, and K. Livescu. 2018. “American Sign Language Fingerspelling Recognition in the Wild.” <em>SLT</em>.</p>
</div>
<div id="ref-shieber1994restricting">
<p>Shieber, Stuart M. 1994. “RESTRICTING the Weak-Generative Capacity of Synchronous Tree-Adjoining Grammars.” <em>Computational Intelligence</em> 10 (4): 371–85.</p>
</div>
<div id="ref-shieber1990synchronous">
<p>Shieber, Stuart M., and Yves Schabes. 1990. “Synchronous Tree-Adjoining Grammars.” In <em>COLING 1990 Volume 3: Papers Presented to the 13th International Conference on Computational Linguistics</em>. <a href="https://aclanthology.org/C90-3045">https://aclanthology.org/C90-3045</a>.</p>
</div>
<div id="ref-shroyer1984signs">
<p>Shroyer, Edgar H, and Susan P Shroyer. 1984. <em>Signs Across America: A Look at Regional Differences in American Sign Language</em>. Gallaudet University Press.</p>
</div>
<div id="ref-pose:simon2017hand">
<p>Simon, Tomas, Hanbyul Joo, Iain A. Matthews, and Yaser Sheikh. 2017. “Hand Keypoint Detection in Single Images Using Multiview Bootstrapping.” In <em>2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, Hi, Usa, July 21-26, 2017</em>, 4645–53. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.494">https://doi.org/10.1109/CVPR.2017.494</a>.</p>
</div>
<div id="ref-simonyan2015very">
<p>Simonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” In <em>3rd International Conference on Learning Representations, ICLR 2015, San Diego, ca, Usa, May 7-9, 2015, Conference Track Proceedings</em>, edited by Yoshua Bengio and Yann LeCun. <a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</p>
</div>
<div id="ref-dataset:sincan2020autsl">
<p>Sincan, Ozge Mercanoglu, and Hacer Yalim Keles. 2020. “AUTSL: A Large Scale Multi-Modal Turkish Sign Language Dataset and Baseline Methods.” <em>IEEE Access</em> 8: 181340–55.</p>
</div>
<div id="ref-sohn1999statistical">
<p>Sohn, Jongseo, Nam Soo Kim, and Wonyong Sung. 1999. “A Statistical Model-Based Voice Activity Detection.” <em>IEEE Signal Processing Letters</em> 6 (1): 1–3.</p>
</div>
<div id="ref-dataset:starner_et_al_2023_PopSignASL_v1">
<p>Starner, Thad, Sean Forbes, Matthew So, David Martin, Rohit Sridhar, Gururaj Deshpande, Sam Sepah, et al. 2023. “PopSign Asl V1.0: An Isolated American Sign Language Dataset Collected via Smartphones.” In <em>Advances in Neural Information Processing Systems</em>, edited by A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, 36:184–96. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/00dada608b8db212ea7d9d92b24c68de-Paper-Datasets_and_Benchmarks.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/00dada608b8db212ea7d9d92b24c68de-Paper-Datasets_and_Benchmarks.pdf</a>.</p>
</div>
<div id="ref-writing:stokoe1960sign">
<p>Stokoe Jr, William C. 1960. “Sign Language Structure: An Outline of the Visual Communication Systems of the American Deaf.” <em>The Journal of Deaf Studies and Deaf Education</em> 10 (1): 3–37. <a href="https://doi.org/10.1093/deafed/eni001">https://doi.org/10.1093/deafed/eni001</a>.</p>
</div>
<div id="ref-stoll2018sign">
<p>Stoll, Stephanie, Necati Cihan Camgöz, Simon Hadfield, and Richard Bowden. 2018. “Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks.” In <em>British Machine Vision Conference 2018, BMVC 2018, Newcastle, Uk, September 3-6, 2018</em>, 304. BMVA Press. <a href="http://bmvc2018.org/contents/papers/0906.pdf">http://bmvc2018.org/contents/papers/0906.pdf</a>.</p>
</div>
<div id="ref-stoll2020text2sign">
<p>———. 2020. “Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks.” <em>International Journal of Computer Vision</em>, 1–18.</p>
</div>
<div id="ref-supalla1986classifier">
<p>Supalla, Ted. 1986. “The Classifier System in American Sign Language.” <em>Noun Classes and Categorization</em> 7: 181–214.</p>
</div>
<div id="ref-writing:sutton1990lessons">
<p>Sutton, Valerie. 1990. <em>Lessons in Sign Writing</em>. SignWriting.</p>
</div>
<div id="ref-tavella-etal-2022-wlasl">
<p>Tavella, Federico, Viktor Schlegel, Marta Romeo, Aphrodite Galata, and Angelo Cangelosi. 2022. “WLASL-LEX: A Dataset for Recognising Phonological Properties in American Sign Language.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 453–63. Dublin, Ireland: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2022.acl-short.49">https://doi.org/10.18653/v1/2022.acl-short.49</a>.</p>
</div>
<div id="ref-un2022">
<p>United Nations. 2022. “International Day of Sign Languages.” <a href="https://www.un.org/en/observances/sign-languages-day">https://www.un.org/en/observances/sign-languages-day</a>.</p>
</div>
<div id="ref-dataset:uthus2023YoutubeASL">
<p>Uthus, Dave, Garrett Tanzer, and Manfred Georg. 2023. “YouTube-Asl: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus.” In <em>Advances in Neural Information Processing Systems</em>, edited by A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, 36:29029–47. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/5c61452daca5f0c260e683b317d13a3f-Paper-Datasets_and_Benchmarks.pdf">https://proceedings.neurips.cc/paper_files/paper/2023/file/5c61452daca5f0c260e683b317d13a3f-Paper-Datasets_and_Benchmarks.pdf</a>.</p>
</div>
<div id="ref-dataset:herreweghe2015VGTCorpus">
<p>Van Herreweghe, Mieke and Vermeerbergen, Myriam and Demey, Eline and De Durpel, Hannes and Nyffels, Hilde and Verstraete, Sam. n.d. “Het Corpus VGT. Een digitaal open access corpus van videos and annotaties van Vlaamse Gebarentaal, ontwikkeld aan de Universiteit Gent ism KU Leuven. &lt;www.corpusvgt.be&gt;.” <a href="%7Bhttp://www.corpusvgt.ugent.be/%7D">{http://www.corpusvgt.ugent.be/}</a>.</p>
</div>
<div id="ref-Varol2021ReadAndAttend">
<p>Varol, Gül, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras, and Andrew Zisserman. 2021. “Read and Attend: Temporal Localisation in Sign Language Videos.” In <em>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual, June 19-25, 2021</em>, 16857–66. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPR46437.2021.01658">https://doi.org/10.1109/CVPR46437.2021.01658</a>.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, ca, USA</em>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 5998–6008. <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>.</p>
</div>
<div id="ref-dataset:viitaniemi-etal-2014-pot">
<p>Viitaniemi, Ville, Tommi Jantunen, Leena Savolainen, Matti Karppa, and Jorma Laaksonen. 2014. “S-Pot - a Benchmark in Spotting Signs Within Continuous Signing.” In <em>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</em>, 1892–7. Reykjavik, Iceland: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/440_Paper.pdf">http://www.lrec-conf.org/proceedings/lrec2014/pdf/440_Paper.pdf</a>.</p>
</div>
<div id="ref-dataset:vintar2012compiling">
<p>Vintar, Špela, Boštjan Jerko, and Marjetka Kulovec. 2012. “Compiling the Slovene Sign Language Corpus.” In <em>5th Workshop on the Representation and Processing of Sign Languages: Interactions Between Corpus and Lexicon. Language Resources and Evaluation Conference (Lrec)</em>, 5:159–62.</p>
</div>
<div id="ref-vogler2005analysis">
<p>Vogler, Christian, and Siome Goldenstein. 2005. “Analysis of Facial Expressions in American Sign Language.” In <em>Proc, of the 3rd Int. Conf. On Universal Access in Human-Computer Interaction, Springer</em>.</p>
</div>
<div id="ref-dataset:von2007towards">
<p>Von Agris, Ulrich, and Karl-Friedrich Kraiss. 2007. “Towards a Video Corpus for Signer-Independent Continuous Sign Language Recognition.” <em>Gesture in Human-Computer Interaction and Simulation, Lisbon, Portugal, May</em> 11.</p>
</div>
<div id="ref-walsh2022changing">
<p>Walsh, Harry, Ben Saunders, and Richard Bowden. 2022. “Changing the Representation: Examining Language Representation for Neural Sign Language Production.” In <em>Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives</em>, 117–24. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.sltat-1.18">https://aclanthology.org/2022.sltat-1.18</a>.</p>
</div>
<div id="ref-pose:wang2018vid2vid">
<p>Wang, Ting-Chun, Ming-Yu Liu, Jun-Yan Zhu, Nikolai Yakovenko, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. “Video-to-Video Synthesis.” In <em>Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, Neurips 2018, December 3-8, 2018, Montréal, Canada</em>, edited by Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, 1152–64. <a href="https://proceedings.neurips.cc/paper/2018/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html">https://proceedings.neurips.cc/paper/2018/hash/d86ea612dec96096c5e0fcc8dd42ab6d-Abstract.html</a>.</p>
</div>
<div id="ref-pose:wei2016cpm">
<p>Wei, Shih-En, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. 2016. “Convolutional Pose Machines.” In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, Nv, Usa, June 27-30, 2016</em>, 4724–32. IEEE Computer Society. <a href="https://doi.org/10.1109/CVPR.2016.511">https://doi.org/10.1109/CVPR.2016.511</a>.</p>
</div>
<div id="ref-wheatland2016analysis">
<p>Wheatland, Nkenge, Ahsan Abdullah, Michael Neff, Sophie Jörg, and Victor Zordan. 2016. “Analysis in Support of Realistic Timing in Animated Fingerspelling.” In <em>2016 Ieee Virtual Reality (Vr)</em>, 309–10. IEEE.</p>
</div>
<div id="ref-wilcox1992phonetics">
<p>Wilcox, Sherman. 1992. <em>The Phonetics of Fingerspelling</em>. Vol. 4. John Benjamins Publishing.</p>
</div>
<div id="ref-wilcox2004rethinking">
<p>Wilcox, Sherman, and Sarah Hafer. 2004. “Rethinking Classifiers. Emmorey, K.(Ed.).(2003). Perspectives on Classifier Constructions in Sign Languages. Mahwah, Nj: Lawrence Erlbaum Associates. 332 Pages. Hardcover.” Oxford University Press.</p>
</div>
<div id="ref-wittenburg2006elan">
<p>Wittenburg, Peter, Hennie Brugman, Albert Russel, Alex Klassmann, and Han Sloetjes. 2006. “ELAN: A Professional Framework for Multimodality Research.” In <em>Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06)</em>. Genoa, Italy: European Language Resources Association (ELRA). <a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/153_pdf.pdf">http://www.lrec-conf.org/proceedings/lrec2006/pdf/153_pdf.pdf</a>.</p>
</div>
<div id="ref-paula:wolfe2011linguistics">
<p>Wolfe, Rosalee, Peter Cook, John C McDonald, and Jerry C Schnepp. 2011. “Linguistics as Structure in Computer Animation: Toward a More Effective Synthesis of Brow Motion in American Sign Language.” <em>Sign Language &amp; Linguistics</em> 14 (1): 179–99.</p>
</div>
<div id="ref-paula:wolfecase">
<p>Wolfe, Rosalee J, Elena Jahn, Ronan Johnson, and John C McDonald. 2019. “The Case for Avatar Makeup.”</p>
</div>
<div id="ref-paula:wolfe2022supporting">
<p>Wolfe, Rosalee, John McDonald, Ronan Johnson, Ben Sturr, Syd Klinghoffer, Anthony Bonzani, Andrew Alexander, and Nicole Barnekow. 2022. “Supporting Mouthing in Signed Languages: New Innovations and a Proposal for Future Corpus Building.” In <em>Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives</em>, 125–30. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2022.sltat-1.19">https://aclanthology.org/2022.sltat-1.19</a>.</p>
</div>
<div id="ref-wfd2022">
<p>World Federation of the Deaf. 2022. “World Federation of the Deaf - Our Work.” <a href="https://wfdeaf.org/our-work/">https://wfdeaf.org/our-work/</a>.</p>
</div>
<div id="ref-who2021">
<p>World Health Organization. 2021. “Deafness and Hearing Loss.” <a href="https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss">https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss</a>.</p>
</div>
<div id="ref-xiao2020skeleton">
<p>Xiao, Qinkun, Minying Qin, and Yuting Yin. 2020. “Skeleton-Based Chinese Sign Language Recognition and Generation for Bidirectional Communication Between Deaf and Hearing People.” <em>Neural Networks</em> 125: 41–55.</p>
</div>
<div id="ref-xie2018SpatiotemporalS3D">
<p>Xie, Saining, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. 2018. “Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-Offs in Video Classification.” In <em>Computer Vision – Eccv 2018</em>, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 318–35. Cham: Springer International Publishing.</p>
</div>
<div id="ref-yin-etal-2021-including">
<p>Yin, Kayo, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. 2021. “Including Signed Languages in Natural Language Processing.” In <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, 7347–60. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.acl-long.570">https://doi.org/10.18653/v1/2021.acl-long.570</a>.</p>
</div>
<div id="ref-yin-read-2020-better">
<p>Yin, Kayo, and Jesse Read. 2020. “Better Sign Language Translation with STMC-Transformer.” In <em>Proceedings of the 28th International Conference on Computational Linguistics</em>, 5975–89. Barcelona, Spain (Online): International Committee on Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.coling-main.525">https://doi.org/10.18653/v1/2020.coling-main.525</a>.</p>
</div>
<div id="ref-Yu2017SpatioTemporalGC">
<p>Yu, Bing, Haoteng Yin, and Zhanxing Zhu. 2018. “Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting.” In <em>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden</em>, edited by Jérôme Lang, 3634–40. ijcai.org. <a href="https://doi.org/10.24963/ijcai.2018/505">https://doi.org/10.24963/ijcai.2018/505</a>.</p>
</div>
<div id="ref-dataset:zafrulla2010novel">
<p>Zafrulla, Zahoor, Helene Brashear, Harley Hamilton, and Thad Starner. 2010. “A Novel Approach to American Sign Language (ASL) Phrase Verification Using Reversed Signing.” In <em>2010 Ieee Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</em>, 48–55. IEEE.</p>
</div>
<div id="ref-pose:zelinka2020neural">
<p>Zelinka, Jan, and Jakub Kanis. 2020. “Neural Sign Language Synthesis: Words Are Our Glosses.” In <em>The Ieee Winter Conference on Applications of Computer Vision</em>, 3395–3403.</p>
</div>
<div id="ref-zhang2023sltunet">
<p>Zhang, Biao, Mathias Müller, and Rico Sennrich. 2023. “SLTUNET: A Simple Unified Model for Sign Language Translation.” In <em>The Eleventh International Conference on Learning Representations</em>. Kigali, Rwanda. <a href="https://openreview.net/forum?id=EBS4C77p_5S">https://openreview.net/forum?id=EBS4C77p_5S</a>.</p>
</div>
<div id="ref-pose-to-image:zhang2023adding">
<p>Zhang, Lvmin, and Maneesh Agrawala. 2023. “Adding Conditional Control to Text-to-Image Diffusion Models.” <a href="http://arxiv.org/abs/2302.05543">http://arxiv.org/abs/2302.05543</a>.</p>
</div>
<div id="ref-Zhang2010RevisedEditDistanceSignVideoRetrieval">
<p>Zhang, Shilin, and Bo Zhang. 2010. “Using Revised String Edit Distance to Sign Language Video Retrieval.” In <em>2010 Second International Conference on Computational Intelligence and Natural Computing</em>, 1:45–49. <a href="https://doi.org/10.1109/CINC.2010.5643895">https://doi.org/10.1109/CINC.2010.5643895</a>.</p>
</div>
<div id="ref-zhao2000machine">
<p>Zhao, Liwei, Karin Kipper, William Schuler, Christian Vogler, Norman Badler, and Martha Palmer. 2000. “A Machine Translation System from English to American Sign Language.” In <em>Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Technical Papers</em>, 54–67. Cuernavaca, Mexico: Springer. <a href="https://link.springer.com/chapter/10.1007/3-540-39965-8_6">https://link.springer.com/chapter/10.1007/3-540-39965-8_6</a>.</p>
</div>
<div id="ref-Zhao_Zhang_Fu_Hu_Su_Chen_2024">
<p>Zhao, Rui, Liang Zhang, Biao Fu, Cong Hu, Jinsong Su, and Yidong Chen. 2024. “Conditional Variational Autoencoder for Sign Language Translation with Cross-Modal Alignment.” <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 38 (17): 19643–51. <a href="https://doi.org/10.1609/aaai.v38i17.29937">https://doi.org/10.1609/aaai.v38i17.29937</a>.</p>
</div>
<div id="ref-Zhao2023BESTPretrainingSignLanguageRecognition">
<p>Zhao, Weichao, Hezhen Hu, Wengang Zhou, Jiaxin Shi, and Houqiang Li. 2023. “BEST: BERT Pre-Training for Sign Language Recognition with Coupling Tokenization.” <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 37 (3): 3597–3605. <a href="https://doi.org/10.1609/aaai.v37i3.25470">https://doi.org/10.1609/aaai.v37i3.25470</a>.</p>
</div>
<div id="ref-dataset:Zhou2021_SignBackTranslation_CSLDaily">
<p>Zhou, Hao, Wengang Zhou, Weizhen Qi, Junfu Pu, and Houqiang Li. 2021. “Improving Sign Language Translation with Monolingual Data by Sign Back-Translation.” In <em>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual, June 19-25, 2021</em>, 1316–25. Computer Vision Foundation / IEEE. <a href="https://doi.org/10.1109/CVPR46437.2021.00137">https://doi.org/10.1109/CVPR46437.2021.00137</a>.</p>
</div>
<div id="ref-zwitserlood2004synthetic">
<p>Zwitserlood, Inge, Margriet Verlinden, Johan Ros, Sanny Van Der Schoot, and T Netherlands. 2004. “Synthetic Signing for the Deaf: Esign.” In <em>Proceedings of the Conference and Workshop on Assistive Technologies for Vision and Hearing Impairment, Cvhi</em>.</p>
</div>
<div id="ref-dataset:acheta2014ACD">
<p>Łacheta, Joanna, and PawełRutkowski. 2014. “A Corpus-Based Dictionary of Polish Sign Language (Pjm).” In.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>When capitalized, “Deaf” refers to a community of deaf people who share a language and a culture, whereas the lowercase “deaf” refers to the audiological condition of not hearing. We follow the more recent convention of abandoning a distinction between “Deaf” and “deaf”, using the latter term also to refer to (deaf) members of the sign language community <span class="citation" data-cites="napier-leeson-2016 kusters-et-al-2017">(Napier and Leeson <a href="#ref-napier-leeson-2016" role="doc-biblioref">2016</a>; Annelies Maria Jozef Kusters, O’Brien, and De Meulder <a href="#ref-kusters-et-al-2017" role="doc-biblioref">2017</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>We mainly refer to ASL, where most sign language research has been conducted, but not exclusively.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><!--
        --></main>
    </div>

    

    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- Add bootstrap classes -->
    <script>
    for(const table of document.getElementsByTagName("table")) {
        table.className += "table table-bordered table-hover";
    }

    // Replace navbar ul with nav elements
    while(ul = document.querySelector("#navbar ul")) {
        const nav = document.createElement("nav");
        nav.classList = "nav nav-pills flex-column";

        for(const child of ul.children) {
            if(child.tagName == "LI") {
                nav.innerHTML += child.innerHTML;
            } else {
                nav.innerHTML += child.outerHTML;
            }
        }
        ul.replaceWith(nav);
    }

    for(const link of document.querySelectorAll("#navbar a")) {
        link.className += "nav-link my-1";
    }
    </script>

    <!-- Bootstrap -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
</body>
</html>